"""
å¤šæºæ£€ç´¢å·¥ä½œæµ - V2
è¾“å‡ºæ ¼å¼ä¼˜åŒ–ï¼šåŒºåˆ†æ—¥å¿—(log)å’Œç»“æœ(result)
åªæœ‰ç»“æœä¼šä¿å­˜åˆ°æ•°æ®åº“
"""
import os
import json
from typing import TypedDict, AsyncGenerator, List, Dict
import asyncio
from sqlalchemy import select, func

from app.core.config import settings
from app.db.database import get_db_session
from app.services.llm_service import llm_service
from app.services.search_workflow_service import search_service
from app.models import WorkflowExecution, Message, MessageType
from app.crud import message as crud_message
from app.schemas.message import MessageCreateSchema


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    conversation_id: int
    user_id: int
    user_query: str
    user_attachments: List[Dict]
    history_messages: List[Dict]

    # æ­¥éª¤ç»“æœï¼ˆå­˜å‚¨ï¼‰
    patient_features: str
    pubmed_query: str
    clinical_trial_keywords: str
    papers: List[Dict]
    trials: List[Dict]
    paper_analyses: List[Dict]
    trial_analysis: str
    final_answer: str

    # æµç¨‹æ§åˆ¶
    current_step: str
    errors: List[str]


class MultiSourceWorkflowV2:
    """å¤šæºæ£€ç´¢å·¥ä½œæµ V2 - ä¼˜åŒ–è¾“å‡ºæ ¼å¼"""

    async def execute_with_streaming(
            self,
            conversation_id: int,
            user_id: int,
            user_query: str,
            user_attachments: List[Dict] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        æ‰§è¡Œå·¥ä½œæµå¹¶æµå¼è¾“å‡º

        è¾“å‡ºæ ¼å¼:
        - type='log': è¿‡ç¨‹æ—¥å¿—ï¼Œä¸ä¿å­˜
        - type='result': æ­¥éª¤ç»“æœï¼Œä¿å­˜åˆ°æœ€ç»ˆæŠ¥å‘Š
        - type='section_start': åŒºå—å¼€å§‹æ ‡è®°
        - type='section_end': åŒºå—ç»“æŸæ ‡è®°
        - type='done': å®Œæˆæ ‡è®°
        """

        # åˆ›å»ºæ‰§è¡Œè®°å½•
        execution_id = None
        async with get_db_session() as db:
            execution = WorkflowExecution(
                conversation_id=conversation_id,
                user_id=user_id,
                workflow_type='multi_source',
                status='running',
                current_step='initializing'
            )
            db.add(execution)
            await db.commit()
            execution_id = execution.id

        # åŠ è½½å†å²å¯¹è¯
        history_messages = await self._load_history(conversation_id)

        # åˆå§‹åŒ–çŠ¶æ€
        state: WorkflowState = {
            'conversation_id': conversation_id,
            'user_id': user_id,
            'user_query': user_query,
            'user_attachments': user_attachments or [],
            'history_messages': history_messages,
            'patient_features': '',
            'pubmed_query': '',
            'clinical_trial_keywords': '',
            'papers': [],
            'trials': [],
            'paper_analyses': [],
            'trial_analysis': '',
            'final_answer': '',
            'current_step': '',
            'errors': []
        }

        try:
            # æ­¥éª¤1: æå–æ‚£è€…ç‰¹å¾
            async for chunk in self._step_extract_features(state):
                yield chunk

            # æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢æ¡ä»¶
            async for chunk in self._step_generate_queries(state):
                yield chunk

            # æ­¥éª¤3: å¤šæºæ£€ç´¢
            async for chunk in self._step_search(state):
                yield chunk

            # æ­¥éª¤4: åˆ†ææ–‡çŒ®
            async for chunk in self._step_analyze_papers(state):
                yield chunk

            # æ­¥éª¤5: åˆ†æä¸´åºŠè¯•éªŒ
            async for chunk in self._step_analyze_trials(state):
                yield chunk

            # æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            async for chunk in self._step_generate_final(state):
                yield chunk

            # ä¿å­˜æœ€ç»ˆç»“æœ
            await self._save_final_result(state, execution_id)

            # æ›´æ–°æ‰§è¡Œè®°å½•
            async with get_db_session() as db:
                execution = await db.get(WorkflowExecution, execution_id)
                execution.status = 'completed'
                execution.completed_at = func.now()
                await db.commit()

            yield {'type': 'done', 'content': ''}

        except Exception as e:
            # è®°å½•é”™è¯¯
            async with get_db_session() as db:
                execution = await db.get(WorkflowExecution, execution_id)
                execution.status = 'failed'
                execution.error_message = str(e)
                await db.commit()

            yield {
                'type': 'error',
                'step': state.get('current_step', 'unknown'),
                'content': f'âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}'
            }

    async def _step_extract_features(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤1: æå–æ‚£è€…ç‰¹å¾"""
        state['current_step'] = 'extract_features'

        yield {
            'type': 'section_start',
            'step': 'extract_features',
            'title': 'æå–æ‚£è€…ç‰¹å¾',
            'collapsible': True
        }

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if state['history_messages']:
            context_parts.append("### å†å²å¯¹è¯")
            for msg in state['history_messages'][-5:]:
                role = "ç”¨æˆ·" if msg['type'] == 'user' else "AI"
                context_parts.append(f"**{role}**: {msg['content'][:200]}...")

        if state['user_attachments']:
            context_parts.append("\n### ç”¨æˆ·ä¸Šä¼ çš„é™„ä»¶")
            for att in state['user_attachments']:
                context_parts.append(f"- {att['original_filename']}")

        context = "\n".join(context_parts)

        prompt = f"""{context}

### å½“å‰ç”¨æˆ·é—®é¢˜
{state['user_query']}

### ä»»åŠ¡
è¯·ä»ä»¥ä¸Šä¿¡æ¯ä¸­æå–æ‚£è€…çš„å…³é”®ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š
1. **ä¸»è¦ç–¾ç—…/è¯Šæ–­**
2. **ç—…ç†ç±»å‹å’Œåˆ†æœŸ**
3. **åŸºå› çªå˜ä¿¡æ¯**
4. **æ—¢å¾€æ²»ç–—å²**
5. **å½“å‰çŠ¶æ€å’Œéœ€æ±‚**

è¯·ä»¥ç»“æ„åŒ–ã€æ¸…æ™°çš„æ–¹å¼åˆ—å‡ºè¿™äº›ä¿¡æ¯ã€‚"""

        messages = [{"role": "user", "content": prompt}]

        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡é™„ä»¶
        image_attachments = [att for att in state['user_attachments']
                             if att.get('mime_type', '').startswith('image/')]

        full_response = ""

        # æ—¥å¿—: å¼€å§‹åˆ†æ
        yield {
            'type': 'log',
            'step': 'extract_features',
            'content': 'ğŸ¤” æ­£åœ¨åˆ†ææ‚£è€…ä¿¡æ¯...\n'
        }

        try:
            if image_attachments:
                for att in image_attachments:
                    async for token in llm_service.chat_with_image_stream(
                            text=prompt,
                            image_path=att['file_path'],
                            history=[]
                    ):
                        full_response += token
            else:
                async for token in llm_service.chat_stream(messages=messages):
                    full_response += token

            state['patient_features'] = full_response

            # ç»“æœ: æå–çš„ç‰¹å¾
            yield {
                'type': 'result',
                'step': 'extract_features',
                'content': full_response,
                'summary': 'âœ… æ‚£è€…ç‰¹å¾æå–å®Œæˆ'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'extract_features',
                'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
            }
            state['errors'].append(f'extract_features: {str(e)}')

        yield {
            'type': 'section_end',
            'step': 'extract_features'
        }

    async def _step_generate_queries(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢æ¡ä»¶"""
        state['current_step'] = 'generate_queries'

        yield {
            'type': 'section_start',
            'step': 'generate_queries',
            'title': 'ç”Ÿæˆæ£€ç´¢æ¡ä»¶',
            'collapsible': True
        }

        prompt = f"""åŸºäºä»¥ä¸‹æ‚£è€…ç‰¹å¾ï¼Œç”Ÿæˆç²¾ç¡®çš„æ£€ç´¢æ¡ä»¶ï¼š

### æ‚£è€…ç‰¹å¾
{state['patient_features']}

### ä»»åŠ¡
è¯·ç”Ÿæˆä»¥ä¸‹æ£€ç´¢æ¡ä»¶ï¼š
1. **PubMed æ£€ç´¢è¡¨è¾¾å¼**: ä½¿ç”¨å¸ƒå°”è¿ç®—ç¬¦ï¼ˆANDã€ORï¼‰ï¼Œæ„å»ºç²¾ç¡®çš„æ£€ç´¢å¼
2. **ClinicalTrials.gov å…³é”®è¯**: æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”

**è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆJSONæ ¼å¼ï¼‰**:
```json
{{
    "pubmed_query": "æ£€ç´¢è¡¨è¾¾å¼",
    "clinical_trial_keywords": "å…³é”®è¯1,å…³é”®è¯2,å…³é”®è¯3"
}}
```

åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""

        messages = [{"role": "user", "content": prompt}]

        yield {
            'type': 'log',
            'step': 'generate_queries',
            'content': 'ğŸ” æ­£åœ¨ç”Ÿæˆæ£€ç´¢æ¡ä»¶...\n'
        }

        full_response = ""
        try:
            async for token in llm_service.chat_stream(messages=messages):
                full_response += token

            # è§£æ JSON
            start = full_response.find('{')
            end = full_response.rfind('}') + 1
            if start != -1 and end > start:
                queries = json.loads(full_response[start:end])
                state['pubmed_query'] = queries.get('pubmed_query', '')
                state['clinical_trial_keywords'] = queries.get('clinical_trial_keywords', '')
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON")

            # ç»“æœ: æ£€ç´¢æ¡ä»¶
            yield {
                'type': 'result',
                'step': 'generate_queries',
                'content': f"""**PubMed æ£€ç´¢å¼**: `{state['pubmed_query']}`

**ä¸´åºŠè¯•éªŒå…³é”®è¯**: `{state['clinical_trial_keywords']}`""",
                'summary': 'âœ… æ£€ç´¢æ¡ä»¶ç”Ÿæˆå®Œæˆ',
                'data': {
                    'pubmed_query': state['pubmed_query'],
                    'clinical_trial_keywords': state['clinical_trial_keywords']
                }
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_queries',
                'content': f'âš ï¸ è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¡ä»¶: {str(e)}\n'
            }
            state['pubmed_query'] = state['user_query']
            state['clinical_trial_keywords'] = state['user_query']
            state['errors'].append(f'generate_queries: {str(e)}')

        yield {
            'type': 'section_end',
            'step': 'generate_queries'
        }

    async def _step_search(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤3: å¤šæºæ£€ç´¢"""
        state['current_step'] = 'search'

        yield {
            'type': 'section_start',
            'step': 'search',
            'title': 'æ‰§è¡Œå¤šæºæ£€ç´¢',
            'collapsible': True
        }

        limit = settings.max_search_results
        progress_queue = asyncio.Queue()

        # å¯åŠ¨æ£€ç´¢ä»»åŠ¡
        async def search_all():
            # PubMed
            papers = await search_service.search_pubmed_with_cache(
                state['pubmed_query'],
                limit,
                progress_queue
            )
            state['papers'].extend(papers[:limit])

            # Europe PMC
            if len(state['papers']) < limit:
                remaining = limit - len(state['papers'])
                papers = await search_service.search_europepmc_with_cache(
                    state['pubmed_query'],
                    remaining,
                    progress_queue
                )
                state['papers'].extend(papers[:remaining])

            # ä¸´åºŠè¯•éªŒ
            trials = await search_service.search_clinical_trials_with_cache(
                state['clinical_trial_keywords'],
                limit,
                progress_queue
            )
            state['trials'].extend(trials[:limit])

            await progress_queue.put({'type': 'DONE'})

        search_task = asyncio.create_task(search_all())

        # å¤„ç†è¿›åº¦æ¶ˆæ¯
        while True:
            msg = await progress_queue.get()

            if isinstance(msg, dict):
                if msg.get('type') == 'DONE':
                    break
                elif msg.get('type') == 'log':
                    # æ—¥å¿—æ¶ˆæ¯
                    yield {
                        'type': 'log',
                        'step': 'search',
                        'source': msg.get('source'),
                        'content': msg['content']
                    }
                elif msg.get('type') == 'result':
                    # ç»“æœæ¶ˆæ¯
                    yield {
                        'type': 'result',
                        'step': 'search',
                        'source': msg.get('source'),
                        'content': msg['content'],
                        'data': msg.get('data')
                    }

        await search_task

        # æ±‡æ€»ç»“æœ
        yield {
            'type': 'result',
            'step': 'search',
            'content': f"""### ğŸ“Š æ£€ç´¢æ±‡æ€»

- **æ–‡çŒ®æ€»æ•°**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒ**: {len(state['trials'])} ä¸ª""",
            'summary': f'âœ… å¤šæºæ£€ç´¢å®Œæˆï¼ˆ{len(state["papers"])} ç¯‡æ–‡çŒ®ï¼Œ{len(state["trials"])} ä¸ªè¯•éªŒï¼‰',
            'data': {
                'paper_count': len(state['papers']),
                'trial_count': len(state['trials'])
            }
        }

        yield {
            'type': 'section_end',
            'step': 'search'
        }

    async def _step_analyze_papers(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤4: åˆ†ææ–‡çŒ®"""
        state['current_step'] = 'analyze_papers'

        yield {
            'type': 'section_start',
            'step': 'analyze_papers',
            'title': 'åˆ†ææ–‡çŒ®',
            'collapsible': True
        }

        if not state['papers']:
            yield {
                'type': 'result',
                'step': 'analyze_papers',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®',
                'summary': 'â„¹ï¸ æ— æ–‡çŒ®å¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_papers'}
            return

        # åªåˆ†æå‰5ç¯‡
        papers_to_analyze = state['papers'][:5]

        for i, paper in enumerate(papers_to_analyze):
            yield {
                'type': 'log',
                'step': 'analyze_papers',
                'content': f'\nğŸ“„ åˆ†ææ–‡çŒ® {i+1}/{len(papers_to_analyze)}: {paper["title"]}\n'
            }

            pdf_path = paper.get('pdf_path')
            if not pdf_path or not os.path.exists(pdf_path):
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'content': 'âš ï¸ PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡\n'
                }
                continue

            prompt = f"""è¯·åˆ†æè¿™ç¯‡PDFæ–‡çŒ®ï¼š

### æ‚£è€…ç‰¹å¾
{state['patient_features']}

### ç”¨æˆ·é—®é¢˜
{state['user_query']}

### æ–‡çŒ®ä¿¡æ¯
- **æ ‡é¢˜**: {paper['title']}
- **ä½œè€…**: {paper.get('authors', 'N/A')}
- **å‘è¡¨æ—¥æœŸ**: {paper.get('pub_date', 'N/A')}

### åˆ†æä»»åŠ¡
1. **æ ¸å¿ƒå†…å®¹**: ç®€è¦æ¦‚è¿°
2. **ç›¸å…³æ€§**: ä¸æ‚£è€…æƒ…å†µçš„ç›¸å…³ç¨‹åº¦
3. **ä¸»è¦å‘ç°**: åˆ—å‡ºå…³é”®ç»“è®º
4. **è¯æ®ç­‰çº§**: è¯„ä¼°ç ”ç©¶ç±»å‹å’Œå¯é æ€§
5. **ä¸´åºŠæ„ä¹‰**: å¯¹æ‚£è€…çš„å®é™…æŒ‡å¯¼ä»·å€¼

è¯·ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼è¾“å‡ºã€‚"""

            analysis = ""
            try:
                async for token in llm_service.chat_with_pdf_stream(
                        text=prompt,
                        pdf_path=pdf_path,
                        history=[]
                ):
                    analysis += token

                state['paper_analyses'].append({
                    'paper': paper,
                    'analysis': analysis
                })

                # ç»“æœ: å•ç¯‡æ–‡çŒ®åˆ†æ
                yield {
                    'type': 'result',
                    'step': 'analyze_papers',
                    'content': f"""### æ–‡çŒ® {i+1}: {paper['title']}

{analysis}""",
                    'data': {
                        'paper_id': paper.get('id'),
                        'pmid': paper.get('pmid'),
                        'title': paper['title']
                    }
                }

            except Exception as e:
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
                }

        yield {
            'type': 'result',
            'step': 'analyze_papers',
            'content': '',
            'summary': f'âœ… æ–‡çŒ®åˆ†æå®Œæˆï¼ˆ{len(state["paper_analyses"])} ç¯‡ï¼‰'
        }

        yield {
            'type': 'section_end',
            'step': 'analyze_papers'
        }

    async def _step_analyze_trials(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤5: åˆ†æä¸´åºŠè¯•éªŒ"""
        state['current_step'] = 'analyze_trials'

        yield {
            'type': 'section_start',
            'step': 'analyze_trials',
            'title': 'åˆ†æä¸´åºŠè¯•éªŒ',
            'collapsible': True
        }

        if not state['trials']:
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ä¸´åºŠè¯•éªŒ',
                'summary': 'â„¹ï¸ æ— è¯•éªŒå¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_trials'}
            return

        yield {
            'type': 'log',
            'step': 'analyze_trials',
            'content': f'ğŸ¤” æ­£åœ¨åˆ†æ {len(state["trials"])} ä¸ªä¸´åºŠè¯•éªŒ...\n'
        }

        # æ ¼å¼åŒ–è¯•éªŒä¿¡æ¯
        trials_text = []
        for i, trial in enumerate(state['trials']):
            trial_info = f"""### è¯•éªŒ {i+1}: {trial.get('title')}
- **NCT ID**: {trial.get('nct_id')}
- **çŠ¶æ€**: {trial.get('status')}
- **é˜¶æ®µ**: {trial.get('phase')}
- **ç–¾ç—…**: {trial.get('conditions')}
- **èµåŠ©æ–¹**: {trial.get('sponsor')}
"""
            trials_text.append(trial_info)

        prompt = f"""åŸºäºæ‚£è€…ç‰¹å¾è¯„ä¼°ä»¥ä¸‹ä¸´åºŠè¯•éªŒï¼š

### æ‚£è€…ç‰¹å¾
{state['patient_features']}

### ä¸´åºŠè¯•éªŒåˆ—è¡¨
{chr(10).join(trials_text)}

### åˆ†æä»»åŠ¡
é’ˆå¯¹æ¯ä¸ªè¯•éªŒ:
1. **é€‚é…åº¦è¯„åˆ†** (0-100åˆ†)
2. **å…¥ç»„æ ‡å‡†åˆ†æ**
3. **æ’é™¤æ ‡å‡†è€ƒé‡**
4. **è¯•éªŒä¼˜åŠ¿**
5. **æ½œåœ¨é£é™©**
6. **æ¨èç­‰çº§**

æœ€åç»™å‡ºç»¼åˆå»ºè®®ã€‚"""

        messages = [{"role": "user", "content": prompt}]

        analysis = ""
        try:
            async for token in llm_service.chat_stream(
                    messages=messages,
                    model=settings.qwen_long_model
            ):
                analysis += token

            state['trial_analysis'] = analysis

            # ç»“æœ: è¯•éªŒåˆ†æ
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': analysis,
                'summary': f'âœ… ä¸´åºŠè¯•éªŒåˆ†æå®Œæˆï¼ˆ{len(state["trials"])} ä¸ªï¼‰'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'analyze_trials',
                'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
            }

        yield {
            'type': 'section_end',
            'step': 'analyze_trials'
        }

    async def _step_generate_final(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        state['current_step'] = 'generate_final'

        yield {
            'type': 'section_start',
            'step': 'generate_final',
            'title': 'ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š',
            'collapsible': False
        }

        yield {
            'type': 'log',
            'step': 'generate_final',
            'content': 'ğŸ“ æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š...\n'
        }

        # æ±‡æ€»æ–‡çŒ®åˆ†æ
        papers_summary = []
        for i, item in enumerate(state['paper_analyses']):
            summary = f"**æ–‡çŒ® {i+1}**: {item['paper']['title']} - {item['analysis'][:200]}..."
            papers_summary.append(summary)

        prompt = f"""è¯·åŸºäºæ‰€æœ‰åˆ†æç”Ÿæˆä¸€ä»½ä¸“ä¸šåŒ»ç–—å’¨è¯¢æŠ¥å‘Šï¼š

### åŸå§‹é—®é¢˜
{state['user_query']}

### æ‚£è€…ç‰¹å¾
{state['patient_features'][:500]}...

### æ–‡çŒ®åˆ†æï¼ˆ{len(state['paper_analyses'])} ç¯‡ï¼‰
{chr(10).join(papers_summary) if papers_summary else "æš‚æ— "}

### ä¸´åºŠè¯•éªŒåˆ†æï¼ˆ{len(state['trials'])} ä¸ªï¼‰
{state['trial_analysis'][:500] if state['trial_analysis'] else "æš‚æ— "}...

### æŠ¥å‘Šè¦æ±‚
ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Šï¼ŒåŒ…å«ï¼š
1. **æ‰§è¡Œæ‘˜è¦**
2. **æ²»ç–—æ–¹æ¡ˆå»ºè®®**
3. **ä¸´åºŠè¯•éªŒæ¨è**
4. **æ³¨æ„äº‹é¡¹**
5. **åç»­è¡ŒåŠ¨å»ºè®®**"""

        messages = [{"role": "user", "content": prompt}]

        final_answer = ""
        try:
            async for token in llm_service.chat_stream(messages=messages):
                final_answer += token

            state['final_answer'] = final_answer

            # ç»“æœ: æœ€ç»ˆæŠ¥å‘Š
            yield {
                'type': 'result',
                'step': 'generate_final',
                'content': final_answer,
                'summary': 'âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_final',
                'content': f'âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n'
            }

        yield {
            'type': 'section_end',
            'step': 'generate_final'
        }

    async def _load_history(self, conversation_id: int) -> List[Dict]:
        """åŠ è½½å†å²å¯¹è¯"""
        async with get_db_session() as db:
            result = await db.execute(
                select(Message)
                .where(Message.conversation_id == conversation_id)
                .order_by(Message.created_at.desc())
                .limit(10)
            )
            messages = result.scalars().all()

            return [
                {
                    'type': 'user' if m.message_type == MessageType.USER else 'assistant',
                    'content': m.content
                }
                for m in reversed(list(messages))
            ]

    async def _save_final_result(self, state: WorkflowState, execution_id: int):
        """ä¿å­˜æœ€ç»ˆç»“æœåˆ°æ•°æ®åº“ï¼ˆåªä¿å­˜ç»“æœï¼Œä¸ä¿å­˜æ—¥å¿—ï¼‰"""
        async with get_db_session() as db:
            # æ„å»ºå®Œæ•´çš„æœ€ç»ˆæŠ¥å‘Š
            full_content = f"""# å¤šæºæ£€ç´¢åˆ†ææŠ¥å‘Š

## 1. æ‚£è€…ç‰¹å¾åˆ†æ
{state['patient_features']}

---

## 2. æ£€ç´¢æ¡ä»¶
- **PubMed**: `{state['pubmed_query']}`
- **ä¸´åºŠè¯•éªŒ**: `{state['clinical_trial_keywords']}`

---

## 3. æ£€ç´¢ç»“æœ
- **æ–‡çŒ®æ•°é‡**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒæ•°é‡**: {len(state['trials'])} ä¸ª

---

## 4. æ–‡çŒ®åˆ†æ
"""

            # æ·»åŠ æ–‡çŒ®åˆ†æ
            if state['paper_analyses']:
                for i, item in enumerate(state['paper_analyses']):
                    full_content += f"\n### æ–‡çŒ® {i+1}: {item['paper']['title']}\n\n"
                    full_content += f"{item['analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— æ–‡çŒ®åˆ†æ\n\n---\n"

            # æ·»åŠ è¯•éªŒåˆ†æ
            full_content += f"\n## 5. ä¸´åºŠè¯•éªŒåˆ†æ\n\n"
            if state['trial_analysis']:
                full_content += f"{state['trial_analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— ä¸´åºŠè¯•éªŒåˆ†æ\n\n---\n"

            # æ·»åŠ æœ€ç»ˆæŠ¥å‘Š
            full_content += f"\n## 6. ç»¼åˆæŠ¥å‘Š\n\n{state['final_answer']}\n"

            # ä¿å­˜ä¸ºæ¶ˆæ¯
            message_schema = MessageCreateSchema(
                conversation_id=state['conversation_id'],
                content=full_content,
                message_type=MessageType.ASSISTANT,
                attachments=[]
            )

            saved_message = await crud_message.create_message(
                db,
                message_schema=message_schema,
                user_id=state['user_id']
            )

            # æ›´æ–°æ‰§è¡Œè®°å½•
            execution = await db.get(WorkflowExecution, execution_id)
            execution.result_message_id = saved_message['id']
            execution.patient_features = state['patient_features']
            execution.search_queries = json.dumps({
                'pubmed': state['pubmed_query'],
                'clinical_trial': state['clinical_trial_keywords']
            })
            await db.commit()


# å…¨å±€å®ä¾‹
workflow_service_v2 = MultiSourceWorkflowV2()