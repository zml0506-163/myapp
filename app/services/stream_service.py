"""
æµå¼æœåŠ¡ - å¤„ç†åå°ç”Ÿæˆä»»åŠ¡å’ŒSSEæ¨é€
app/services/stream_service.py
"""
import json
import asyncio
from typing import AsyncGenerator, List, Dict, Any
from app.models import MessageType, MessageStatus
from app.db.database import get_db_session
from app.crud import message as crud_message
from app.utils.cache_helper import set_cache, get_cache, delete_cache
from app.utils.message_helper import reconstruct_content_from_events
from app.core.logger import get_logger
from app.services.llm_service import llm_service
from app.services.workflow_service import workflow_service
from app.services.file_service import file_service
from app.services.smart_qa_service import smart_qa_service

logger = get_logger(__name__)


async def background_generate_task(
    message_id: int,
    conversation_id: int,
    user_id: int,
    user_query: str,
    mode: str,
    attachments: List[Dict],
    is_first_conversation: bool = False
):
    """åå°ç”Ÿæˆä»»åŠ¡ - ç‹¬ç«‹è¿è¡Œï¼Œä¸å—SSEæ–­å¼€å½±å“"""
    
    cache_key = f"message:{message_id}"
    events = []  # å­˜å‚¨æ‰€æœ‰äº‹ä»¶
    
    try:
        # è®¾ç½®åˆå§‹çŠ¶æ€
        await set_cache(f"{cache_key}:status", "generating")
        await set_cache(f"{cache_key}:events", json.dumps([], ensure_ascii=False))
        
        logger.info(f"å¼€å§‹ç”Ÿæˆæ¶ˆæ¯ {message_id}, æ¨¡å¼: {mode}")
        
        if mode == "multi_source":
            # å¤šæºæ£€ç´¢å·¥ä½œæµ
            async for output in workflow_service.execute_with_streaming(
                conversation_id=conversation_id,
                user_id=user_id,
                user_query=user_query,
                message_id=message_id,  # ä¼ é€’ message_id
                user_attachments=attachments,
                is_first_conversation=is_first_conversation
            ):
                events.append(output)
                # å®æ—¶æ›´æ–°ç¼“å­˜
                await set_cache(
                    f"{cache_key}:events",
                    json.dumps(events, ensure_ascii=False)
                )
        
        elif mode == "smart_qa":
            # æ™ºèƒ½é—®ç­”æ¨¡å¼ï¼ˆåŸºäºå†å²ä¸Šä¸‹æ–‡ï¼‰
            async with get_db_session() as db:
                history_messages = await crud_message.get_messages_by_conversation(
                    db,
                    conversation_id=conversation_id,
                    user_id=user_id
                ) or []
            
            # åŸºäºå†å²ä¸Šä¸‹æ–‡å›ç­”
            answer = await smart_qa_service.answer_with_history_context(
                user_query=user_query,
                conversation_id=conversation_id,
                history_messages=history_messages
            )
            
            # æµå¼è¾“å‡ºå›ç­”
            for char in answer:
                events.append({
                    'type': 'token',
                    'content': char
                })
                await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
                await asyncio.sleep(0.01)  # æ§åˆ¶è¾“å‡ºé€Ÿåº¦
        
        elif mode == "attachment":
            # é™„ä»¶æ¨¡å¼
            if not attachments:
                raise ValueError("é™„ä»¶æ¨¡å¼ä½†æœªæä¾›é™„ä»¶")
            
            # å¤„ç†é™„ä»¶äº‹ä»¶
            events.append({
                'type': 'log',
                'content': 'ğŸ“ æ­£åœ¨å¤„ç†é™„ä»¶ï¼Œä¸Šä¼ åˆ°é˜¿é‡Œå¹³å°...\n',
                'source': 'attachment',
                'newline': True
            })
            await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
            
            for idx, att in enumerate(attachments, 1):
                events.append({
                    'type': 'log',
                    'content': f'  [{idx}/{len(attachments)}] æ­£åœ¨ä¸Šä¼ : {att.get("original_filename", "æœªçŸ¥æ–‡ä»¶")}...\n',
                    'source': 'attachment',
                    'newline': True
                })
                await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
            
            file_ids, only_images = await file_service.process_attachments(attachments)
            
            if not file_ids:
                raise ValueError("é™„ä»¶å¤„ç†å¤±è´¥")
            
            events.append({
                'type': 'log',
                'content': 'âœ… é™„ä»¶ä¸Šä¼ å®Œæˆï¼Œå¼€å§‹åˆ†æ...\n',
                'source': 'attachment',
                'newline': True
            })
            await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
            
            # ä½¿ç”¨LLMåˆ†æ
            full_response = ""
            
            if only_images and len(file_ids) == 1:
                # å•å¼ å›¾ç‰‡ç”¨VLæ¨¡å‹
                image_att = attachments[0]
                # è·å–å†å²æ¶ˆæ¯ç”¨äºä¸Šä¸‹æ–‡
                async with get_db_session() as db:
                    history_messages = await crud_message.get_messages_by_conversation(
                        db,
                        conversation_id=conversation_id,
                        user_id=user_id
                    ) or []
                
                # æ„å»ºå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ŒæŒ‰ç…§è¦æ±‚çš„æ ¼å¼å¤„ç†é™„ä»¶
                history_context = []
                for msg in history_messages:
                    # æ·»åŠ ç”¨æˆ·æˆ–åŠ©æ‰‹æ¶ˆæ¯
                    if msg["message_type"] == "user":
                        history_context.append({"role": "user", "content": msg["content"]})
                    elif msg["message_type"] == "assistant":
                        history_context.append({"role": "assistant", "content": msg["content"]})
                    
                    # å¦‚æœæ¶ˆæ¯æœ‰é™„ä»¶ï¼ŒæŒ‰ç…§è¦æ±‚æ ¼å¼æ·»åŠ systemæ¶ˆæ¯
                    if msg.get("attachments"):
                        file_ids_context = []
                        for att in msg["attachments"]:
                            # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºfileidï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä¿å­˜çœŸå®çš„file_idï¼‰
                            file_ids_context.append(f"fileid://{att['filename']}")
                        if file_ids_context:
                            history_context.append({"role": "system", "content": ",".join(file_ids_context)})
                
                async for token in llm_service.chat_with_image_stream(
                    text=user_query,
                    image_path=image_att['file_path'],
                    history=history_context if history_context else None
                ):
                    full_response += token
                    events.append({
                        'type': 'token',
                        'content': token
                    })
                    await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
            else:
                # å¤šä¸ªæ–‡ä»¶æˆ–åŒ…å«æ–‡æ¡£
                # è·å–å†å²æ¶ˆæ¯ç”¨äºä¸Šä¸‹æ–‡
                async with get_db_session() as db:
                    history_messages = await crud_message.get_messages_by_conversation(
                        db,
                        conversation_id=conversation_id,
                        user_id=user_id
                    ) or []
                
                # æ„å»ºå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ŒæŒ‰ç…§è¦æ±‚çš„æ ¼å¼å¤„ç†é™„ä»¶
                history_context = []
                for msg in history_messages:
                    # æ·»åŠ ç”¨æˆ·æˆ–åŠ©æ‰‹æ¶ˆæ¯
                    if msg["message_type"] == "user":
                        history_context.append({"role": "user", "content": msg["content"]})
                    elif msg["message_type"] == "assistant":
                        history_context.append({"role": "assistant", "content": msg["content"]})
                    
                    # å¦‚æœæ¶ˆæ¯æœ‰é™„ä»¶ï¼ŒæŒ‰ç…§è¦æ±‚æ ¼å¼æ·»åŠ systemæ¶ˆæ¯
                    if msg.get("attachments"):
                        file_ids_context = []
                        for att in msg["attachments"]:
                            # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºfileidï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä¿å­˜çœŸå®çš„file_idï¼‰
                            file_ids_context.append(f"fileid://{att['filename']}")
                        if file_ids_context:
                            history_context.append({"role": "system", "content": ",".join(file_ids_context)})
                
                async for token in llm_service.chat_with_context(
                    user_query=user_query,
                    history=history_context if history_context else None,
                    file_ids=file_ids,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚"
                ):
                    full_response += token
                    events.append({
                        'type': 'token',
                        'content': token
                    })
                    await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
        
        else:
            # æ™®é€šæ¨¡å¼
            # è·å–å†å²æ¶ˆæ¯ç”¨äºä¸Šä¸‹æ–‡
            async with get_db_session() as db:
                history_messages = await crud_message.get_messages_by_conversation(
                    db,
                    conversation_id=conversation_id,
                    user_id=user_id
                ) or []
            
            # æ„å»ºå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ŒæŒ‰ç…§è¦æ±‚çš„æ ¼å¼å¤„ç†é™„ä»¶
            history_context = []
            for msg in history_messages:
                # æ·»åŠ ç”¨æˆ·æˆ–åŠ©æ‰‹æ¶ˆæ¯
                if msg["message_type"] == "user":
                    history_context.append({"role": "user", "content": msg["content"]})
                elif msg["message_type"] == "assistant":
                    history_context.append({"role": "assistant", "content": msg["content"]})
                
                # å¦‚æœæ¶ˆæ¯æœ‰é™„ä»¶ï¼ŒæŒ‰ç…§è¦æ±‚æ ¼å¼æ·»åŠ systemæ¶ˆæ¯
                if msg.get("attachments"):
                    file_ids_context = []
                    for att in msg["attachments"]:
                        # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºfileidï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä¿å­˜çœŸå®çš„file_idï¼‰
                        file_ids_context.append(f"fileid://{att['filename']}")
                    if file_ids_context:
                        history_context.append({"role": "system", "content": ",".join(file_ids_context)})
            
            async for token in llm_service.chat_with_context(
                user_query=user_query,
                history=history_context if history_context else None,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"
            ):
                events.append({
                    'type': 'token',
                    'content': token
                })
                await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
        
        # ç”Ÿæˆå®Œæˆ
        await set_cache(f"{cache_key}:status", "completed")
        
        # å¤šæºæ£€ç´¢æ¨¡å¼å·²ç»åœ¨ workflow_service ä¸­ä¿å­˜ç»“æœï¼Œä¸éœ€è¦é‡å¤ä¿å­˜
        if mode != "multi_source":
            # é‡å»ºå®Œæ•´å†…å®¹ç”¨äºæŒä¹…åŒ–
            final_content = reconstruct_content_from_events(events)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            async with get_db_session() as db:
                await crud_message.update_message(
                    db,
                    message_id=message_id,
                    content=final_content,
                    status=MessageStatus.COMPLETED
                )
                
                # æ›´æ–°æ¶ˆæ¯å…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if events:
                    # æŸ¥æ‰¾äº‹ä»¶ä¸­çš„å…ƒæ•°æ®
                    metadata_events = [event for event in events if event.get('type') == 'metadata']
                    if metadata_events:
                        # åˆå¹¶æ‰€æœ‰å…ƒæ•°æ®äº‹ä»¶
                        metadata = {}
                        for event in metadata_events:
                            if isinstance(event.get('data'), dict):
                                metadata.update(event['data'])
                        
                        # æ›´æ–°æ•°æ®åº“ä¸­çš„å…ƒæ•°æ®
                        from sqlalchemy import update
                        from app.models import Message
                        await db.execute(
                            update(Message)
                            .where(Message.id == message_id)
                            .values(metadata_json=json.dumps(metadata, ensure_ascii=False))
                        )
                        await db.commit()
                
                # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œå°è¯•ç”Ÿæˆæ ‡é¢˜
                if is_first_conversation and final_content.strip():
                    # è·å–ç”¨æˆ·æ¶ˆæ¯å†…å®¹ç”¨äºç”Ÿæˆæ ‡é¢˜
                    user_message = await crud_message.get_message_by_id(db, message_id-1)  # å‡è®¾ç”¨æˆ·æ¶ˆæ¯IDæ˜¯AIæ¶ˆæ¯ID-1
                    if user_message and user_message.message_type == MessageType.USER:
                        user_query = user_message.content
                        ai_response = final_content
                        
                        # åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿæˆæ ‡é¢˜
                        from app.api.v1.chat import should_generate_title, generate_conversation_title
                        if await should_generate_title(user_query, ai_response):
                            new_title = await generate_conversation_title(user_query, ai_response)
                            
                            # æ›´æ–°ä¼šè¯æ ‡é¢˜
                            from app.schemas.conversation import ConversationUpdateSchema
                            from app.crud import conversation as crud_conversation
                            await crud_conversation.update_conversation(
                                db,
                                conversation_id=conversation_id,
                                conversation_schema=ConversationUpdateSchema(title=new_title),
                                user_id=user_id
                            )
                            
                            # æ¨é€æ ‡é¢˜æ›´æ–°äº‹ä»¶
                            events.append({
                                'type': 'title_updated',
                                'conversation_id': conversation_id,
                                'title': new_title
                            })
                            await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
                            logger.info(f"å¯¹è¯å·²è‡ªåŠ¨é‡å‘½åä¸ºã€Œ{new_title}ã€")
        
        logger.info(f"æ¶ˆæ¯ {message_id} ç”Ÿæˆå®Œæˆ")
        
        # å»¶è¿Ÿæ¸…é™¤ç¼“å­˜ï¼ˆç»™å‰ç«¯æ—¶é—´è·å–æœ€ç»ˆçŠ¶æ€ï¼‰
        await asyncio.sleep(10)
        await delete_cache(f"{cache_key}:status")
        await delete_cache(f"{cache_key}:events")
        
    except Exception as e:
        logger.error(f"æ¶ˆæ¯ {message_id} ç”Ÿæˆå¤±è´¥: {e}")
        await set_cache(f"{cache_key}:status", "failed")
        
        # å¤šæºæ£€ç´¢æ¨¡å¼å·²ç»åœ¨ workflow_service ä¸­ä¿å­˜é”™è¯¯ç»“æœï¼Œä¸éœ€è¦é‡å¤æ›´æ–°
        if mode != "multi_source":
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            async with get_db_session() as db:
                await crud_message.update_message_status(
                    db,
                    message_id=message_id,
                    status=MessageStatus.FAILED
                )


async def stream_events(message_id: int) -> AsyncGenerator[str, None]:
    """ç»Ÿä¸€çš„SSEäº‹ä»¶æµç”Ÿæˆå™¨ï¼ˆé¦–æ¬¡è¿æ¥å’Œæ–­çº¿é‡è¿å…±ç”¨ï¼‰"""
    
    cache_key = f"message:{message_id}"
    last_sent_index = -1  # å·²å‘é€çš„äº‹ä»¶ç´¢å¼•
    
    while True:
        # æ£€æŸ¥çŠ¶æ€
        status = await get_cache(f"{cache_key}:status")
        
        if status == "failed":
            yield f"data: {json.dumps({'type': 'error', 'content': 'ç”Ÿæˆå¤±è´¥'}, ensure_ascii=False)}\n\n"
            break
        
        if status == "completed":
            # å‘é€å‰©ä½™äº‹ä»¶åç»“æŸ
            events_json = await get_cache(f"{cache_key}:events")
            if events_json:
                events = json.loads(events_json)
                for i in range(last_sent_index + 1, len(events)):
                    yield f"data: {json.dumps(events[i], ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
            break
        
        # è·å–äº‹ä»¶æ•°ç»„
        events_json = await get_cache(f"{cache_key}:events")
        
        if not events_json:
            await asyncio.sleep(0.05)
            continue
        
        events = json.loads(events_json)
        
        # æ¨é€æ–°å¢çš„äº‹ä»¶ï¼ˆå¢é‡ï¼‰
        for i in range(last_sent_index + 1, len(events)):
            yield f"data: {json.dumps(events[i], ensure_ascii=False)}\n\n"
            last_sent_index = i
        
        await asyncio.sleep(0.05)  # è½®è¯¢é—´éš”
