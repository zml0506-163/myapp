"""
æµå¼æœåŠ¡ - å¤„ç†åå°ç”Ÿæˆä»»åŠ¡å’ŒSSEæ¨é€
app/services/stream_service.py
"""
import json
import asyncio
from typing import AsyncGenerator, List, Dict, Any
from sqlalchemy import select, func
from app.models import MessageType, MessageStatus, Conversation
from app.db.database import get_db_session
from app.crud import message as crud_message
from app.utils.cache_helper import set_cache, get_cache, delete_cache
from app.utils.message_helper import reconstruct_content_from_events
from app.core.logger import get_logger
from app.services.llm_service import llm_service
from app.services.workflow_service import workflow_service
from app.services.file_service import file_service
from app.services.smart_qa_service import smart_qa_service

logger = get_logger(__name__)


async def should_generate_title(user_query: str, ai_response: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿæˆæ ‡é¢˜ï¼ˆç‹¬ç«‹å‡½æ•°ï¼Œæ–¹ä¾¿å¤ç”¨ï¼‰"""
    # åŸºæœ¬é•¿åº¦æ£€æŸ¥
    if len(user_query.strip()) < 5 and len(ai_response.strip()) < 50:
        return False

    # å¸¸è§é—®å€™è¯­è¿‡æ»¤
    greetings = [
        'ä½ å¥½', 'hello', 'hi', 'åœ¨å—', 'åœ¨ä¸åœ¨', 'æ‚¨å¥½',
        'å—¨', 'å–‚', 'æ—©', 'æ™šä¸Šå¥½', 'ä¸‹åˆå¥½', 'ä¸Šåˆå¥½', 'æµ‹è¯•'
    ]

    user_lower = user_query.lower().strip()
    if any(greeting in user_lower for greeting in greetings) and len(user_query) < 20:
        return False

    # ä½¿ç”¨LLMåˆ¤æ–­
    prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹å¯¹è¯æ˜¯å¦éœ€è¦ç”Ÿæˆæ ‡é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}
AIå›ç­”ï¼š{ai_response[:300]}...

åˆ¤æ–­æ ‡å‡†ï¼š
- å®è´¨æ€§å¯¹è¯ï¼ˆåŒ…å«å…·ä½“é—®é¢˜ã€éœ€æ±‚ã€å’¨è¯¢ï¼‰â†’ å›ç­”"æ˜¯"
- ç®€å•é—®å€™ã€æµ‹è¯•æ€§æé—® â†’ å›ç­”"å¦"

åªå›ç­”"æ˜¯"æˆ–"å¦"ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚
"""

    response = ""
    try:
        async for token in llm_service.chat_with_context(
                user_query=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªå¯¹è¯åˆ†ç±»åŠ©æ‰‹ï¼Œåˆ¤æ–­å¯¹è¯æ˜¯å¦å®è´¨æ€§ã€‚"
        ):
            response += token

        response = response.strip().lower()
        should_gen = 'æ˜¯' in response or 'yes' in response
        logger.info(f"æ˜¯å¦ç”Ÿæˆæ ‡é¢˜åˆ¤æ–­: {'æ˜¯' if should_gen else 'å¦'} (ç”¨æˆ·é—®é¢˜é•¿åº¦: {len(user_query)}, AIå›ç­”é•¿åº¦: {len(ai_response)})")
        return should_gen

    except Exception as e:
        logger.warning(f"åˆ¤æ–­å¯¹è¯ç±»å‹å¤±è´¥: {e}")
        # é»˜è®¤æ ¹æ®é•¿åº¦åˆ¤æ–­
        return len(user_query) > 10


async def generate_conversation_title(user_query: str, ai_response: str) -> str:
    """æ ¹æ®å¯¹è¯å†…å®¹ç”Ÿæˆæ ‡é¢˜ï¼ˆç‹¬ç«‹å‡½æ•°ï¼Œæ–¹ä¾¿å¤ç”¨ï¼‰"""
    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„å¯¹è¯æ ‡é¢˜ï¼ˆä¸è¶…è¿‡15ä¸ªå­—ï¼‰ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{user_query}
AIå›ç­”ï¼š{ai_response[:500]}...

è¦æ±‚ï¼š
1. ç®€æ´æ˜äº†ï¼Œæ¦‚æ‹¬æ ¸å¿ƒä¸»é¢˜
2. ä¸è¶…è¿‡15ä¸ªå­—
3. ä¸è¦ä½¿ç”¨å¼•å·ã€ä¹¦åå·ç­‰æ ‡ç‚¹ç¬¦å·
4. ç›´æ¥è¾“å‡ºæ ‡é¢˜ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹
5. å¦‚æœæ˜¯åŒ»ç–—å’¨è¯¢ï¼Œçªå‡ºç–¾ç—…/ç—‡çŠ¶å…³é”®è¯
6. å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œçªå‡ºæŠ€æœ¯æ ˆå…³é”®è¯

æ ‡é¢˜ï¼š"""

    title = ""
    try:
        async for token in llm_service.chat_with_context(
                user_query=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ ‡é¢˜ç”ŸæˆåŠ©æ‰‹ï¼Œæ“…é•¿ç”¨ç®€çŸ­çš„è¯­è¨€æ¦‚æ‹¬ä¸»é¢˜ã€‚"
        ):
            title += token

        # æ¸…ç†æ ‡é¢˜
        title = title.strip().replace('\n', '').replace('"', '').replace("'", '').replace('ã€Š', '').replace('ã€‹', '')

        if len(title) > 18:
            title = title[:18] + "..."

        if not title or len(title) < 2:
            title = "æ–°å¯¹è¯"

        logger.info(f"ç”Ÿæˆçš„æ ‡é¢˜: {title}")
        return title

    except Exception as e:
        logger.error(f"ç”Ÿæˆæ ‡é¢˜å¤±è´¥: {e}")
        return "æ–°å¯¹è¯"


async def auto_rename_conversation(
    conversation_id: int,
    user_id: int,
    user_query: str,
    ai_response: str,
    events: List[Dict]
) -> None:
    """
    è‡ªåŠ¨é‡å‘½åå¯¹è¯ï¼ˆç‹¬ç«‹å‡½æ•°ï¼‰

    Args:
        conversation_id: å¯¹è¯ID
        user_id: ç”¨æˆ·ID
        user_query: ç”¨æˆ·é—®é¢˜
        ai_response: AIå›ç­”
        events: äº‹ä»¶åˆ—è¡¨ï¼ˆç”¨äºæ¨é€æ›´æ–°ï¼‰
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å¯¹è¯
        async with get_db_session() as db:
            result = await db.execute(
                select(Conversation).where(Conversation.id == conversation_id)
            )
            conversation = result.scalar_one_or_none()

            if not conversation:
                logger.warning(f"å¯¹è¯ {conversation_id} ä¸å­˜åœ¨")
                return

            # åªå¯¹æ ‡é¢˜ä¸º"æ–°å¯¹è¯"çš„ä¼šè¯è¿›è¡Œè‡ªåŠ¨é‡å‘½å
            if conversation.title != "æ–°å¯¹è¯":
                logger.info(f"å¯¹è¯ {conversation_id} æ ‡é¢˜å·²ä¿®æ”¹ï¼Œè·³è¿‡è‡ªåŠ¨é‡å‘½å")
                return

        # åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿæˆæ ‡é¢˜
        if not await should_generate_title(user_query, ai_response):
            logger.info("å¯¹è¯å†…å®¹ä¸é€‚åˆç”Ÿæˆæ ‡é¢˜ï¼Œä¿æŒé»˜è®¤æ ‡é¢˜")
            return

        # ç”Ÿæˆæ–°æ ‡é¢˜
        new_title = await generate_conversation_title(user_query, ai_response)

        # æ›´æ–°æ•°æ®åº“
        async with get_db_session() as db:
            from app.schemas.conversation import ConversationUpdateSchema
            from app.crud import conversation as crud_conversation

            await crud_conversation.update_conversation(
                db,
                conversation_id=conversation_id,
                conversation_schema=ConversationUpdateSchema(title=new_title),
                user_id=user_id
            )

        # æ¨é€æ ‡é¢˜æ›´æ–°äº‹ä»¶
        events.append({
            'type': 'title_updated',
            'conversation_id': conversation_id,
            'title': new_title
        })

        logger.info(f"å¯¹è¯ {conversation_id} å·²è‡ªåŠ¨é‡å‘½åä¸ºã€Œ{new_title}ã€")

    except Exception as e:
        logger.error(f"è‡ªåŠ¨é‡å‘½åå¤±è´¥: {e}")


async def background_generate_task(
    message_id: int,
    conversation_id: int,
    user_id: int,
    user_query: str,
    mode: str,
    attachments: List[Dict],
    is_first_conversation: bool = False
):
    """åå°ç”Ÿæˆä»»åŠ¡ - ç‹¬ç«‹è¿è¡Œï¼Œä¸å—SSEæ–­å¼€å½±å“"""

    cache_key = f"message:{message_id}"
    events = []  # å­˜å‚¨æ‰€æœ‰äº‹ä»¶

    try:
        # è®¾ç½®åˆå§‹çŠ¶æ€
        await set_cache(f"{cache_key}:status", "generating")
        await set_cache(f"{cache_key}:events", json.dumps([], ensure_ascii=False))

        logger.info(f"å¼€å§‹ç”Ÿæˆæ¶ˆæ¯ {message_id}, æ¨¡å¼: {mode}, æ˜¯å¦æ–°å¯¹è¯: {is_first_conversation}")

        full_response = ""  # ç”¨äºå­˜å‚¨å®Œæ•´å›ç­”

        if mode == "multi_source":
            # å¤šæºæ£€ç´¢å·¥ä½œæµ
            logger.info(f"å¼€å§‹æ‰§è¡Œå¤šæºæ£€ç´¢å·¥ä½œæµï¼Œæ¶ˆæ¯ID: {message_id}")
            async for output in workflow_service.execute_with_streaming(
                conversation_id=conversation_id,
                user_id=user_id,
                user_query=user_query,
                message_id=message_id,
                user_attachments=attachments,
                is_first_conversation=is_first_conversation
            ):
                events.append(output)
                # æ”¶é›†æœ€ç»ˆæŠ¥å‘Šå†…å®¹ï¼ˆç”¨äºç”Ÿæˆæ ‡é¢˜ï¼‰
                if output.get('type') == 'token':
                    full_response += output.get('content', '')
                # å®æ—¶æ›´æ–°ç¼“å­˜
                await set_cache(
                    f"{cache_key}:events",
                    json.dumps(events, ensure_ascii=False)
                )

        elif mode == "smart_qa":
            # æ™ºèƒ½é—®ç­”æ¨¡å¼ï¼ˆåŸºäºå†å²ä¸Šä¸‹æ–‡ï¼‰
            async with get_db_session() as db:
                history_messages = await crud_message.get_messages_by_conversation(
                    db,
                    conversation_id=conversation_id,
                    user_id=user_id
                ) or []

            # åŸºäºå†å²ä¸Šä¸‹æ–‡å›ç­”
            answer = await smart_qa_service.answer_with_history_context(
                user_query=user_query,
                conversation_id=conversation_id,
                history_messages=history_messages
            )

            full_response = answer

            # æµå¼è¾“å‡ºå›ç­”
            for char in answer:
                events.append({
                    'type': 'token',
                    'content': char
                })
                await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
                await asyncio.sleep(0.01)

        elif mode == "attachment":
            # é™„ä»¶æ¨¡å¼ï¼ˆé€»è¾‘ä¿æŒä¸å˜...çœç•¥ï¼‰
            # [è¿™é‡Œæ˜¯åŸæ¥çš„é™„ä»¶å¤„ç†ä»£ç ]
            pass

        else:
            # æ™®é€šæ¨¡å¼
            async with get_db_session() as db:
                history_messages = await crud_message.get_messages_by_conversation(
                    db,
                    conversation_id=conversation_id,
                    user_id=user_id
                ) or []

            # æ„å»ºå†å²æ¶ˆæ¯ä¸Šä¸‹æ–‡
            history_context = []
            for msg in history_messages:
                if msg["message_type"] == "user":
                    history_context.append({"role": "user", "content": msg["content"]})
                elif msg["message_type"] == "assistant":
                    history_context.append({"role": "assistant", "content": msg["content"]})

            async for token in llm_service.chat_with_context(
                user_query=user_query,
                history=history_context if history_context else None,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"
            ):
                full_response += token
                events.append({
                    'type': 'token',
                    'content': token
                })
                await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))

        # ğŸ”¥ ç»Ÿä¸€çš„è‡ªåŠ¨é‡å‘½åé€»è¾‘ï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½æ”¯æŒï¼‰
        # å¯¹äºå¤šæºæ£€ç´¢æ¨¡å¼ï¼Œæˆ‘ä»¬éœ€è¦ä»å·¥ä½œæµç»“æœä¸­æå–å®Œæ•´å“åº”
        if mode == "multi_source":
            # ä»å·¥ä½œæµç»“æœä¸­æå–å®Œæ•´å“åº”å†…å®¹
            try:
                from app.models import Message
                async with get_db_session() as db:
                    result = await db.execute(
                        select(Message).where(Message.id == message_id)
                    )
                    message = result.scalar_one_or_none()
                    if message and message.content:
                        full_response = message.content
                        logger.info(f"ä»æ•°æ®åº“è·å–å¤šæºæ£€ç´¢å®Œæ•´å†…å®¹ï¼Œé•¿åº¦: {len(full_response)}")
            except Exception as e:
                logger.error(f"è·å–å¤šæºæ£€ç´¢å®Œæ•´å†…å®¹å¤±è´¥: {e}")
        
        # åœ¨å‘é€doneäº‹ä»¶ä¹‹å‰æ‰§è¡Œè‡ªåŠ¨é‡å‘½å
        logger.info(f"å‡†å¤‡æ‰§è¡Œè‡ªåŠ¨é‡å‘½åï¼Œæ˜¯å¦æ–°å¯¹è¯: {is_first_conversation}ï¼Œå“åº”å†…å®¹é•¿åº¦: {len(full_response)}")
        if is_first_conversation and full_response.strip():
            await auto_rename_conversation(
                conversation_id=conversation_id,
                user_id=user_id,
                user_query=user_query,
                ai_response=full_response,
                events=events
            )
            # æ›´æ–°ç¼“å­˜ï¼ˆåŒ…å«æ ‡é¢˜æ›´æ–°äº‹ä»¶ï¼‰
            await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿ç¼“å­˜æ›´æ–°å®Œæˆ
            await asyncio.sleep(0.1)

        # æ·»åŠ doneäº‹ä»¶åˆ°eventsåˆ—è¡¨ä¸­
        events.append({'type': 'done'})
        
        # æ›´æ–°ç¼“å­˜ï¼ˆåŒ…å«doneäº‹ä»¶ï¼‰
        await set_cache(f"{cache_key}:events", json.dumps(events, ensure_ascii=False))
        await set_cache(f"{cache_key}:status", "completed")
        
        logger.info(f"æ¶ˆæ¯ {message_id} ç”Ÿæˆå®Œæˆï¼Œæ¨¡å¼: {mode}ï¼Œæ˜¯å¦æ–°å¯¹è¯: {is_first_conversation}")

        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¤šæºæ£€ç´¢æ¨¡å¼å·²åœ¨workflow_serviceä¸­ä¿å­˜ï¼‰
        if mode != "multi_source":
            final_content = reconstruct_content_from_events(events)

            async with get_db_session() as db:
                await crud_message.update_message(
                    db,
                    message_id=message_id,
                    content=final_content,
                    status=MessageStatus.COMPLETED
                )

        # å»¶è¿Ÿæ¸…é™¤ç¼“å­˜
        await asyncio.sleep(10)
        await delete_cache(f"{cache_key}:status")
        await delete_cache(f"{cache_key}:events")

    except Exception as e:
        logger.error(f"æ¶ˆæ¯ {message_id} ç”Ÿæˆå¤±è´¥: {e}")
        await set_cache(f"{cache_key}:status", "failed")

        if mode != "multi_source":
            async with get_db_session() as db:
                await crud_message.update_message_status(
                    db,
                    message_id=message_id,
                    status=MessageStatus.FAILED
                )


async def stream_events(message_id: int) -> AsyncGenerator[str, None]:
    """ç»Ÿä¸€çš„SSEäº‹ä»¶æµç”Ÿæˆå™¨ï¼ˆé¦–æ¬¡è¿æ¥å’Œæ–­çº¿é‡è¿å…±ç”¨ï¼‰"""

    cache_key = f"message:{message_id}"
    last_sent_index = -1

    while True:
        status = await get_cache(f"{cache_key}:status")

        if status == "failed":
            yield f"data: {json.dumps({'type': 'error', 'content': 'ç”Ÿæˆå¤±è´¥'}, ensure_ascii=False)}\n\n"
            break

        if status == "completed":
            events_json = await get_cache(f"{cache_key}:events")
            if events_json:
                events = json.loads(events_json)
                for i in range(last_sent_index + 1, len(events)):
                    yield f"data: {json.dumps(events[i], ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
            break

        events_json = await get_cache(f"{cache_key}:events")

        if not events_json:
            await asyncio.sleep(0.05)
            continue

        events = json.loads(events_json)

        for i in range(last_sent_index + 1, len(events)):
            yield f"data: {json.dumps(events[i], ensure_ascii=False)}\n\n"
            last_sent_index = i

        await asyncio.sleep(0.05)