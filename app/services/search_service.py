"""
ä¼˜åŒ–çš„å¤šæºæ£€ç´¢æœåŠ¡
app/services/search_service.py
"""
import asyncio
from typing import List, Dict, Set, Optional
from concurrent.futures import ThreadPoolExecutor
from sqlalchemy import select, or_, and_
import difflib

from app.core.config import settings
from app.db.database import get_db_session
from app.models import Paper, ClinicalTrial
from app.db.crud import upsert_paper, upsert_clinical_trial

from app.tools.pubmed_client import pubmed_client
from app.tools.europepmc_client import search_europe_pmc
from app.tools.clinical_trials_client import async_search_trials


class SearchProgress:
    """è¿›åº¦å›è°ƒå°è£…"""
    def __init__(self, queue: asyncio.Queue, source: str):
        self.queue = queue
        self.source = source
        self.loop = asyncio.get_running_loop()

    def callback(self, message: str, newline: bool = True):
        """åŒæ­¥å›è°ƒ"""
        asyncio.run_coroutine_threadsafe(
            self.queue.put({
                'type': 'log',
                'source': self.source,
                'content': message,
                'newline': newline
            }),
            self.loop
        )


class SearchService:
    """ä¼˜åŒ–çš„å¤šæºæ£€ç´¢æœåŠ¡"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=settings.max_concurrent_downloads)

    def _calculate_relevance(self, query: str, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸æŸ¥è¯¢çš„ç›¸å…³åº¦ï¼ˆ0-100åˆ†ï¼‰
        """
        if not text:
            return 0.0

        query_terms = set(query.lower().replace('and', '').replace('or', '').split())
        query_terms = {term.strip() for term in query_terms if len(term.strip()) > 2}

        if not query_terms:
            return 50.0

        text_lower = text.lower()
        matches = sum(1 for term in query_terms if term in text_lower)
        score = (matches / len(query_terms)) * 100

        return min(score, 100.0)

    def _deduplicate_papers(self, papers: List[Dict]) -> List[Dict]:
        """
        å»é‡æ–‡çŒ®ï¼ˆåŸºäºPMIDã€PMCIDæˆ–æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
        """
        seen_ids: Set[str] = set()
        seen_titles: List[str] = []
        unique_papers = []

        for paper in papers:
            if paper.get('pmid') and paper['pmid'] in seen_ids:
                continue

            if paper.get('pmcid') and paper['pmcid'] in seen_ids:
                continue

            title = paper.get('title', '')
            is_duplicate = False
            for seen_title in seen_titles:
                similarity = difflib.SequenceMatcher(None, title.lower(), seen_title.lower()).ratio()
                if similarity > 0.9:
                    is_duplicate = True
                    break

            if is_duplicate:
                continue

            if paper.get('pmid'):
                seen_ids.add(paper['pmid'])
            if paper.get('pmcid'):
                seen_ids.add(paper['pmcid'])
            seen_titles.append(title)
            unique_papers.append(paper)

        return unique_papers

    async def search_papers_with_ranking(
            self,
            query: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """
        æ£€ç´¢å¹¶æ’åºæ–‡çŒ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        ç­–ç•¥ï¼š
        1. å…ˆä»æ•°æ®åº“æŸ¥ç¼“å­˜
        2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢ï¼ˆæ£€ç´¢æ›´å¤šï¼‰
        3. å»é‡å¹¶æŒ‰ç›¸å…³åº¦æ’åº
        4. è¿”å›å‰ N ç¯‡
        """
        # 1. æŸ¥è¯¢ç¼“å­˜
        cached_papers = await self._search_cached_papers(query, target_count * settings.search_multiplier, progress_queue)

        all_papers = cached_papers.copy()

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢
        if len(all_papers) < target_count * settings.search_multiplier:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'\nğŸ” å¼€å§‹æ£€ç´¢ PubMed å’Œ Europe PMC...\n',
                'newline': True
            })

            # å¹¶å‘æ£€ç´¢ PubMed å’Œ Europe PMC
            pubmed_task = asyncio.create_task(
                self._fetch_pubmed_papers(query, target_count, progress_queue)
            )
            europepmc_task = asyncio.create_task(
                self._fetch_europepmc_papers(query, target_count, progress_queue)
            )

            pubmed_papers, europepmc_papers = await asyncio.gather(
                pubmed_task,
                europepmc_task,
                return_exceptions=True
            )

            if not isinstance(pubmed_papers, Exception):
                all_papers.extend(pubmed_papers)
            if not isinstance(europepmc_papers, Exception):
                all_papers.extend(europepmc_papers)

        # 3. å»é‡
        all_papers = self._deduplicate_papers(all_papers)

        await progress_queue.put({
            'type': 'log',
            'source': 'dedup',
            'content': f'ğŸ”„ å»é‡åå…± {len(all_papers)} ç¯‡æ–‡çŒ®\n',
            'newline': True
        })

        # 4. è®¡ç®—ç›¸å…³åº¦å¹¶æ’åº
        for paper in all_papers:
            title_score = self._calculate_relevance(query, paper.get('title', ''))
            abstract_score = self._calculate_relevance(query, paper.get('abstract', ''))
            paper['relevance_score'] = (title_score * 0.7 + abstract_score * 0.3)

        all_papers.sort(key=lambda p: p.get('relevance_score', 0), reverse=True)

        # 5. è¿”å›å‰ N ç¯‡
        selected_papers = all_papers[:target_count]

        await progress_queue.put({
            'type': 'result',
            'source': 'papers',
            'content': f'âœ… ç­›é€‰å‡ºæœ€ç›¸å…³çš„ {len(selected_papers)} ç¯‡æ–‡çŒ®',
            'data': {
                'count': len(selected_papers),
                'papers': selected_papers
            }
        })

        return selected_papers

    async def _search_cached_papers(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """ä»æ•°æ®åº“æŸ¥è¯¢ç¼“å­˜çš„æ–‡çŒ®"""
        cached_papers = []

        async with get_db_session() as db:
            search_terms = query.replace('AND', '').replace('OR', '').split()[:5]
            if search_terms:
                query_filter = select(Paper).where(
                    and_(
                        or_(
                            Paper.source_type == 'pubmed',
                            Paper.source_type == 'europepmc'
                        ),
                        or_(*[Paper.title.ilike(f"%{term}%") for term in search_terms if len(term) > 2])
                    )
                ).limit(limit)

                result = await db.execute(query_filter)
                cached = result.scalars().all()

                if cached:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'cache',
                        'content': f'ğŸ“š æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached)} ç¯‡å·²ç¼“å­˜æ–‡çŒ®\n',
                        'newline': True
                    })

                    for paper in cached:
                        cached_papers.append(self._paper_to_dict(paper))

        return cached_papers

    async def _fetch_pubmed_papers(
            self,
            query: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """
        æ£€ç´¢ PubMedï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        æ”¹è¿›ï¼š
        1. ä½¿ç”¨é…ç½®çš„è¶…æ—¶å’Œå¹¶å‘æ§åˆ¶
        2. è¾¾åˆ°ç›®æ ‡æ•°é‡åç«‹å³åœæ­¢
        3. æ›´è¯¦ç»†çš„è¿›åº¦åé¦ˆ
        """
        results = []

        try:
            # æœç´¢ PMIDï¼ˆè·å–æ›´å¤šä»¥åº”å¯¹ä¸‹è½½å¤±è´¥ï¼‰
            pmids = await pubmed_client.esearch_pmids(
                query,
                retmax=settings.max_pmids_to_fetch
            )

            if not pmids:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'pubmed',
                    'content': 'âš ï¸ PubMed æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'ğŸ“¥ æ‰¾åˆ° {len(pmids)} ä¸ª PMIDï¼Œå‡†å¤‡ä¸‹è½½ PDF...\n',
                'newline': True
            })

            # è·å–å…ƒæ•°æ®
            meta = await pubmed_client.efetch_metadata(pmids)

            # å¹¶å‘ä¸‹è½½ PDFï¼ˆä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘ï¼‰
            async with get_db_session() as db:
                download_tasks = []

                for pid in pmids:
                    # è¾¾åˆ°ç›®æ ‡æ•°é‡ååœæ­¢
                    if len(results) >= target_count:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'pubmed',
                            'content': f'âœ… å·²è·å–è¶³å¤Ÿæ–‡çŒ®ï¼ˆ{target_count} ç¯‡ï¼‰ï¼Œåœæ­¢æ£€ç´¢\n',
                            'newline': True
                        })
                        break

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    result = await db.execute(
                        select(Paper).where(
                            Paper.pmid == pid,
                            Paper.source_type == 'pubmed'
                        )
                    )
                    existing = result.scalar_one_or_none()

                    if existing:
                        results.append(self._paper_to_dict(existing))
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'pubmed',
                            'content': f'  âœ“ PMID {pid} å·²å­˜åœ¨ç¼“å­˜\n',
                            'newline': True
                        })
                        continue

                    # åˆ›å»ºä¸‹è½½ä»»åŠ¡
                    task = self._download_and_save_paper(
                        pid,
                        meta.get(pid, {}),
                        progress_queue
                    )
                    download_tasks.append(task)

                # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
                if download_tasks:
                    papers = await asyncio.gather(*download_tasks, return_exceptions=True)

                    for paper in papers:
                        if paper and not isinstance(paper, Exception):
                            results.append(paper)

                            # è¾¾åˆ°ç›®æ ‡æ•°é‡ååœæ­¢
                            if len(results) >= target_count:
                                break

            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'âœ… PubMed æ£€ç´¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(results)} ç¯‡æ–‡çŒ®\n',
                'newline': True
            })

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'âŒ PubMed æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def _download_and_save_paper(
            self,
            pmid: str,
            metadata: Dict,
            progress_queue: asyncio.Queue
    ) -> Optional[Dict]:
        """ä¸‹è½½å¹¶ä¿å­˜å•ç¯‡æ–‡çŒ®"""
        try:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'  ğŸ“„ å¤„ç† PMID {pmid}...',
                'newline': False
            })

            # åˆ›å»ºè¿›åº¦å›è°ƒ
            progress = SearchProgress(progress_queue, 'pubmed')

            # ä½¿ç”¨ä¼˜åŒ–çš„å®¢æˆ·ç«¯ä¸‹è½½ï¼ˆå¸¦è¶…æ—¶å’Œå¹¶å‘æ§åˆ¶ï¼‰
            pdf_path = await pubmed_client.download_pdf_with_limit(
                pmid,
                metadata.get("pmcid"),
                self.executor,
                progress.callback
            )

            if not pdf_path:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'pubmed',
                    'content': ' âŒ è·³è¿‡\n',
                    'newline': True
                })
                return None

            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': ' âœ…\n',
                'newline': True
            })

            # ä¿å­˜åˆ°æ•°æ®åº“
            async with get_db_session() as db:
                paper = await upsert_paper(
                    db,
                    pmid=pmid,
                    pmcid=metadata.get("pmcid"),
                    title=metadata.get("title") or "(no title)",
                    source_type='pubmed',
                    abstract=metadata.get("abstract"),
                    pub_date=metadata.get("pub_date"),
                    authors=metadata.get("authors"),
                    pdf_path=str(pdf_path),
                    source_url=f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                )

                return self._paper_to_dict(paper)

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f' âŒ é”™è¯¯: {str(e)}\n',
                'newline': True
            })
            return None

    async def _fetch_europepmc_papers(
            self,
            query: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢ Europe PMCï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        results = []

        try:
            records = await search_europe_pmc(query, limit=settings.max_pmids_to_fetch)

            if not records:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'europepmc',
                    'content': 'âš ï¸ Europe PMC æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'ğŸ“¥ æ‰¾åˆ° {len(records)} æ¡è®°å½•\n',
                'newline': True
            })

            # å¤„ç†è®°å½•
            async with get_db_session() as db:
                for record in records:
                    # è¾¾åˆ°ç›®æ ‡æ•°é‡ååœæ­¢
                    if len(results) >= target_count:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': f'âœ… å·²è·å–è¶³å¤Ÿæ–‡çŒ®ï¼ˆ{target_count} ç¯‡ï¼‰ï¼Œåœæ­¢æ£€ç´¢\n',
                            'newline': True
                        })
                        break

                    pmid = record.get("pmid")
                    pmcid = record.get("pmcid")
                    has_pdf = record.get("hasPDF")

                    if has_pdf == 'N':
                        continue

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if pmid:
                        result = await db.execute(
                            select(Paper).where(
                                Paper.pmid == pmid,
                                Paper.source_type == 'europepmc'
                            )
                        )
                    elif pmcid:
                        result = await db.execute(
                            select(Paper).where(
                                Paper.pmcid == pmcid,
                                Paper.source_type == 'europepmc'
                            )
                        )
                    else:
                        continue

                    existing = result.scalar_one_or_none()

                    if existing:
                        results.append(self._paper_to_dict(existing))
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': f'  âœ“ {pmcid or pmid} å·²å­˜åœ¨ç¼“å­˜\n',
                            'newline': True
                        })
                        continue

                    title = record.get("title")
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': f'  ğŸ“„ å¤„ç† {pmcid or pmid}...',
                        'newline': False
                    })

                    # ä¸‹è½½ PDF
                    from pathlib import Path
                    import requests

                    pdf_url = f"https://europepmc.org/articles/{pmcid}?pdf=render" if pmcid else None

                    if not pdf_url:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': ' âš ï¸ æ— PDF\n',
                            'newline': True
                        })
                        continue

                    filename = f"europepmc_{pmcid or pmid}.pdf"
                    pdf_path = Path(settings.pdf_dir) / filename

                    loop = asyncio.get_running_loop()

                    def download_pdf():
                        try:
                            r = requests.get(pdf_url, timeout=settings.pdf_download_timeout)
                            if r.status_code == 200 and "pdf" in r.headers.get("content-type", "").lower():
                                pdf_path.parent.mkdir(parents=True, exist_ok=True)
                                with open(pdf_path, "wb") as f:
                                    f.write(r.content)
                                return True
                        except:
                            pass
                        return False

                    try:
                        download_success = await asyncio.wait_for(
                            loop.run_in_executor(self.executor, download_pdf),
                            timeout=settings.pdf_download_timeout
                        )
                    except asyncio.TimeoutError:
                        download_success = False

                    if not download_success:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': ' âŒ\n',
                            'newline': True
                        })
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': ' âœ…\n',
                        'newline': True
                    })

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    paper = await upsert_paper(
                        db,
                        pmid=pmid,
                        pmcid=pmcid,
                        title=title,
                        source_type='europepmc',
                        abstract='',
                        pub_date=record.get("pubYear"),
                        authors=record.get("authorString"),
                        pdf_path=str(pdf_path),
                        source_url=f"https://europepmc.org/article/MED/{pmid}" if pmid else f"https://europepmc.org/articles/{pmcid}"
                    )

                    results.append(self._paper_to_dict(paper))

            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'âœ… Europe PMC æ£€ç´¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(results)} ç¯‡æ–‡çŒ®\n',
                'newline': True
            })

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'âŒ Europe PMC æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def search_trials_with_ranking(
            self,
            keywords: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢å¹¶æ’åºä¸´åºŠè¯•éªŒ"""
        all_trials = []

        await progress_queue.put({
            'type': 'log',
            'source': 'clinical_trials',
            'content': f'\nğŸ” å¼€å§‹æ£€ç´¢ä¸´åºŠè¯•éªŒ: {keywords}\n',
            'newline': True
        })

        # 1. æŸ¥è¯¢ç¼“å­˜
        async with get_db_session() as db:
            keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]
            if keyword_list:
                query_filter = select(ClinicalTrial).where(
                    or_(*[ClinicalTrial.conditions.ilike(f"%{kw}%") for kw in keyword_list])
                ).limit(target_count * settings.search_multiplier)

                result = await db.execute(query_filter)
                cached = result.scalars().all()

                if cached:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'clinical_trials',
                        'content': f'ğŸ“š æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached)} ä¸ªå·²ç¼“å­˜è¯•éªŒ\n',
                        'newline': True
                    })

                    for trial in cached:
                        all_trials.append(self._trial_to_dict(trial))

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢
        if len(all_trials) < target_count * settings.search_multiplier:
            remaining = target_count * settings.search_multiplier - len(all_trials)

            try:
                keyword_list = [kw.strip() for kw in keywords.split(',')]
                trials, _ = await async_search_trials(
                    keyword_list,
                    logic="OR",
                    size=remaining * 2
                )

                await progress_queue.put({
                    'type': 'log',
                    'source': 'clinical_trials',
                    'content': f'ğŸ“¥ æ‰¾åˆ° {len(trials)} ä¸ªä¸´åºŠè¯•éªŒ\n',
                    'newline': True
                })

                # ä¿å­˜åˆ°æ•°æ®åº“
                async with get_db_session() as db:
                    for trial in trials:
                        nct_id = trial["nct_id"]

                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        result = await db.execute(
                            select(ClinicalTrial).where(ClinicalTrial.nct_id == nct_id)
                        )
                        existing = result.scalar_one_or_none()

                        if existing:
                            all_trials.append(self._trial_to_dict(existing))
                            continue

                        await progress_queue.put({
                            'type': 'log',
                            'source': 'clinical_trials',
                            'content': f'  ğŸ’Š ä¿å­˜ {nct_id}\n',
                            'newline': True
                        })

                        # ä¿å­˜åˆ°æ•°æ®åº“
                        await upsert_clinical_trial(
                            db,
                            nct_id=trial["nct_id"],
                            title=trial["title"],
                            official_title=trial.get("official_title"),
                            status=trial.get("status"),
                            start_date=trial.get("start_date"),
                            completion_date=trial.get("completion_date"),
                            study_type=trial.get("study_type"),
                            phase=trial.get("phase"),
                            allocation=trial.get("allocation"),
                            intervention_model=trial.get("intervention_model"),
                            conditions=trial.get("conditions"),
                            sponsor=trial.get("sponsor"),
                            locations=trial.get("locations"),
                            source_url=trial.get("source_url"),
                        )

                        all_trials.append(trial)

            except Exception as e:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'clinical_trials',
                    'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                    'newline': True
                })

        # 3. è®¡ç®—ç›¸å…³åº¦å¹¶æ’åº
        for trial in all_trials:
            title_score = self._calculate_relevance(keywords, trial.get('title', ''))
            condition_score = self._calculate_relevance(keywords, trial.get('conditions', ''))
            trial['relevance_score'] = (title_score * 0.5 + condition_score * 0.5)

        all_trials.sort(key=lambda t: t.get('relevance_score', 0), reverse=True)

        # 4. è¿”å›å‰ N ä¸ª
        selected_trials = all_trials[:target_count]

        await progress_queue.put({
            'type': 'result',
            'source': 'clinical_trials',
            'content': f'âœ… ç­›é€‰å‡ºæœ€ç›¸å…³çš„ {len(selected_trials)} ä¸ªä¸´åºŠè¯•éªŒ',
            'data': {
                'count': len(selected_trials),
                'trials': selected_trials
            }
        })

        return selected_trials

    def _paper_to_dict(self, paper: Paper) -> Dict:
        """Paper æ¨¡å‹è½¬å­—å…¸"""
        return {
            'id': paper.id,
            'pmid': paper.pmid,
            'pmcid': paper.pmcid,
            'title': paper.title,
            'abstract': paper.abstract,
            'pub_date': paper.pub_date,
            'authors': paper.authors,
            'pdf_path': paper.pdf_path,
            'source_url': paper.source_url,
            'source_type': paper.source_type
        }

    def _trial_to_dict(self, trial: ClinicalTrial) -> Dict:
        """ClinicalTrial æ¨¡å‹è½¬å­—å…¸"""
        return {
            'nct_id': trial.nct_id,
            'title': trial.title,
            'official_title': trial.official_title,
            'status': trial.status,
            'phase': trial.phase,
            'study_type': trial.study_type,
            'conditions': trial.conditions,
            'sponsor': trial.sponsor,
            'locations': trial.locations,
            'source_url': trial.source_url
        }

    def __del__(self):
        """æ¸…ç†çº¿ç¨‹æ± """
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)


# å…¨å±€å®ä¾‹
search_service = SearchService()