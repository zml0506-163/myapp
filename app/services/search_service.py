"""
ä¼˜åŒ–çš„å¤šæºæ£€ç´¢æœåŠ¡
app/services/search_service.py
"""
import asyncio
import logging

from typing import List, Dict, Set, Optional
from concurrent.futures import ThreadPoolExecutor
from sqlalchemy import select, or_, and_
import difflib

from app.core.config import settings
from app.db.database import get_db_session
from app.models import Paper, ClinicalTrial
from app.db.crud import upsert_paper, upsert_clinical_trial
from app.utils.storage_helper import storage_helper

from app.tools.pubmed_client import pubmed_client
from app.tools.europepmc_client import search_europe_pmc
from app.tools.clinical_trials_client import async_search_trials

logger = logging.getLogger("search_service")

class SearchProgress:
    """è¿›åº¦å›è°ƒå°è£…"""
    def __init__(self, queue: asyncio.Queue, source: str):
        self.queue = queue
        self.source = source
        self.loop = asyncio.get_running_loop()

    def callback(self, message: str, newline: bool = True):
        """åŒæ­¥å›è°ƒ"""
        asyncio.run_coroutine_threadsafe(
            self.queue.put({
                'type': 'log',
                'source': self.source,
                'content': message,
                'newline': newline
            }),
            self.loop
        )
        logger.info(f"Progress callback: {message}")


class SearchService:
    """ä¼˜åŒ–çš„å¤šæºæ£€ç´¢æœåŠ¡"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=settings.max_concurrent_downloads)
        self.logger = logging.getLogger("search_service")

    def _calculate_relevance(self, query: str, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸æŸ¥è¯¢çš„ç›¸å…³åº¦ï¼ˆ0-100åˆ†ï¼‰
        """
        if not text:
            return 0.0

        query_terms = set(query.lower().replace('and', '').replace('or', '').split())
        query_terms = {term.strip() for term in query_terms if len(term.strip()) > 2}

        if not query_terms:
            return 50.0

        text_lower = text.lower()
        matches = sum(1 for term in query_terms if term in text_lower)
        score = (matches / len(query_terms)) * 100

        return min(score, 100.0)

    def _deduplicate_papers(self, papers: List[Dict]) -> List[Dict]:
        """
        å»é‡æ–‡çŒ®ï¼ˆåŸºäºPMIDã€PMCIDæˆ–æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
        """
        seen_ids: Set[str] = set()
        seen_titles: List[str] = []
        unique_papers = []

        for paper in papers:
            if paper.get('pmid') and paper['pmid'] in seen_ids:
                continue

            if paper.get('pmcid') and paper['pmcid'] in seen_ids:
                continue

            title = paper.get('title', '')
            is_duplicate = False
            for seen_title in seen_titles:
                similarity = difflib.SequenceMatcher(None, title.lower(), seen_title.lower()).ratio()
                if similarity > 0.9:
                    is_duplicate = True
                    break

            if is_duplicate:
                continue

            if paper.get('pmid'):
                seen_ids.add(paper['pmid'])
            if paper.get('pmcid'):
                seen_ids.add(paper['pmcid'])
            seen_titles.append(title)
            unique_papers.append(paper)

        return unique_papers

    async def search_papers_with_ranking(
            self,
            query: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """
        æ£€ç´¢å¹¶æ’åºæ–‡çŒ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        ç­–ç•¥ï¼š
        1. å…ˆä»æ•°æ®åº“æŸ¥ç¼“å­˜
        2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢ï¼ˆæ£€ç´¢æ›´å¤šï¼‰
        3. å»é‡å¹¶æŒ‰ç›¸å…³åº¦æ’åº
        4. è¿”å›å‰ N ç¯‡
        """
        # 1. æŸ¥è¯¢ç¼“å­˜
        cached_papers = await self._search_cached_papers(query, target_count * settings.search_multiplier, progress_queue)

        all_papers = cached_papers.copy()

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢
        if len(all_papers) < target_count * settings.search_multiplier:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'\nğŸ” å¼€å§‹æ£€ç´¢ PubMed å’Œ Europe PMC...\n\n',
                'newline': True
            })

            # å¹¶å‘æ£€ç´¢ PubMed å’Œ Europe PMC
            pubmed_task = asyncio.create_task(
                self._fetch_pubmed_papers(query, target_count, progress_queue)
            )
            europepmc_task = asyncio.create_task(
                self._fetch_europepmc_papers(query, target_count, progress_queue)
            )

            results = await asyncio.gather(
                pubmed_task,
                europepmc_task,
                return_exceptions=True
            )

            # å¤„ç† PubMed ç»“æœ
            pubmed_papers = results[0]
            if isinstance(pubmed_papers, list):
                all_papers.extend(pubmed_papers)
            
            # å¤„ç† Europe PMC ç»“æœ
            europepmc_papers = results[1]
            if isinstance(europepmc_papers, list):
                all_papers.extend(europepmc_papers)

        # 3. å»é‡
        all_papers = self._deduplicate_papers(all_papers)

        await progress_queue.put({
            'type': 'log',
            'source': 'dedup',
            'content': f'\nğŸ”„ å»é‡åå…± {len(all_papers)} ç¯‡æ–‡çŒ®\n\n',
            'newline': True
        })

        # 4. è®¡ç®—ç›¸å…³åº¦å¹¶æ’åº
        for paper in all_papers:
            title_score = self._calculate_relevance(query, paper.get('title', ''))
            abstract_score = self._calculate_relevance(query, paper.get('abstract', ''))
            paper['relevance_score'] = (title_score * 0.7 + abstract_score * 0.3)

        all_papers.sort(key=lambda p: p.get('relevance_score', 0), reverse=True)

        # 5. è¿”å›å‰ N ç¯‡
        selected_papers = all_papers[:target_count]

        await progress_queue.put({
            'type': 'result',
            'source': 'papers',
            'content': f'âœ… ç­›é€‰å‡ºæœ€ç›¸å…³çš„ {len(selected_papers)} ç¯‡æ–‡çŒ®',
            'data': {
                'count': len(selected_papers),
                'papers': selected_papers
            }
        })

        return selected_papers

    async def _search_cached_papers(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """ä»æ•°æ®åº“æŸ¥è¯¢ç¼“å­˜çš„æ–‡çŒ®"""
        cached_papers = []

        async with get_db_session() as db:
            search_terms = query.replace('AND', '').replace('OR', '').split()[:5]
            if search_terms:
                query_filter = select(Paper).where(
                    and_(
                        or_(
                            Paper.source_type == 'pubmed',
                            Paper.source_type == 'europepmc'
                        ),
                        or_(*[Paper.title.ilike(f"%{term}%") for term in search_terms if len(term) > 2])
                    )
                ).limit(limit)

                result = await db.execute(query_filter)
                cached = result.scalars().all()

                if cached:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'cache',
                        'content': f'ğŸ“š æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached)} ç¯‡å·²ç¼“å­˜æ–‡çŒ®\n',
                        'newline': True
                    })

                    for paper in cached:
                        cached_papers.append(self._paper_to_dict(paper))

        return cached_papers

    async def _fetch_pubmed_papers(
            self,
            query: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """
        æ£€ç´¢ PubMedï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        æ”¹è¿›ï¼š
        1. ä½¿ç”¨é…ç½®çš„è¶…æ—¶å’Œå¹¶å‘æ§åˆ¶
        2. è¾¾åˆ°ç›®æ ‡æ•°é‡åç«‹å³åœæ­¢
        3. æ›´è¯¦ç»†çš„è¿›åº¦åé¦ˆ
        """
        results = []

        try:
            # æœç´¢ PMIDï¼ˆè·å–æ›´å¤šä»¥åº”å¯¹ä¸‹è½½å¤±è´¥ï¼‰
            pmids = await pubmed_client.esearch_pmids(
                query,
                retmax=settings.max_pmids_to_fetch
            )

            if not pmids:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'pubmed',
                    'content': 'âš ï¸ PubMed æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'ğŸ“¥ æ‰¾åˆ° {len(pmids)} ä¸ª PMIDï¼Œå‡†å¤‡ä¸‹è½½ PDF...\n\n',
                'newline': True
            })

            # è·å–å…ƒæ•°æ®
            meta = await pubmed_client.efetch_metadata(pmids)

            # æ‰¹é‡æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ–‡çŒ®
            async with get_db_session() as db:
                # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„PMID
                result = await db.execute(
                    select(Paper).where(
                        Paper.pmid.in_(pmids),
                        Paper.source_type == 'pubmed'
                    )
                )
                existing_papers = result.scalars().all()
                existing_pmids = {p.pmid for p in existing_papers}
                
                # æ·»åŠ å·²å­˜åœ¨çš„æ–‡çŒ®åˆ°ç»“æœ
                for paper in existing_papers:
                    results.append(self._paper_to_dict(paper))
                
                if existing_pmids:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': f'  âœ“ æ‰¾åˆ° {len(existing_pmids)} ç¯‡å·²ç¼“å­˜æ–‡çŒ®\n\n',
                        'newline': True
                    })
                
                # è¿‡æ»¤å‡ºéœ€è¦ä¸‹è½½çš„PMID
                pmids_to_download = [pid for pid in pmids if pid not in existing_pmids]
                
                if not pmids_to_download:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': 'âœ… æ‰€æœ‰æ–‡çŒ®å‡å·²ç¼“å­˜\n\n',
                        'newline': True
                    })
                else:
                    # é™åˆ¶ä¸‹è½½æ•°é‡
                    max_to_download = min(len(pmids_to_download), target_count - len(results))
                    pmids_to_download = pmids_to_download[:max_to_download]
                    
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': f'  ğŸ“¥ å‡†å¤‡ä¸‹è½½ {len(pmids_to_download)} ç¯‡æ–°æ–‡çŒ®...\n\n',
                        'newline': True
                    })
                    
                    # å¹¶å‘ä¸‹è½½ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
                    download_tasks = []
                    for pid in pmids_to_download:
                        task = self._download_and_save_paper(
                            pid,
                            meta.get(pid, {}),
                            progress_queue
                        )
                        download_tasks.append(task)
                    
                    # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
                    papers = await asyncio.gather(*download_tasks, return_exceptions=True)
                    
                    for paper in papers:
                        if paper and not isinstance(paper, Exception):
                            results.append(paper)
                            
                            # è¾¾åˆ°ç›®æ ‡æ•°é‡ååœæ­¢
                            if len(results) >= target_count:
                                break

            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'\nâœ… PubMed æ£€ç´¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(results)} ç¯‡æ–‡çŒ®\n\n',
                'newline': True
            })

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'âŒ PubMed æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def _download_and_save_paper(
            self,
            pmid: str,
            metadata: Dict,
            progress_queue: asyncio.Queue
    ) -> Optional[Dict]:
        """ä¸‹è½½å¹¶ä¿å­˜å•ç¯‡æ–‡çŒ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘æ—¥å¿—è¾“å‡ºï¼‰"""
        try:
            # åˆ›å»ºè¿›åº¦å›è°ƒï¼ˆé™é»˜æ¨¡å¼ï¼‰
            progress = SearchProgress(progress_queue, 'pubmed')

            # æ¨é€é˜Ÿåˆ—çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯è¡¨æ ¼ upsertï¼‰
            await progress_queue.put({
                'type': 'progress',
                'entity': 'download',
                'id': f'PMID:{pmid}',
                'source': 'pubmed',
                'status': 'queued',
                'title': (metadata.get("title") or "(no title)"),
                'pmid': pmid,
                'pmcid': metadata.get("pmcid")
            })
            self.logger.info("progress queued pubmed id=%s title=%s", pmid, (metadata.get("title") or "(no title)"))

            # å¸¦ item ç»´åº¦çš„æ—¥å¿—å›è°ƒï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            def item_log_callback(message: str, newline: bool = True):
                asyncio.run_coroutine_threadsafe(
                    progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'item_id': f'PMID:{pmid}',
                        'content': message,
                        'newline': newline
                    }),
                    progress.loop
                )

            # ä½¿ç”¨ä¼˜åŒ–çš„å®¢æˆ·ç«¯ä¸‹è½½ï¼ˆå¸¦è¶…æ—¶å’Œå¹¶å‘æ§åˆ¶ï¼‰
            pdf_path = await pubmed_client.download_pdf_with_limit(
                pmid,
                metadata.get("pmcid"),
                self.executor,
                item_log_callback  # è½¬å‘è¯¦ç»†æ—¥å¿—åˆ°å‰ç«¯ï¼ˆç»‘å®š item_idï¼‰
            )

            if not pdf_path:
                # åªåœ¨å¤±è´¥æ—¶è¾“å‡ºç®€çŸ­æ—¥å¿—
                await progress_queue.put({
                    'type': 'progress',
                    'entity': 'download',
                    'id': f'PMID:{pmid}',
                    'source': 'pubmed',
                    'status': 'failed'
                })
                self.logger.info("progress failed pubmed id=%s", pmid)
                return None

            # ä¿å­˜åˆ°æ•°æ®åº“
            async with get_db_session() as db:
                paper = await upsert_paper(
                    db,
                    pmid=pmid,
                    pmcid=metadata.get("pmcid"),
                    title=metadata.get("title") or "(no title)",
                    source_type='pubmed',
                    abstract=metadata.get("abstract"),
                    pub_date=metadata.get("pub_date"),
                    authors=metadata.get("authors"),
                    pdf_path=str(pdf_path),
                    source_url=f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                )

                # æˆåŠŸåè¾“å‡ºç®€çŸ­æ—¥å¿—
                await progress_queue.put({
                    'type': 'log',
                    'source': 'pubmed',
                    'content': f'  âœ“ {pmid}\n',
                    'newline': True
                })

                # åŒæ­¥æ›´æ–°è¿›åº¦çŠ¶æ€
                await progress_queue.put({
                    'type': 'progress',
                    'entity': 'download',
                    'id': f'PMID:{pmid}',
                    'source': 'pubmed',
                    'status': 'success',
                    'pdf_path': str(pdf_path)
                })
                self.logger.info("progress success pubmed id=%s path=%s", pmid, str(pdf_path))

                return self._paper_to_dict(paper)

        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œä¸è¾“å‡ºé”™è¯¯æ—¥å¿—
            await progress_queue.put({
                'type': 'progress',
                'entity': 'download',
                'id': f'PMID:{pmid}',
                'source': 'pubmed',
                'status': 'failed'
            })
            try:
                self.logger.exception("exception during pubmed download id=%s", pmid)
            except Exception:
                pass
            return None
    
    async def _download_europepmc_paper(
            self,
            record: Dict,
            progress_queue: asyncio.Queue
    ) -> Optional[Dict]:
        """ä¸‹è½½å¹¶ä¿å­˜Europe PMCæ–‡çŒ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            pmid = record.get("pmid")
            pmcid = record.get("pmcid")
            title = record.get("title")

            if not pmcid or not title:
                return None

            # æ¨é€é˜Ÿåˆ—çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯è¡¨æ ¼ upsertï¼‰
            await progress_queue.put({
                'type': 'progress',
                'entity': 'download',
                'id': f'PMCID:{pmcid}',
                'source': 'europepmc',
                'status': 'queued',
                'title': title,
                'pmid': pmid,
                'pmcid': pmcid
            })
            self.logger.info("progress queued europepmc pmcid=%s pmid=%s title=%s", pmcid, pmid, title)

            # ä¸‹è½½ PDF
            from pathlib import Path
            import requests
            
            pdf_url = f"https://europepmc.org/articles/{pmcid}?pdf=render"
            filename = f"europepmc_{pmcid}.pdf"
            pdf_path = storage_helper.get_pdf_storage_path('europepmc', filename)
            
            loop = asyncio.get_running_loop()
            
            def download_pdf():
                try:
                    r = requests.get(pdf_url, timeout=settings.pdf_download_timeout)
                    if r.status_code == 200 and "pdf" in r.headers.get("content-type", "").lower():
                        pdf_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(pdf_path, "wb") as f:
                            f.write(r.content)
                        return True
                except:
                    pass
                return False
            
            try:
                download_success = await asyncio.wait_for(
                    loop.run_in_executor(self.executor, download_pdf),
                    timeout=settings.pdf_download_timeout
                )
            except asyncio.TimeoutError:
                download_success = False

            if not download_success:
                await progress_queue.put({
                    'type': 'progress',
                    'entity': 'download',
                    'id': f'PMCID:{pmcid}',
                    'source': 'europepmc',
                    'status': 'failed'
                })
                self.logger.info("progress failed europepmc pmcid=%s", pmcid)
                return None

            # ä¿å­˜åˆ°æ•°æ®åº“
            async with get_db_session() as db:
                paper = await upsert_paper(
                    db,
                    pmid=pmid,
                    pmcid=pmcid,
                    title=title,
                    source_type='europepmc',
                    abstract='',
                    pub_date=record.get("pubYear"),
                    authors=record.get("authorString"),
                    pdf_path=str(pdf_path),
                    source_url=f"https://europepmc.org/article/MED/{pmid}" if pmid else f"https://europepmc.org/articles/{pmcid}"
                )

                # æˆåŠŸåè¾“å‡ºç®€çŸ­æ—¥å¿—
                await progress_queue.put({
                    'type': 'log',
                    'source': 'europepmc',
                    'content': f'  âœ“ {pmcid}\n',
                    'newline': True
                })

                await progress_queue.put({
                    'type': 'progress',
                    'entity': 'download',
                    'id': f'PMCID:{pmcid}',
                    'source': 'europepmc',
                    'status': 'success',
                    'pdf_path': str(pdf_path)
                })
                self.logger.info("progress success europepmc pmcid=%s path=%s", pmcid, str(pdf_path))

                return self._paper_to_dict(paper)
        
        except Exception as e:
            # é™é»˜å¤±è´¥
            await progress_queue.put({
                'type': 'progress',
                'entity': 'download',
                'id': f'PMCID:{record.get("pmcid")}',
                'source': 'europepmc',
                'status': 'failed'
            })
            try:
                self.logger.exception("exception during europepmc download pmcid=%s", record.get("pmcid"))
            except Exception:
                pass
            return None

    async def _fetch_europepmc_papers(
            self,
            query: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢ Europe PMCï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        results = []

        try:
            records = await search_europe_pmc(query, limit=settings.max_pmids_to_fetch)

            if not records:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'europepmc',
                    'content': 'âš ï¸ Europe PMC æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'ğŸ“¥ æ‰¾åˆ° {len(records)} æ¡è®°å½•\n\n',
                'newline': True
            })

            # è¿‡æ»¤æœ‰PDFçš„è®°å½•
            records_with_pdf = [r for r in records if r.get("hasPDF") == 'Y' and r.get("pmcid")]
            
            if not records_with_pdf:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'europepmc',
                    'content': 'âš ï¸ æ²¡æœ‰å¯ä¸‹è½½çš„PDFæ–‡çŒ®\n\n',
                    'newline': True
                })
                return results
            
            # æ‰¹é‡æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ–‡çŒ®
            async with get_db_session() as db:
                pmcids = [r.get("pmcid") for r in records_with_pdf if r.get("pmcid")]
                
                # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„PMCID
                result = await db.execute(
                    select(Paper).where(
                        Paper.pmcid.in_(pmcids),
                        Paper.source_type == 'europepmc'
                    )
                )
                existing_papers = result.scalars().all()
                existing_pmcids = {p.pmcid for p in existing_papers}
                
                # æ·»åŠ å·²å­˜åœ¨çš„æ–‡çŒ®åˆ°ç»“æœ
                for paper in existing_papers:
                    results.append(self._paper_to_dict(paper))
                
                if existing_pmcids:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': f'  âœ“ æ‰¾åˆ° {len(existing_pmcids)} ç¯‡å·²ç¼“å­˜æ–‡çŒ®\n\n',
                        'newline': True
                    })
                
                # è¿‡æ»¤å‡ºéœ€è¦ä¸‹è½½çš„è®°å½•
                records_to_download = [r for r in records_with_pdf if r.get("pmcid") not in existing_pmcids]
                
                if not records_to_download:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': 'âœ… æ‰€æœ‰æ–‡çŒ®å‡å·²ç¼“å­˜\n\n',
                        'newline': True
                    })
                else:
                    # é™åˆ¶ä¸‹è½½æ•°é‡
                    max_to_download = min(len(records_to_download), target_count - len(results))
                    records_to_download = records_to_download[:max_to_download]
                    
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': f'  ğŸ“¥ å‡†å¤‡ä¸‹è½½ {len(records_to_download)} ç¯‡æ–°æ–‡çŒ®...\n\n',
                        'newline': True
                    })
                    
                    # å¹¶å‘ä¸‹è½½
                    download_tasks = []
                    for record in records_to_download:
                        task = self._download_europepmc_paper(record, progress_queue)
                        download_tasks.append(task)
                    
                    # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
                    papers = await asyncio.gather(*download_tasks, return_exceptions=True)
                    
                    for paper in papers:
                        if paper and not isinstance(paper, Exception):
                            results.append(paper)
                            
                            # è¾¾åˆ°ç›®æ ‡æ•°é‡ååœæ­¢
                            if len(results) >= target_count:
                                break

            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'\nâœ… Europe PMC æ£€ç´¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(results)} ç¯‡æ–‡çŒ®\n\n',
                'newline': True
            })

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'âŒ Europe PMC æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def search_trials_with_ranking(
            self,
            keywords: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢å¹¶æ’åºä¸´åºŠè¯•éªŒ"""
        all_trials = []

        await progress_queue.put({
            'type': 'log',
            'source': 'clinical_trials',
            'content': f'\nğŸ” å¼€å§‹æ£€ç´¢ä¸´åºŠè¯•éªŒ: {keywords}\n\n',
            'newline': True
        })

        # 1. æŸ¥è¯¢ç¼“å­˜
        async with get_db_session() as db:
            keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]
            if keyword_list:
                query_filter = select(ClinicalTrial).where(
                    or_(*[ClinicalTrial.conditions.ilike(f"%{kw}%") for kw in keyword_list])
                ).limit(target_count * settings.search_multiplier)

                result = await db.execute(query_filter)
                cached = result.scalars().all()

                if cached:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'clinical_trials',
                        'content': f'ğŸ“š æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached)} ä¸ªå·²ç¼“å­˜è¯•éªŒ\n\n',
                        'newline': True
                    })

                    for trial in cached:
                        all_trials.append(self._trial_to_dict(trial))

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢
        if len(all_trials) < target_count * settings.search_multiplier:
            remaining = target_count * settings.search_multiplier - len(all_trials)

            try:
                keyword_list = [kw.strip() for kw in keywords.split(',')]
                trials, _ = await async_search_trials(
                    keyword_list,
                    logic="OR",
                    size=remaining * 2
                )

                await progress_queue.put({
                    'type': 'log',
                    'source': 'clinical_trials',
                    'content': f'ğŸ“¥ æ‰¾åˆ° {len(trials)} ä¸ªä¸´åºŠè¯•éªŒ\n\n',
                    'newline': True
                })

                # æ‰¹é‡æ£€æŸ¥å·²å­˜åœ¨çš„è¯•éªŒ
                async with get_db_session() as db:
                    nct_ids = [trial["nct_id"] for trial in trials]
                    
                    # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„NCT ID
                    result = await db.execute(
                        select(ClinicalTrial).where(ClinicalTrial.nct_id.in_(nct_ids))
                    )
                    existing_trials = result.scalars().all()
                    existing_nct_ids = {t.nct_id for t in existing_trials}
                    
                    # æ·»åŠ å·²å­˜åœ¨çš„è¯•éªŒåˆ°ç»“æœ
                    for trial in existing_trials:
                        all_trials.append(self._trial_to_dict(trial))
                    
                    # è¿‡æ»¤å‡ºéœ€è¦ä¿å­˜çš„è¯•éªŒ
                    trials_to_save = [t for t in trials if t["nct_id"] not in existing_nct_ids]
                    
                    if trials_to_save:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'clinical_trials',
                            'content': f'  ğŸ’Š ä¿å­˜ {len(trials_to_save)} ä¸ªæ–°è¯•éªŒ...\n',
                            'newline': True
                        })
                        
                        # æ‰¹é‡ä¿å­˜
                        for trial in trials_to_save:
                            await upsert_clinical_trial(
                                db,
                                nct_id=trial["nct_id"],
                                title=trial["title"],
                                official_title=trial.get("official_title"),
                                status=trial.get("status"),
                                start_date=trial.get("start_date"),
                                completion_date=trial.get("completion_date"),
                                study_type=trial.get("study_type"),
                                phase=trial.get("phase"),
                                allocation=trial.get("allocation"),
                                intervention_model=trial.get("intervention_model"),
                                conditions=trial.get("conditions"),
                                sponsor=trial.get("sponsor"),
                                locations=trial.get("locations"),
                                source_url=trial.get("source_url"),
                            )
                            all_trials.append(trial)

            except Exception as e:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'clinical_trials',
                    'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                    'newline': True
                })

        # 3. è®¡ç®—ç›¸å…³åº¦å¹¶æ’åº
        for trial in all_trials:
            title_score = self._calculate_relevance(keywords, trial.get('title', ''))
            condition_score = self._calculate_relevance(keywords, trial.get('conditions', ''))
            trial['relevance_score'] = (title_score * 0.5 + condition_score * 0.5)

        all_trials.sort(key=lambda t: t.get('relevance_score', 0), reverse=True)

        # 4. è¿”å›å‰ N ä¸ª
        selected_trials = all_trials[:target_count]

        await progress_queue.put({
            'type': 'result',
            'source': 'clinical_trials',
            'content': f'âœ… ç­›é€‰å‡ºæœ€ç›¸å…³çš„ {len(selected_trials)} ä¸ªä¸´åºŠè¯•éªŒ',
            'data': {
                'count': len(selected_trials),
                'trials': selected_trials
            }
        })

        return selected_trials

    def _paper_to_dict(self, paper: Paper) -> Dict:
        """Paper æ¨¡å‹è½¬å­—å…¸"""
        return {
            'id': paper.id,
            'pmid': paper.pmid,
            'pmcid': paper.pmcid,
            'title': paper.title,
            'abstract': paper.abstract,
            'pub_date': paper.pub_date,
            'authors': paper.authors,
            'pdf_path': paper.pdf_path,
            'source_url': paper.source_url,
            'source_type': paper.source_type
        }

    def _trial_to_dict(self, trial: ClinicalTrial) -> Dict:
        """ClinicalTrial æ¨¡å‹è½¬å­—å…¸"""
        return {
            'nct_id': trial.nct_id,
            'title': trial.title,
            'official_title': trial.official_title,
            'status': trial.status,
            'phase': trial.phase,
            'study_type': trial.study_type,
            'conditions': trial.conditions,
            'sponsor': trial.sponsor,
            'locations': trial.locations,
            'source_url': trial.source_url
        }

    def __del__(self):
        """æ¸…ç†çº¿ç¨‹æ± """
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)


# å…¨å±€å®ä¾‹
search_service = SearchService()