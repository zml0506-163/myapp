"""
ä¼˜åŒ–çš„å¤šæºæ£€ç´¢æœåŠ¡ V2
- æ£€ç´¢æ›´å¤šï¼Œç­›é€‰æœ€ç›¸å…³
- å»é‡å¤„ç†
- PDFä¸‹è½½å¤±è´¥ç»§ç»­æ£€ç´¢
"""
import asyncio
from typing import List, Dict, Set
from concurrent.futures import ThreadPoolExecutor
from sqlalchemy import select, or_, and_
import difflib

from app.core.config import settings
from app.db.database import get_db_session
from app.models import Paper, ClinicalTrial
from app.db.crud import upsert_paper, upsert_clinical_trial

from app.tools.pubmed_client import esearch_pmids, efetch_metadata, get_pdf_from_pubmed
from app.tools.europepmc_client import search_europe_pmc
from app.tools.clinical_trials_client import async_search_trials


class SearchProgress:
    """è¿›åº¦å›è°ƒå°è£…"""
    def __init__(self, queue: asyncio.Queue, source: str):
        self.queue = queue
        self.source = source
        self.loop = asyncio.get_running_loop()

    def callback(self, message: str, newline: bool = True):
        """åŒæ­¥å›è°ƒ"""
        asyncio.run_coroutine_threadsafe(
            self.queue.put({
                'type': 'log',
                'source': self.source,
                'content': message,
                'newline': newline
            }),
            self.loop
        )


class OptimizedSearchService:
    """ä¼˜åŒ–çš„å¤šæºæ£€ç´¢æœåŠ¡"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=5)

    def _calculate_relevance(self, query: str, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸æŸ¥è¯¢çš„ç›¸å…³åº¦ï¼ˆ0-100åˆ†ï¼‰

        ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ç®—æ³•
        """
        if not text:
            return 0.0

        # æå–æŸ¥è¯¢å…³é”®è¯
        query_terms = set(query.lower().replace('and', '').replace('or', '').split())
        query_terms = {term.strip() for term in query_terms if len(term.strip()) > 2}

        if not query_terms:
            return 50.0  # é»˜è®¤åˆ†æ•°

        # è®¡ç®—åŒ¹é…åº¦
        text_lower = text.lower()
        matches = sum(1 for term in query_terms if term in text_lower)
        score = (matches / len(query_terms)) * 100

        return min(score, 100.0)

    def _deduplicate_papers(self, papers: List[Dict]) -> List[Dict]:
        """
        å»é‡æ–‡çŒ®ï¼ˆåŸºäºPMIDã€PMCIDæˆ–æ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
        """
        seen_ids: Set[str] = set()
        seen_titles: List[str] = []
        unique_papers = []

        for paper in papers:
            # æ£€æŸ¥PMID
            if paper.get('pmid') and paper['pmid'] in seen_ids:
                continue

            # æ£€æŸ¥PMCID
            if paper.get('pmcid') and paper['pmcid'] in seen_ids:
                continue

            # æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆ> 0.9 è®¤ä¸ºé‡å¤ï¼‰
            title = paper.get('title', '')
            is_duplicate = False
            for seen_title in seen_titles:
                similarity = difflib.SequenceMatcher(None, title.lower(), seen_title.lower()).ratio()
                if similarity > 0.9:
                    is_duplicate = True
                    break

            if is_duplicate:
                continue

            # æ·»åŠ å”¯ä¸€æ–‡çŒ®
            if paper.get('pmid'):
                seen_ids.add(paper['pmid'])
            if paper.get('pmcid'):
                seen_ids.add(paper['pmcid'])
            seen_titles.append(title)
            unique_papers.append(paper)

        return unique_papers

    async def search_papers_with_ranking(
            self,
            query: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """
        æ£€ç´¢å¹¶æ’åºæ–‡çŒ®

        Args:
            query: æ£€ç´¢è¡¨è¾¾å¼
            target_count: ç›®æ ‡æ–‡çŒ®æ•°é‡
            progress_queue: è¿›åº¦é˜Ÿåˆ—

        Returns:
            æ’åºåçš„æ–‡çŒ®åˆ—è¡¨
        """
        # 1. å…ˆæŸ¥æ•°æ®åº“ç¼“å­˜
        cached_papers = []
        async with get_db_session() as db:
            search_terms = query.replace('AND', '').replace('OR', '').split()[:5]
            if search_terms:
                query_filter = select(Paper).where(
                    and_(
                        or_(
                            Paper.source_type == 'pubmed',
                            Paper.source_type == 'europepmc'
                        ),
                        or_(*[Paper.title.ilike(f"%{term}%") for term in search_terms if len(term) > 2])
                    )
                ).limit(target_count * 3)

                result = await db.execute(query_filter)
                cached = result.scalars().all()

                if cached:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'cache',
                        'content': f'ğŸ“š æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached)} ç¯‡å·²ç¼“å­˜æ–‡çŒ®\n',
                        'newline': True
                    })

                    for paper in cached:
                        cached_papers.append(self._paper_to_dict(paper))

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢ï¼ˆæ£€ç´¢æ›´å¤šï¼‰
        all_papers = cached_papers.copy()

        if len(all_papers) < target_count * 3:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'\nğŸ” å¼€å§‹æ£€ç´¢ PubMed å’Œ Europe PMC...\n',
                'newline': True
            })

            # PubMedæ£€ç´¢ï¼ˆæ£€ç´¢3å€æ•°é‡ï¼‰
            pubmed_papers = await self._fetch_pubmed_papers(
                query, target_count * 3, progress_queue
            )
            all_papers.extend(pubmed_papers)

            # Europe PMCæ£€ç´¢ï¼ˆæ£€ç´¢3å€æ•°é‡ï¼‰
            europepmc_papers = await self._fetch_europepmc_papers(
                query, target_count * 3, progress_queue
            )
            all_papers.extend(europepmc_papers)

        # 3. å»é‡
        all_papers = self._deduplicate_papers(all_papers)

        await progress_queue.put({
            'type': 'log',
            'source': 'dedup',
            'content': f'ğŸ”„ å»é‡åå…± {len(all_papers)} ç¯‡æ–‡çŒ®\n',
            'newline': True
        })

        # 4. è®¡ç®—ç›¸å…³åº¦å¹¶æ’åº
        for paper in all_papers:
            title_score = self._calculate_relevance(query, paper.get('title', ''))
            abstract_score = self._calculate_relevance(query, paper.get('abstract', ''))
            paper['relevance_score'] = (title_score * 0.7 + abstract_score * 0.3)

        all_papers.sort(key=lambda p: p.get('relevance_score', 0), reverse=True)

        # 5. è¿”å›å‰Nç¯‡
        selected_papers = all_papers[:target_count]

        await progress_queue.put({
            'type': 'result',
            'source': 'papers',
            'content': f'âœ… ç­›é€‰å‡ºæœ€ç›¸å…³çš„ {len(selected_papers)} ç¯‡æ–‡çŒ®',
            'data': {
                'count': len(selected_papers),
                'papers': selected_papers
            }
        })

        return selected_papers

    async def _fetch_pubmed_papers(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢PubMedï¼ˆæŒç»­æ£€ç´¢ç›´åˆ°æ»¡è¶³æ•°é‡ï¼‰"""
        results = []

        try:
            # æœç´¢PMID
            pmids = await esearch_pmids(query, retmax=limit * 5)

            if not pmids:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'pubmed',
                    'content': 'âš ï¸ PubMed æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            # è·å–å…ƒæ•°æ®
            meta = await efetch_metadata(pmids)

            # å¤„ç†æ¯ä¸ªPMIDï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡
            async with get_db_session() as db:
                for pid in pmids:
                    if len(results) >= limit:
                        break

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    result = await db.execute(
                        select(Paper).where(
                            Paper.pmid == pid,
                            Paper.source_type == 'pubmed'
                        )
                    )
                    existing = result.scalar_one_or_none()

                    if existing:
                        results.append(self._paper_to_dict(existing))
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': f'  ğŸ“„ å¤„ç† PMID {pid}...',
                        'newline': False
                    })

                    m = meta.get(pid, {})
                    progress = SearchProgress(progress_queue, 'pubmed')

                    # ä¸‹è½½PDFï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                    max_retries = 2
                    pdf_path = None

                    for retry in range(max_retries):
                        pdf_path = await get_pdf_from_pubmed(
                            pid,
                            m.get("pmcid"),
                            self.executor,
                            progress.callback
                        )

                        if pdf_path:
                            break

                        if retry < max_retries - 1:
                            await progress_queue.put({
                                'type': 'log',
                                'source': 'pubmed',
                                'content': ' é‡è¯•...',
                                'newline': False
                            })

                    if not pdf_path:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'pubmed',
                            'content': ' âŒ è·³è¿‡\n',
                            'newline': True
                        })
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': ' âœ…\n',
                        'newline': True
                    })

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    paper = await upsert_paper(
                        db,
                        pmid=pid,
                        pmcid=m.get("pmcid"),
                        title=m.get("title") or "(no title)",
                        source_type='pubmed',
                        abstract=m.get("abstract"),
                        pub_date=m.get("pub_date"),
                        authors=m.get("authors"),
                        pdf_path=str(pdf_path),
                        source_url=f"https://pubmed.ncbi.nlm.nih.gov/{pid}/"
                    )

                    results.append(self._paper_to_dict(paper))

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'âŒ PubMed æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def _fetch_europepmc_papers(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢Europe PMCï¼ˆæŒç»­æ£€ç´¢ç›´åˆ°æ»¡è¶³æ•°é‡ï¼‰"""
        results = []

        try:
            records = await search_europe_pmc(query, limit=limit * 5)

            if not records:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'europepmc',
                    'content': 'âš ï¸ Europe PMC æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            # å¤„ç†è®°å½•
            async with get_db_session() as db:
                for record in records:
                    if len(results) >= limit:
                        break

                    pmid = record.get("pmid")
                    pmcid = record.get("pmcid")
                    has_pdf = record.get("hasPDF")

                    if has_pdf == 'N':
                        continue

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if pmid:
                        result = await db.execute(
                            select(Paper).where(
                                Paper.pmid == pmid,
                                Paper.source_type == 'europepmc'
                            )
                        )
                    elif pmcid:
                        result = await db.execute(
                            select(Paper).where(
                                Paper.pmcid == pmcid,
                                Paper.source_type == 'europepmc'
                            )
                        )
                    else:
                        continue

                    existing = result.scalar_one_or_none()

                    if existing:
                        results.append(self._paper_to_dict(existing))
                        continue

                    title = record.get("title")
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': f'  ğŸ“„ å¤„ç† {pmcid or pmid}...',
                        'newline': False
                    })

                    # ä¸‹è½½PDF
                    from pathlib import Path
                    import requests

                    pdf_url = f"https://europepmc.org/articles/{pmcid}?pdf=render" if pmcid else None

                    if not pdf_url:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': ' âš ï¸ æ— PDF\n',
                            'newline': True
                        })
                        continue

                    filename = f"europepmc_{pmcid or pmid}.pdf"
                    pdf_path = Path(settings.pdf_dir) / filename

                    loop = asyncio.get_running_loop()

                    def download_pdf():
                        try:
                            r = requests.get(pdf_url, timeout=60)
                            if r.status_code == 200 and "pdf" in r.headers.get("content-type", "").lower():
                                pdf_path.parent.mkdir(parents=True, exist_ok=True)
                                with open(pdf_path, "wb") as f:
                                    f.write(r.content)
                                return True
                        except:
                            pass
                        return False

                    download_success = await loop.run_in_executor(
                        self.executor,
                        download_pdf
                    )

                    if not download_success:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': ' âŒ\n',
                            'newline': True
                        })
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': ' âœ…\n',
                        'newline': True
                    })

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    paper = await upsert_paper(
                        db,
                        pmid=pmid,
                        pmcid=pmcid,
                        title=title,
                        source_type='europepmc',
                        abstract='',
                        pub_date=record.get("pubYear"),
                        authors=record.get("authorString"),
                        pdf_path=str(pdf_path),
                        source_url=f"https://europepmc.org/article/MED/{pmid}" if pmid else f"https://europepmc.org/articles/{pmcid}"
                    )

                    results.append(self._paper_to_dict(paper))

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'âŒ Europe PMC æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def search_trials_with_ranking(
            self,
            keywords: str,
            target_count: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """
        æ£€ç´¢å¹¶æ’åºä¸´åºŠè¯•éªŒ

        Args:
            keywords: å…³é”®è¯
            target_count: ç›®æ ‡è¯•éªŒæ•°é‡
            progress_queue: è¿›åº¦é˜Ÿåˆ—

        Returns:
            æ’åºåçš„è¯•éªŒåˆ—è¡¨
        """
        all_trials = []

        await progress_queue.put({
            'type': 'log',
            'source': 'clinical_trials',
            'content': f'\nğŸ” å¼€å§‹æ£€ç´¢ä¸´åºŠè¯•éªŒ: {keywords}\n',
            'newline': True
        })

        # 1. å…ˆæŸ¥æ•°æ®åº“ç¼“å­˜
        async with get_db_session() as db:
            keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]
            if keyword_list:
                query_filter = select(ClinicalTrial).where(
                    or_(*[ClinicalTrial.conditions.ilike(f"%{kw}%") for kw in keyword_list])
                ).limit(target_count * 3)

                result = await db.execute(query_filter)
                cached = result.scalars().all()

                if cached:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'clinical_trials',
                        'content': f'ğŸ“š æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached)} ä¸ªå·²ç¼“å­˜è¯•éªŒ\n',
                        'newline': True
                    })

                    for trial in cached:
                        all_trials.append(self._trial_to_dict(trial))

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡Œæ£€ç´¢ï¼ˆæ£€ç´¢3å€æ•°é‡ï¼‰
        if len(all_trials) < target_count * 3:
            remaining = target_count * 3 - len(all_trials)

            try:
                keyword_list = [kw.strip() for kw in keywords.split(',')]
                trials, _ = await async_search_trials(
                    keyword_list,
                    logic="OR",
                    size=remaining * 2
                )

                # ä¿å­˜åˆ°æ•°æ®åº“
                async with get_db_session() as db:
                    for trial in trials:
                        nct_id = trial["nct_id"]

                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        result = await db.execute(
                            select(ClinicalTrial).where(ClinicalTrial.nct_id == nct_id)
                        )
                        existing = result.scalar_one_or_none()

                        if existing:
                            all_trials.append(self._trial_to_dict(existing))
                            continue

                        await progress_queue.put({
                            'type': 'log',
                            'source': 'clinical_trials',
                            'content': f'  ğŸ’Š ä¿å­˜ {nct_id}\n',
                            'newline': True
                        })

                        # ä¿å­˜åˆ°æ•°æ®åº“
                        saved_trial = await upsert_clinical_trial(
                            db,
                            nct_id=trial["nct_id"],
                            title=trial["title"],
                            official_title=trial.get("official_title"),
                            status=trial.get("status"),
                            start_date=trial.get("start_date"),
                            completion_date=trial.get("completion_date"),
                            study_type=trial.get("study_type"),
                            phase=trial.get("phase"),
                            allocation=trial.get("allocation"),
                            intervention_model=trial.get("intervention_model"),
                            conditions=trial.get("conditions"),
                            sponsor=trial.get("sponsor"),
                            locations=trial.get("locations"),
                            source_url=trial.get("source_url"),
                        )

                        all_trials.append(trial)

            except Exception as e:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'clinical_trials',
                    'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                    'newline': True
                })

        # 3. è®¡ç®—ç›¸å…³åº¦å¹¶æ’åº
        for trial in all_trials:
            title_score = self._calculate_relevance(keywords, trial.get('title', ''))
            condition_score = self._calculate_relevance(keywords, trial.get('conditions', ''))
            trial['relevance_score'] = (title_score * 0.5 + condition_score * 0.5)

        all_trials.sort(key=lambda t: t.get('relevance_score', 0), reverse=True)

        # 4. è¿”å›å‰Nä¸ª
        selected_trials = all_trials[:target_count]

        await progress_queue.put({
            'type': 'result',
            'source': 'clinical_trials',
            'content': f'âœ… ç­›é€‰å‡ºæœ€ç›¸å…³çš„ {len(selected_trials)} ä¸ªä¸´åºŠè¯•éªŒ',
            'data': {
                'count': len(selected_trials),
                'trials': selected_trials
            }
        })

        return selected_trials

    def _paper_to_dict(self, paper: Paper) -> Dict:
        """Paper æ¨¡å‹è½¬å­—å…¸"""
        return {
            'id': paper.id,
            'pmid': paper.pmid,
            'pmcid': paper.pmcid,
            'title': paper.title,
            'abstract': paper.abstract,
            'pub_date': paper.pub_date,
            'authors': paper.authors,
            'pdf_path': paper.pdf_path,
            'source_url': paper.source_url,
            'source_type': paper.source_type
        }

    def _trial_to_dict(self, trial: ClinicalTrial) -> Dict:
        """ClinicalTrial æ¨¡å‹è½¬å­—å…¸"""
        return {
            'nct_id': trial.nct_id,
            'title': trial.title,
            'official_title': trial.official_title,
            'status': trial.status,
            'phase': trial.phase,
            'study_type': trial.study_type,
            'conditions': trial.conditions,
            'sponsor': trial.sponsor,
            'locations': trial.locations,
            'source_url': trial.source_url
        }

    def __del__(self):
        """æ¸…ç†çº¿ç¨‹æ± """
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)


# å…¨å±€å®ä¾‹
optimized_search_service = OptimizedSearchService()