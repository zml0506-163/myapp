"""
å¤šæºæ£€ç´¢å·¥ä½œæµæœåŠ¡
"""
import asyncio
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor
from sqlalchemy import select, or_, and_

from app.core.config import settings
from app.db.database import get_db_session
from app.models import Paper, ClinicalTrial
from app.db.crud import upsert_paper, upsert_clinical_trial

from app.tools.pubmed_client import esearch_pmids, efetch_metadata, get_pdf_from_pubmed
from app.tools.europepmc_client import search_europe_pmc
from app.tools.clinical_trials_client import async_search_trials


class SearchProgress:
    """è¿›åº¦å›è°ƒå°è£…"""
    def __init__(self, queue: asyncio.Queue, source: str):
        self.queue = queue
        self.source = source
        self.loop = asyncio.get_running_loop()

    def callback(self, message: str, newline: bool = True):
        """åŒæ­¥å›è°ƒï¼Œåœ¨çº¿ç¨‹æ± ä¸­å®‰å…¨è°ƒç”¨"""
        asyncio.run_coroutine_threadsafe(
            self.queue.put({
                'type': 'log',
                'source': self.source,
                'content': message,
                'newline': newline
            }),
            self.loop
        )


class MultiSourceSearchService:
    """å¤šæºæ£€ç´¢æœåŠ¡ - å°è£…åŸæœ‰å®¢æˆ·ç«¯"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=5)

    async def search_pubmed_with_cache(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """
        æ£€ç´¢ PubMedï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰
        è¿”å›: æ–‡çŒ®åˆ—è¡¨
        """
        results = []

        await progress_queue.put({
            'type': 'log',
            'source': 'pubmed',
            'content': f'ğŸ” å¼€å§‹æ£€ç´¢ PubMed: {query}\n',
            'newline': True
        })

        # 1. å…ˆæŸ¥æ•°æ®åº“ç¼“å­˜
        async with get_db_session() as db:
            search_terms = query.replace('AND', '').replace('OR', '').split()[:3]
            if search_terms:
                query_filter = select(Paper).where(
                    and_(
                        Paper.source_type == 'pubmed',
                        or_(*[Paper.title.ilike(f"%{term}%") for term in search_terms])
                    )
                ).limit(limit)

                result = await db.execute(query_filter)
                cached_papers = result.scalars().all()

                if cached_papers:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': f'âœ… æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached_papers)} ç¯‡å·²ç¼“å­˜æ–‡çŒ®\n',
                        'newline': True
                    })

                    for paper in cached_papers:
                        results.append(self._paper_to_dict(paper))

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡ŒçœŸå®æ£€ç´¢
        if len(results) < limit:
            remaining = limit - len(results)
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'ğŸ“¥ éœ€è¦ä¸‹è½½ {remaining} ç¯‡æ–°æ–‡çŒ®\n',
                'newline': True
            })

            new_papers = await self._fetch_new_pubmed_papers(
                query, remaining, progress_queue
            )
            results.extend(new_papers)

        await progress_queue.put({
            'type': 'result',
            'source': 'pubmed',
            'content': f'âœ… PubMed æ£€ç´¢å®Œæˆï¼Œå…± {len(results)} ç¯‡æ–‡çŒ®',
            'data': {
                'count': len(results),
                'papers': results
            }
        })

        return results

    async def _fetch_new_pubmed_papers(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ‰§è¡ŒçœŸå®çš„ PubMed æ£€ç´¢å’Œä¸‹è½½"""
        results = []

        try:
            # æœç´¢ PMID
            pmids = await esearch_pmids(query, retmax=limit * 5)

            if not pmids:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'pubmed',
                    'content': 'âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'æ‰¾åˆ° {len(pmids)} ä¸ª PMID\n',
                'newline': True
            })

            # è·å–å…ƒæ•°æ®
            meta = await efetch_metadata(pmids)

            # ä¸‹è½½ PDF å¹¶ä¿å­˜
            success_count = 0
            async with get_db_session() as db:
                for pid in pmids:
                    if success_count >= limit:
                        break

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    result = await db.execute(
                        select(Paper).where(
                            Paper.pmid == pid,
                            Paper.source_type == 'pubmed'
                        )
                    )
                    if result.scalar_one_or_none():
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'pubmed',
                            'content': f'  âœ“ PMID {pid} å·²å­˜åœ¨ï¼Œè·³è¿‡\n',
                            'newline': True
                        })
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': f'  ğŸ“„ å¤„ç† PMID {pid}...',
                        'newline': False
                    })

                    m = meta.get(pid, {})

                    # åˆ›å»ºè¿›åº¦å›è°ƒ
                    progress = SearchProgress(progress_queue, 'pubmed')

                    # ä¸‹è½½ PDF
                    pdf_path = await get_pdf_from_pubmed(
                        pid,
                        m.get("pmcid"),
                        self.executor,
                        progress.callback
                    )

                    if not pdf_path:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'pubmed',
                            'content': ' âŒ\n',
                            'newline': True
                        })
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'pubmed',
                        'content': ' âœ…\n',
                        'newline': True
                    })

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    paper = await upsert_paper(
                        db,
                        pmid=pid,
                        pmcid=m.get("pmcid"),
                        title=m.get("title") or "(no title)",
                        source_type='pubmed',
                        abstract=m.get("abstract"),
                        pub_date=m.get("pub_date"),
                        authors=m.get("authors"),
                        pdf_path=str(pdf_path),
                        source_url=f"https://pubmed.ncbi.nlm.nih.gov/{pid}/"
                    )

                    results.append(self._paper_to_dict(paper))
                    success_count += 1

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'pubmed',
                'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def search_europepmc_with_cache(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢ Europe PMCï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        results = []

        await progress_queue.put({
            'type': 'log',
            'source': 'europepmc',
            'content': f'ğŸ” å¼€å§‹æ£€ç´¢ Europe PMC: {query}\n',
            'newline': True
        })

        # 1. å…ˆæŸ¥æ•°æ®åº“ç¼“å­˜
        async with get_db_session() as db:
            search_terms = query.replace('AND', '').replace('OR', '').split()[:3]
            if search_terms:
                query_filter = select(Paper).where(
                    and_(
                        Paper.source_type == 'europepmc',
                        or_(*[Paper.title.ilike(f"%{term}%") for term in search_terms])
                    )
                ).limit(limit)

                result = await db.execute(query_filter)
                cached_papers = result.scalars().all()

                if cached_papers:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': f'âœ… æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached_papers)} ç¯‡å·²ç¼“å­˜æ–‡çŒ®\n',
                        'newline': True
                    })

                    for paper in cached_papers:
                        results.append(self._paper_to_dict(paper))

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡ŒçœŸå®æ£€ç´¢
        if len(results) < limit:
            remaining = limit - len(results)
            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'ğŸ“¥ éœ€è¦ä¸‹è½½ {remaining} ç¯‡æ–°æ–‡çŒ®\n',
                'newline': True
            })

            new_papers = await self._fetch_new_europepmc_papers(
                query, remaining, progress_queue
            )
            results.extend(new_papers)

        await progress_queue.put({
            'type': 'result',
            'source': 'europepmc',
            'content': f'âœ… Europe PMC æ£€ç´¢å®Œæˆï¼Œå…± {len(results)} ç¯‡æ–‡çŒ®',
            'data': {
                'count': len(results),
                'papers': results
            }
        })

        return results

    async def _fetch_new_europepmc_papers(
            self,
            query: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ‰§è¡ŒçœŸå®çš„ Europe PMC æ£€ç´¢"""
        results = []

        try:
            # æœç´¢è®°å½•ï¼ˆä½¿ç”¨åŸæœ‰å®¢æˆ·ç«¯ï¼‰
            records = await search_europe_pmc(query, limit=limit * 3)

            if not records:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'europepmc',
                    'content': 'âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®\n',
                    'newline': True
                })
                return results

            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'æ‰¾åˆ° {len(records)} æ¡è®°å½•\n',
                'newline': True
            })

            # å¤„ç†è®°å½•ï¼ˆå°è£…ç‰ˆæœ¬ï¼Œä¸è°ƒç”¨åŸæœ‰çš„ process_records_and_save_to_dbï¼‰
            success_count = 0
            async with get_db_session() as db:
                for record in records:
                    if success_count >= limit:
                        break

                    pmid = record.get("pmid")
                    pmcid = record.get("pmcid")
                    has_pdf = record.get("hasPDF")

                    if has_pdf == 'N':
                        continue

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if pmid:
                        result = await db.execute(
                            select(Paper).where(
                                Paper.pmid == pmid,
                                Paper.source_type == 'europepmc'
                            )
                        )
                    elif pmcid:
                        result = await db.execute(
                            select(Paper).where(
                                Paper.pmcid == pmcid,
                                Paper.source_type == 'europepmc'
                            )
                        )
                    else:
                        continue

                    if result.scalar_one_or_none():
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': f'  âœ“ {pmid or pmcid} å·²å­˜åœ¨ï¼Œè·³è¿‡\n',
                            'newline': True
                        })
                        continue

                    title = record.get("title")
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': f'  ğŸ“„ å¤„ç† {pmid or pmcid}: {title[:50]}...',
                        'newline': False
                    })

                    # è·å– PDF URL
                    pdf_url = f"https://europepmc.org/articles/{pmcid}?pdf=render" if pmcid else None

                    if not pdf_url:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': ' âš ï¸ æ— PDF\n',
                            'newline': True
                        })
                        continue

                    # ä¸‹è½½ PDFï¼ˆä½¿ç”¨çº¿ç¨‹æ± ï¼‰
                    from pathlib import Path
                    import requests

                    filename = f"europepmc_{pmcid or pmid}.pdf"
                    pdf_path = Path(settings.pdf_dir) / filename

                    loop = asyncio.get_running_loop()

                    def download_pdf():
                        try:
                            r = requests.get(pdf_url, timeout=20)
                            if r.status_code == 200 and "pdf" in r.headers.get("content-type", "").lower():
                                pdf_path.parent.mkdir(parents=True, exist_ok=True)
                                with open(pdf_path, "wb") as f:
                                    f.write(r.content)
                                return True
                        except:
                            pass
                        return False

                    download_success = await loop.run_in_executor(
                        self.executor,
                        download_pdf
                    )

                    if not download_success:
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'europepmc',
                            'content': ' âŒ\n',
                            'newline': True
                        })
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'europepmc',
                        'content': ' âœ…\n',
                        'newline': True
                    })

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    paper = await upsert_paper(
                        db,
                        pmid=pmid,
                        pmcid=pmcid,
                        title=title,
                        source_type='europepmc',
                        abstract='',
                        pub_date=record.get("pubYear"),
                        authors=record.get("authorString"),
                        pdf_path=str(pdf_path),
                        source_url=f"https://europepmc.org/article/MED/{pmid}" if pmid else f"https://europepmc.org/articles/{pmcid}"
                    )

                    results.append(self._paper_to_dict(paper))
                    success_count += 1

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'europepmc',
                'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    async def search_clinical_trials_with_cache(
            self,
            keywords: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ£€ç´¢ä¸´åºŠè¯•éªŒï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        results = []

        await progress_queue.put({
            'type': 'log',
            'source': 'clinical_trials',
            'content': f'ğŸ” å¼€å§‹æ£€ç´¢ä¸´åºŠè¯•éªŒ: {keywords}\n',
            'newline': True
        })

        # 1. å…ˆæŸ¥æ•°æ®åº“ç¼“å­˜
        async with get_db_session() as db:
            keyword_list = [kw.strip() for kw in keywords.split(',')]
            if keyword_list:
                query_filter = select(ClinicalTrial).where(
                    or_(*[ClinicalTrial.conditions.ilike(f"%{kw}%") for kw in keyword_list])
                ).limit(limit)

                result = await db.execute(query_filter)
                cached_trials = result.scalars().all()

                if cached_trials:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'clinical_trials',
                        'content': f'âœ… æ•°æ®åº“ä¸­æ‰¾åˆ° {len(cached_trials)} ä¸ªå·²ç¼“å­˜è¯•éªŒ\n',
                        'newline': True
                    })

                    for trial in cached_trials:
                        results.append(self._trial_to_dict(trial))

        # 2. å¦‚æœç¼“å­˜ä¸è¶³ï¼Œæ‰§è¡ŒçœŸå®æ£€ç´¢
        if len(results) < limit:
            remaining = limit - len(results)
            await progress_queue.put({
                'type': 'log',
                'source': 'clinical_trials',
                'content': f'ğŸ“¥ éœ€è¦æ£€ç´¢ {remaining} ä¸ªæ–°è¯•éªŒ\n',
                'newline': True
            })

            new_trials = await self._fetch_new_clinical_trials(
                keywords, remaining, progress_queue
            )
            results.extend(new_trials)

        await progress_queue.put({
            'type': 'result',
            'source': 'clinical_trials',
            'content': f'âœ… ä¸´åºŠè¯•éªŒæ£€ç´¢å®Œæˆï¼Œå…± {len(results)} ä¸ªè¯•éªŒ',
            'data': {
                'count': len(results),
                'trials': results
            }
        })

        return results

    async def _fetch_new_clinical_trials(
            self,
            keywords: str,
            limit: int,
            progress_queue: asyncio.Queue
    ) -> List[Dict]:
        """æ‰§è¡ŒçœŸå®çš„ä¸´åºŠè¯•éªŒæ£€ç´¢"""
        results = []

        try:
            # è°ƒç”¨åŸæœ‰å®¢æˆ·ç«¯
            keyword_list = [kw.strip() for kw in keywords.split(',')]
            trials, _ = await async_search_trials(
                keyword_list,
                logic="OR",
                size=limit * 2
            )

            if not trials:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'clinical_trials',
                    'content': 'âš ï¸ æœªæ‰¾åˆ°ç›¸å…³è¯•éªŒ\n',
                    'newline': True
                })
                return results

            await progress_queue.put({
                'type': 'log',
                'source': 'clinical_trials',
                'content': f'æ‰¾åˆ° {len(trials)} ä¸ªè¯•éªŒ\n',
                'newline': True
            })

            # ä¿å­˜åˆ°æ•°æ®åº“
            success_count = 0
            async with get_db_session() as db:
                for trial in trials:
                    if success_count >= limit:
                        break

                    nct_id = trial["nct_id"]

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    result = await db.execute(
                        select(ClinicalTrial).where(ClinicalTrial.nct_id == nct_id)
                    )
                    if result.scalar_one_or_none():
                        await progress_queue.put({
                            'type': 'log',
                            'source': 'clinical_trials',
                            'content': f'  âœ“ {nct_id} å·²å­˜åœ¨ï¼Œè·³è¿‡\n',
                            'newline': True
                        })
                        continue

                    await progress_queue.put({
                        'type': 'log',
                        'source': 'clinical_trials',
                        'content': f'  ğŸ’Š ä¿å­˜ {nct_id}\n',
                        'newline': True
                    })

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    saved_trial = await upsert_clinical_trial(
                        db,
                        nct_id=trial["nct_id"],
                        title=trial["title"],
                        official_title=trial.get("official_title"),
                        status=trial.get("status"),
                        start_date=trial.get("start_date"),
                        completion_date=trial.get("completion_date"),
                        study_type=trial.get("study_type"),
                        phase=trial.get("phase"),
                        allocation=trial.get("allocation"),
                        intervention_model=trial.get("intervention_model"),
                        conditions=trial.get("conditions"),
                        sponsor=trial.get("sponsor"),
                        locations=trial.get("locations"),
                        source_url=trial.get("source_url"),
                    )

                    results.append(trial)
                    success_count += 1

        except Exception as e:
            await progress_queue.put({
                'type': 'log',
                'source': 'clinical_trials',
                'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                'newline': True
            })

        return results

    def _paper_to_dict(self, paper: Paper) -> Dict:
        """Paper æ¨¡å‹è½¬å­—å…¸"""
        return {
            'id': paper.id,
            'pmid': paper.pmid,
            'pmcid': paper.pmcid,
            'title': paper.title,
            'abstract': paper.abstract,
            'pub_date': paper.pub_date,
            'authors': paper.authors,
            'pdf_path': paper.pdf_path,
            'source_url': paper.source_url,
            'source_type': paper.source_type
        }

    def _trial_to_dict(self, trial: ClinicalTrial) -> Dict:
        """ClinicalTrial æ¨¡å‹è½¬å­—å…¸"""
        return {
            'nct_id': trial.nct_id,
            'title': trial.title,
            'official_title': trial.official_title,
            'status': trial.status,
            'phase': trial.phase,
            'study_type': trial.study_type,
            'conditions': trial.conditions,
            'sponsor': trial.sponsor,
            'locations': trial.locations,
            'source_url': trial.source_url
        }

    def __del__(self):
        """æ¸…ç†çº¿ç¨‹æ± """
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)


# å…¨å±€å®ä¾‹
search_service = MultiSourceSearchService()