import os
import json
from typing import TypedDict, AsyncGenerator, List, Dict, Optional
from langgraph.graph import StateGraph, END
from sqlalchemy import select, func

from app.core.config import settings
from app.db.database import get_db_session
from app.services.llm_service import llm_service
from app.services.search_service import search_service
from app.models import WorkflowExecution, Message, MessageType


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    conversation_id: int
    user_id: int
    user_query: str
    user_attachments: List[Dict]  # ç”¨æˆ·ä¸Šä¼ çš„é™„ä»¶
    history_messages: List[Dict]  # å†å²å¯¹è¯

    # æ­¥éª¤1ï¼šç‰¹å¾æå–
    patient_features: str

    # æ­¥éª¤2ï¼šæ£€ç´¢æ¡ä»¶ç”Ÿæˆ
    pubmed_query: str
    clinical_trial_keywords: str

    # æ­¥éª¤3ï¼šæ£€ç´¢ç»“æœ
    papers: List[Dict]
    trials: List[Dict]

    # æ­¥éª¤4ï¼šæ–‡çŒ®åˆ†æç»“æœ
    paper_analyses: List[Dict]

    # æ­¥éª¤5ï¼šä¸´åºŠè¯•éªŒåˆ†æ
    trial_analysis: str

    # æ­¥éª¤6ï¼šæœ€ç»ˆæŠ¥å‘Š
    final_answer: str

    # æµç¨‹æ§åˆ¶
    current_step: str
    errors: List[str]


class MultiSourceWorkflow:
    """å¤šæºæ£€ç´¢å·¥ä½œæµ - ä½¿ç”¨ LangGraph ç¼–æ’"""

    def __init__(self):
        self.workflow = self._build_workflow()

    def _build_workflow(self) -> StateGraph:
        """æ„å»ºå·¥ä½œæµå›¾"""
        workflow = StateGraph(WorkflowState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("extract_features", self._extract_features)
        workflow.add_node("generate_queries", self._generate_queries)
        workflow.add_node("search", self._search)
        workflow.add_node("analyze_papers", self._analyze_papers)
        workflow.add_node("analyze_trials", self._analyze_trials)
        workflow.add_node("generate_final", self._generate_final)

        # å®šä¹‰æµç¨‹
        workflow.set_entry_point("extract_features")
        workflow.add_edge("extract_features", "generate_queries")
        workflow.add_edge("generate_queries", "search")
        workflow.add_edge("search", "analyze_papers")
        workflow.add_edge("analyze_papers", "analyze_trials")
        workflow.add_edge("analyze_trials", "generate_final")
        workflow.add_edge("generate_final", END)

        return workflow.compile()

    async def execute_with_streaming(
            self,
            conversation_id: int,
            user_id: int,
            user_query: str,
            user_attachments: List[Dict] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        æ‰§è¡Œå·¥ä½œæµå¹¶æµå¼è¾“å‡º

        Yields:
            {
                'type': 'step_start' | 'token' | 'step_complete' | 'error' | 'done',
                'step': str,
                'content': str
            }
        """
        # åˆ›å»ºå·¥ä½œæµæ‰§è¡Œè®°å½•
        execution_id = None
        async with get_db_session() as db:
            execution = WorkflowExecution(
                conversation_id=conversation_id,
                user_id=user_id,
                workflow_type='multi_source',
                status='running',
                current_step='initializing'
            )
            db.add(execution)
            await db.commit()
            execution_id = execution.id

        # åŠ è½½å†å²å¯¹è¯
        history_messages = await self._load_history(conversation_id)

        # åˆå§‹åŒ–çŠ¶æ€
        state: WorkflowState = {
            'conversation_id': conversation_id,
            'user_id': user_id,
            'user_query': user_query,
            'user_attachments': user_attachments or [],
            'history_messages': history_messages,
            'patient_features': '',
            'pubmed_query': '',
            'clinical_trial_keywords': '',
            'papers': [],
            'trials': [],
            'paper_analyses': [],
            'trial_analysis': '',
            'final_answer': '',
            'current_step': '',
            'errors': []
        }

        try:
            # æ‰§è¡Œå„ä¸ªæ­¥éª¤
            async for chunk in self._execute_step(state, 'extract_features', self._extract_features):
                yield chunk

            async for chunk in self._execute_step(state, 'generate_queries', self._generate_queries):
                yield chunk

            async for chunk in self._execute_step(state, 'search', self._search):
                yield chunk

            async for chunk in self._execute_step(state, 'analyze_papers', self._analyze_papers):
                yield chunk

            async for chunk in self._execute_step(state, 'analyze_trials', self._analyze_trials):
                yield chunk

            async for chunk in self._execute_step(state, 'generate_final', self._generate_final):
                yield chunk

            # ä¿å­˜æœ€ç»ˆç»“æœ
            await self._save_final_result(state, execution_id)

            # æ›´æ–°æ‰§è¡Œè®°å½•
            async with get_db_session() as db:
                execution = await db.get(WorkflowExecution, execution_id)
                execution.status = 'completed'
                execution.completed_at = func.now()
                await db.commit()

            yield {'type': 'done', 'step': 'workflow', 'content': ''}

        except Exception as e:
            # è®°å½•é”™è¯¯
            async with get_db_session() as db:
                execution = await db.get(WorkflowExecution, execution_id)
                execution.status = 'failed'
                execution.error_message = str(e)
                await db.commit()

            yield {'type': 'error', 'step': state.get('current_step', 'unknown'), 'content': str(e)}

    async def _execute_step(
            self,
            state: WorkflowState,
            step_name: str,
            step_func
    ) -> AsyncGenerator[Dict, None]:
        """æ‰§è¡Œå•ä¸ªæ­¥éª¤"""
        state['current_step'] = step_name

        yield {
            'type': 'step_start',
            'step': step_name,
            'content': f'\n\n## æ­¥éª¤ï¼š{self._get_step_title(step_name)}\n\n'
        }

        try:
            async for chunk in step_func(state):
                yield chunk

            yield {
                'type': 'step_complete',
                'step': step_name,
                'content': '\n\n---\n'
            }

        except Exception as e:
            state['errors'].append(f"{step_name}: {str(e)}")
            yield {
                'type': 'error',
                'step': step_name,
                'content': f'\n\nâŒ é”™è¯¯: {str(e)}\n\n'
            }

    def _get_step_title(self, step_name: str) -> str:
        """è·å–æ­¥éª¤æ ‡é¢˜"""
        titles = {
            'extract_features': 'æå–æ‚£è€…ç‰¹å¾',
            'generate_queries': 'ç”Ÿæˆæ£€ç´¢æ¡ä»¶',
            'search': 'æ‰§è¡Œæ£€ç´¢',
            'analyze_papers': 'åˆ†ææ–‡çŒ®',
            'analyze_trials': 'åˆ†æä¸´åºŠè¯•éªŒ',
            'generate_final': 'ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š'
        }
        return titles.get(step_name, step_name)

    async def _extract_features(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤1: æå–æ‚£è€…ç‰¹å¾"""

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []

        # æ·»åŠ å†å²å¯¹è¯
        if state['history_messages']:
            context_parts.append("### å†å²å¯¹è¯")
            for msg in state['history_messages'][-5:]:
                role = "ç”¨æˆ·" if msg['type'] == 'user' else "AI"
                context_parts.append(f"**{role}**: {msg['content']}")
            context_parts.append("")

        # å¤„ç†é™„ä»¶
        if state['user_attachments']:
            context_parts.append("### ç”¨æˆ·ä¸Šä¼ çš„é™„ä»¶")
            for att in state['user_attachments']:
                context_parts.append(f"- {att['original_filename']} ({att.get('mime_type', 'unknown')})")
            context_parts.append("")

        context = "\n".join(context_parts)

        # æ„å»ºæç¤ºè¯
        prompt = f"""{context}

### å½“å‰ç”¨æˆ·é—®é¢˜
{state['user_query']}

### ä»»åŠ¡
è¯·ä»ä»¥ä¸Šä¿¡æ¯ä¸­æå–æ‚£è€…çš„å…³é”®ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š

1. **ä¸»è¦ç–¾ç—…/è¯Šæ–­**: æ˜ç¡®æ‚£è€…çš„ä¸»è¦ç–¾ç—…åç§°
2. **ç—…ç†ç±»å‹å’Œåˆ†æœŸ**: å¦‚æœæåˆ°ï¼Œè¯·åˆ—å‡ºè¯¦ç»†çš„ç—…ç†ç±»å‹å’ŒTNMåˆ†æœŸ
3. **åŸºå› çªå˜ä¿¡æ¯**: åˆ—å‡ºæ‰€æœ‰æåˆ°çš„åŸºå› çªå˜ï¼ˆå¦‚EGFRã€ALKã€ROS1ç­‰ï¼‰
4. **æ—¢å¾€æ²»ç–—å²**: ä¹‹å‰æ¥å—è¿‡çš„æ²»ç–—æ–¹æ¡ˆ
5. **å½“å‰çŠ¶æ€å’Œéœ€æ±‚**: æ‚£è€…ç›®å‰çš„çŠ¶æ€å’Œæƒ³äº†è§£çš„å†…å®¹

è¯·ä»¥ç»“æ„åŒ–ã€æ¸…æ™°çš„æ–¹å¼åˆ—å‡ºè¿™äº›ä¿¡æ¯ã€‚å¦‚æœæŸäº›ä¿¡æ¯æœªæåŠï¼Œè¯·æ ‡æ³¨"æœªæåŠ"ã€‚"""

        messages = [{"role": "user", "content": prompt}]

        # å¦‚æœæœ‰å›¾ç‰‡é™„ä»¶ï¼Œä½¿ç”¨è§†è§‰æ¨¡å‹
        image_attachments = [att for att in state['user_attachments']
                             if att.get('mime_type', '').startswith('image/')]

        full_response = ""

        if image_attachments:
            # ä½¿ç”¨ qwen3-vl-plus å¤„ç†å›¾ç‰‡
            for att in image_attachments:
                async for token in llm_service.chat_with_image_stream(
                        text=prompt,
                        image_path=att['file_path'],
                        history=state['history_messages']
                ):
                    full_response += token
                    yield {'type': 'token', 'step': 'extract_features', 'content': token}
        else:
            # ä½¿ç”¨ qwen-max å¤„ç†æ–‡æœ¬
            async for token in llm_service.chat_stream(messages=messages):
                full_response += token
                yield {'type': 'token', 'step': 'extract_features', 'content': token}

        state['patient_features'] = full_response

    async def _generate_queries(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢æ¡ä»¶"""

        prompt = f"""åŸºäºä»¥ä¸‹æ‚£è€…ç‰¹å¾ï¼Œç”Ÿæˆç²¾ç¡®çš„æ£€ç´¢æ¡ä»¶ï¼š

### æ‚£è€…ç‰¹å¾
{state['patient_features']}

### ä»»åŠ¡
è¯·ç”Ÿæˆä»¥ä¸‹æ£€ç´¢æ¡ä»¶ï¼š

1. **PubMed æ£€ç´¢è¡¨è¾¾å¼**: ä½¿ç”¨å¸ƒå°”è¿ç®—ç¬¦ï¼ˆANDã€ORã€NOTï¼‰ï¼Œæ„å»ºç²¾ç¡®çš„æ£€ç´¢å¼ï¼Œç¡®ä¿èƒ½æ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®
2. **ClinicalTrials.gov å…³é”®è¯**: æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”

**è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆJSONæ ¼å¼ï¼‰**:
```json
{{
    "pubmed_query": "è¿™é‡Œæ˜¯PubMedæ£€ç´¢è¡¨è¾¾å¼",
    "clinical_trial_keywords": "å…³é”®è¯1,å…³é”®è¯2,å…³é”®è¯3"
}}
```

åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""

        messages = [{"role": "user", "content": prompt}]

        full_response = ""
        async for token in llm_service.chat_stream(messages=messages):
            full_response += token
            yield {'type': 'token', 'step': 'generate_queries', 'content': token}

        # è§£æ JSON
        try:
            start = full_response.find('{')
            end = full_response.rfind('}') + 1
            if start != -1 and end > start:
                queries = json.loads(full_response[start:end])
                state['pubmed_query'] = queries.get('pubmed_query', '')
                state['clinical_trial_keywords'] = queries.get('clinical_trial_keywords', '')
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON")
        except Exception as e:
            error_msg = f"\n\nâš ï¸ æ£€ç´¢æ¡ä»¶è§£æå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤æ¡ä»¶\n\n"
            yield {'type': 'token', 'step': 'generate_queries', 'content': error_msg}
            state['errors'].append(f"è§£ææ£€ç´¢æ¡ä»¶å¤±è´¥: {str(e)}")
            # ä½¿ç”¨é»˜è®¤æ¡ä»¶
            state['pubmed_query'] = state['user_query']
            state['clinical_trial_keywords'] = state['user_query']

    async def _search(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤3: æ‰§è¡Œæ£€ç´¢ï¼ˆè°ƒç”¨æ£€ç´¢å·¥å…·ï¼‰"""

        # æ£€ç´¢ PubMed
        yield {
            'type': 'token',
            'step': 'search',
            'content': f"ğŸ” æ­£åœ¨æ£€ç´¢ PubMed: `{state['pubmed_query']}`\n\n"
        }

        papers = await search_service.search_pubmed(state['pubmed_query'])
        state['papers'] = papers[:settings.max_search_results]

        yield {
            'type': 'token',
            'step': 'search',
            'content': f"âœ… æ£€ç´¢åˆ° **{len(state['papers'])}** ç¯‡ç›¸å…³æ–‡çŒ®\n\n"
        }

        # æ£€ç´¢ä¸´åºŠè¯•éªŒ
        yield {
            'type': 'token',
            'step': 'search',
            'content': f"ğŸ” æ­£åœ¨æ£€ç´¢ä¸´åºŠè¯•éªŒ: `{state['clinical_trial_keywords']}`\n\n"
        }

        trials = await search_service.search_clinical_trials(state['clinical_trial_keywords'])
        state['trials'] = trials[:settings.max_search_results]

        yield {
            'type': 'token',
            'step': 'search',
            'content': f"âœ… æ£€ç´¢åˆ° **{len(state['trials'])}** ä¸ªä¸´åºŠè¯•éªŒ\n\n"
        }

    async def _analyze_papers(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤4: é€ä¸ªåˆ†æ PDFï¼ˆä½¿ç”¨ qwen-longï¼Œè®©æ¨¡å‹ç›´æ¥è¯»å–PDFï¼‰"""

        if not state['papers']:
            yield {
                'type': 'token',
                'step': 'analyze_papers',
                'content': "â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®\n\n"
            }
            return

        for i, paper in enumerate(state['papers']):
            yield {
                'type': 'token',
                'step': 'analyze_papers',
                'content': f"\n### ğŸ“„ æ–‡çŒ® {i+1}/{len(state['papers'])}: {paper['title']}\n\n"
            }

            # æ£€æŸ¥ PDF æ˜¯å¦å­˜åœ¨
            pdf_path = paper.get('pdf_path')
            if not pdf_path or not os.path.exists(pdf_path):
                yield {
                    'type': 'token',
                    'step': 'analyze_papers',
                    'content': "âš ï¸ PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡è¯¥æ–‡çŒ®\n\n"
                }
                continue

            # æ„å»ºåˆ†ææç¤ºè¯
            prompt = f"""è¯·ä»”ç»†é˜…è¯»è¿™ç¯‡PDFæ–‡çŒ®ï¼Œå¹¶åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ·±å…¥åˆ†æï¼š

### æ‚£è€…ç‰¹å¾
{state['patient_features']}

### ç”¨æˆ·é—®é¢˜
{state['user_query']}

### æ–‡çŒ®åŸºæœ¬ä¿¡æ¯
- **æ ‡é¢˜**: {paper['title']}
- **ä½œè€…**: {paper.get('authors', 'N/A')}
- **å‘è¡¨æ—¥æœŸ**: {paper.get('pub_date', 'N/A')}

### åˆ†æä»»åŠ¡
è¯·å®Œæˆä»¥ä¸‹åˆ†æï¼ˆåŸºäºPDFå…¨æ–‡ï¼‰ï¼š

1. **æ–‡çŒ®æ ¸å¿ƒå†…å®¹æ¦‚è¿°**: ç®€è¦è¯´æ˜æ–‡çŒ®çš„ä¸»è¦ç ”ç©¶å†…å®¹
2. **ä¸æ‚£è€…æƒ…å†µçš„ç›¸å…³æ€§**: åˆ†æè¯¥æ–‡çŒ®ä¸æ‚£è€…æƒ…å†µçš„ç›¸å…³ç¨‹åº¦
3. **ä¸»è¦å‘ç°å’Œç»“è®º**: åˆ—å‡ºæ–‡çŒ®çš„å…³é”®å‘ç°
4. **è¯æ®ç­‰çº§è¯„ä¼°**: è¯„ä¼°è¯¥ç ”ç©¶çš„è¯æ®çº§åˆ«ï¼ˆå¦‚RCTã€å›é¡¾æ€§ç ”ç©¶ç­‰ï¼‰
5. **å¯¹æ‚£è€…çš„ä¸´åºŠæ„ä¹‰**: è¯´æ˜è¯¥æ–‡çŒ®å¯¹æ‚£è€…çš„å®é™…æŒ‡å¯¼æ„ä¹‰

è¯·ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼è¾“å‡ºï¼Œä¾¿äºé˜…è¯»ã€‚"""

            # ä½¿ç”¨ qwen-long ç›´æ¥è¯»å– PDF
            analysis = ""
            try:
                async for token in llm_service.chat_with_pdf_stream(
                        text=prompt,
                        pdf_path=pdf_path,
                        history=[]
                ):
                    analysis += token
                    yield {'type': 'token', 'step': 'analyze_papers', 'content': token}
            except Exception as e:
                error_msg = f"\n\nâš ï¸ åˆ†æå¤±è´¥: {str(e)}\n\n"
                yield {'type': 'token', 'step': 'analyze_papers', 'content': error_msg}
                continue

            state['paper_analyses'].append({
                'paper': paper,
                'analysis': analysis
            })

            yield {'type': 'token', 'step': 'analyze_papers', 'content': '\n\n---\n\n'}

    async def _analyze_trials(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤5: åˆ†æä¸´åºŠè¯•éªŒ"""

        if not state['trials']:
            yield {
                'type': 'token',
                'step': 'analyze_trials',
                'content': "â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ä¸´åºŠè¯•éªŒ\n\n"
            }
            return

        # æ ¼å¼åŒ–ä¸´åºŠè¯•éªŒä¿¡æ¯
        trials_text = []
        for i, trial in enumerate(state['trials']):
            trial_info = f"""### è¯•éªŒ {i+1}
- **NCT ID**: {trial.get('nct_id', 'N/A')}
- **æ ‡é¢˜**: {trial.get('title', 'N/A')}
- **çŠ¶æ€**: {trial.get('status', 'N/A')}
- **é˜¶æ®µ**: {trial.get('phase', 'N/A')}
- **ç ”ç©¶ç±»å‹**: {trial.get('study_type', 'N/A')}
- **ç–¾ç—…/æ¡ä»¶**: {trial.get('conditions', 'N/A')}
- **èµåŠ©æ–¹**: {trial.get('sponsor', 'N/A')}
- **åœ°ç‚¹**: {trial.get('locations', 'N/A')}
"""
            trials_text.append(trial_info)

        prompt = f"""åŸºäºæ‚£è€…ç‰¹å¾è¯„ä¼°ä»¥ä¸‹ä¸´åºŠè¯•éªŒçš„é€‚é…æ€§ï¼š

### æ‚£è€…ç‰¹å¾
{state['patient_features']}

### ä¸´åºŠè¯•éªŒåˆ—è¡¨
{chr(10).join(trials_text)}

### åˆ†æä»»åŠ¡
è¯·é’ˆå¯¹æ¯ä¸ªè¯•éªŒè¿›è¡Œè¯„ä¼°ï¼š

1. **é€‚é…åº¦è¯„åˆ†** (0-100åˆ†): è¯„ä¼°è¯¥è¯•éªŒä¸æ‚£è€…çš„åŒ¹é…ç¨‹åº¦
2. **å…¥ç»„æ ‡å‡†åˆ†æ**: åˆ†ææ‚£è€…æ˜¯å¦ç¬¦åˆå…¥ç»„æ¡ä»¶
3. **æ’é™¤æ ‡å‡†è€ƒé‡**: è¯„ä¼°æ˜¯å¦å­˜åœ¨æ’é™¤å› ç´ 
4. **è¯•éªŒä¼˜åŠ¿**: è¯´æ˜è¯¥è¯•éªŒçš„ä¼˜åŠ¿å’Œç‰¹ç‚¹
5. **æ½œåœ¨é£é™©**: æç¤ºå¯èƒ½çš„é£é™©
6. **æ¨èç­‰çº§**: ç»™å‡ºæ¨èçº§åˆ«ï¼ˆå¼ºçƒˆæ¨è/æ¨è/è°¨æ…æ¨è/ä¸æ¨èï¼‰

æœ€åç»™å‡º**ç»¼åˆå»ºè®®**ï¼Œè¯´æ˜æœ€é€‚åˆçš„1-2ä¸ªè¯•éªŒã€‚"""

        messages = [{"role": "user", "content": prompt}]

        analysis = ""
        async for token in llm_service.chat_stream(
                messages=messages,
                model=settings.qwen_long_model
        ):
            analysis += token
            yield {'type': 'token', 'step': 'analyze_trials', 'content': token}

        state['trial_analysis'] = analysis

    async def _generate_final(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""

        # æ±‡æ€»æ–‡çŒ®åˆ†æ
        papers_summary = []
        for i, item in enumerate(state['paper_analyses']):
            summary = f"""### æ–‡çŒ® {i+1}: {item['paper']['title']}

{item['analysis'][:500]}...

[æŸ¥çœ‹å®Œæ•´åˆ†æè¯·å‚è€ƒä¸Šæ–‡]
"""
            papers_summary.append(summary)

        prompt = f"""è¯·åŸºäºæ‰€æœ‰åˆ†æç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„æœ€ç»ˆæŠ¥å‘Šï¼š

### åŸå§‹é—®é¢˜
{state['user_query']}

### æ‚£è€…ç‰¹å¾æ‘˜è¦
{state['patient_features'][:500]}...

### æ–‡çŒ®åˆ†ææ±‡æ€»
{chr(10).join(papers_summary)}

### ä¸´åºŠè¯•éªŒåˆ†ææ‘˜è¦
{state['trial_analysis'][:500]}...

### æŠ¥å‘Šè¦æ±‚
è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„åŒ»ç–—å’¨è¯¢æŠ¥å‘Šï¼ŒåŒ…å«ï¼š

1. **æ‰§è¡Œæ‘˜è¦**: ç®€è¦æ€»ç»“æœ¬æ¬¡åˆ†æçš„æ ¸å¿ƒå†…å®¹
2. **æ²»ç–—æ–¹æ¡ˆå»ºè®®**: åŸºäºæ–‡çŒ®åˆ†æï¼Œæä¾›æ²»ç–—æ–¹æ¡ˆå»ºè®®
3. **ä¸´åºŠè¯•éªŒæ¨è**: æ¨èæœ€é€‚åˆçš„1-2ä¸ªä¸´åºŠè¯•éªŒ
4. **æ³¨æ„äº‹é¡¹**: æç¤ºéœ€è¦æ³¨æ„çš„é£é™©å’Œé—®é¢˜
5. **åç»­è¡ŒåŠ¨å»ºè®®**: ç»™å‡ºå…·ä½“çš„ä¸‹ä¸€æ­¥å»ºè®®

è¯·ä¿æŒä¸“ä¸šã€å®¢è§‚ï¼Œä½¿ç”¨æ˜“æ‡‚çš„è¯­è¨€ã€‚"""

        messages = [{"role": "user", "content": prompt}]

        final_answer = ""
        async for token in llm_service.chat_stream(messages=messages):
            final_answer += token
            yield {'type': 'token', 'step': 'generate_final', 'content': token}

        state['final_answer'] = final_answer

    async def _load_history(self, conversation_id: int) -> List[Dict]:
        """åŠ è½½å†å²å¯¹è¯"""
        async with get_db_session() as db:
            result = await db.execute(
                select(Message)
                .where(Message.conversation_id == conversation_id)
                .order_by(Message.created_at.desc())
                .limit(10)
            )
            messages = result.scalars().all()

            return [
                {
                    'type': 'user' if m.message_type == MessageType.USER else 'assistant',
                    'content': m.content
                }
                for m in reversed(list(messages))
            ]

    async def _save_final_result(self, state: WorkflowState, execution_id: int):
        """ä¿å­˜æœ€ç»ˆç»“æœåˆ°æ•°æ®åº“"""
        async with get_db_session() as db:
            # æ‹¼æ¥å®Œæ•´çš„ AI å›ç­”
            full_content = f"""# å¤šæºæ£€ç´¢åˆ†ææŠ¥å‘Š

## 1. æ‚£è€…ç‰¹å¾åˆ†æ
{state['patient_features']}

## 2. æ£€ç´¢æ¡ä»¶
- **PubMed**: {state['pubmed_query']}
- **ä¸´åºŠè¯•éªŒ**: {state['clinical_trial_keywords']}

## 3. æ£€ç´¢ç»“æœ
- **æ–‡çŒ®æ•°é‡**: {len(state['papers'])}
- **ä¸´åºŠè¯•éªŒæ•°é‡**: {len(state['trials'])}

## 4. æ–‡çŒ®åˆ†æ
"""

            # æ·»åŠ æ¯ç¯‡æ–‡çŒ®çš„åˆ†æ
            for i, item in enumerate(state['paper_analyses']):
                full_content += f"\n### æ–‡çŒ® {i+1}: {item['paper']['title']}\n\n"
                full_content += f"{item['analysis']}\n\n---\n\n"

            # æ·»åŠ ä¸´åºŠè¯•éªŒåˆ†æ
            full_content += f"\n## 5. ä¸´åºŠè¯•éªŒåˆ†æ\n\n{state['trial_analysis']}\n\n"

            # æ·»åŠ æœ€ç»ˆæŠ¥å‘Š
            full_content += f"\n## 6. æœ€ç»ˆæŠ¥å‘Š\n\n{state['final_answer']}\n"

            # ä¿å­˜ä¸ºæ¶ˆæ¯
            from app.crud import message as crud_message
            from app.schemas.message import MessageCreateSchema

            message_schema = MessageCreateSchema(
                conversation_id=state['conversation_id'],
                content=full_content,
                message_type=MessageType.ASSISTANT,
                attachments=[]
            )

            saved_message = await crud_message.create_message(
                db,
                message_schema=message_schema,
                user_id=state['user_id']
            )

            # æ›´æ–°æ‰§è¡Œè®°å½•
            execution = await db.get(WorkflowExecution, execution_id)
            execution.result_message_id = saved_message['id']
            execution.patient_features = state['patient_features']
            execution.search_queries = json.dumps({
                'pubmed': state['pubmed_query'],
                'clinical_trial': state['clinical_trial_keywords']
            })
            await db.commit()


# å…¨å±€å®ä¾‹
streaming_workflow = MultiSourceWorkflow()