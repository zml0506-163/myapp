"""
å·¥ä½œæµæœåŠ¡
app/services/workflow_service.py
"""
import os
import json
from typing import TypedDict, AsyncGenerator, List, Dict
import asyncio
from sqlalchemy import select, func

from app.core.config import settings
from app.db.database import get_db_session
from app.services.llm_service import llm_service
from app.services.search_service import search_service
from app.prompts.workflow_prompts import WorkflowPrompts
from app.models import WorkflowExecution, Message, MessageType
from app.crud import message as crud_message
from app.schemas.message import MessageCreateSchema


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    conversation_id: int
    user_id: int
    user_query: str
    user_attachments: List[Dict]
    history_messages: List[Dict]
    patient_features: str
    pubmed_query: str
    clinical_trial_keywords: str
    papers: List[Dict]
    trials: List[Dict]
    paper_analyses: List[Dict]
    trial_analysis: str
    final_answer: str
    current_step: str
    errors: List[str]


class WorkflowService:
    """ä¼˜åŒ–çš„å·¥ä½œæµæœåŠ¡"""

    def __init__(self):
        self.prompts = WorkflowPrompts()

    async def execute_with_streaming(
            self,
            conversation_id: int,
            user_id: int,
            user_query: str,
            user_attachments: List[Dict] = None,
            is_first_conversation: bool = False
    ) -> AsyncGenerator[Dict, None]:
        """æ‰§è¡Œå·¥ä½œæµå¹¶æµå¼è¾“å‡º"""

        execution_id = await self._create_execution(conversation_id, user_id)

        state: WorkflowState = {
            'conversation_id': conversation_id,
            'user_id': user_id,
            'user_query': user_query,
            'user_attachments': user_attachments or [],
            'history_messages': await self._load_history(conversation_id),
            'patient_features': '',
            'pubmed_query': '',
            'clinical_trial_keywords': '',
            'papers': [],
            'trials': [],
            'paper_analyses': [],
            'trial_analysis': '',
            'final_answer': '',
            'current_step': '',
            'errors': []
        }

        try:
            # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            async for chunk in self._step_extract_features(state):
                yield chunk
                # æ·»åŠ å»¶è¿Ÿç¡®ä¿å‰ç«¯æ¥æ”¶
                await asyncio.sleep(0.01)

            async for chunk in self._step_generate_queries(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_search(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_analyze_papers(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_analyze_trials(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_generate_final(state):
                yield chunk
                await asyncio.sleep(0.01)

            # ä¿å­˜ç»“æœ
            await self._save_result(state, execution_id)
            await self._update_execution(execution_id, 'completed')

            # ç”Ÿæˆæ ‡é¢˜
            if is_first_conversation:
                await self._generate_title(state, conversation_id, user_id)

            # æœ€ç»ˆå®Œæˆä¿¡å·
            yield {'type': 'done', 'content': ''}

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error_detail}")

            await self._update_execution(execution_id, 'failed', str(e))
            yield {
                'type': 'error',
                'step': state.get('current_step', 'unknown'),
                'content': f'âŒ æ‰§è¡Œå¤±è´¥: {str(e)}'
            }

    async def _step_extract_features(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤1: æå–æ‚£è€…ç‰¹å¾ï¼ˆä¿®å¤æ—¥å¿—è¾“å‡ºï¼‰"""
        state['current_step'] = 'extract_features'

        # å¼€å§‹åŒºå—
        yield {
            'type': 'section_start',
            'step': 'extract_features',
            'title': 'ğŸ” æå–æ‚£è€…ç‰¹å¾',
            'collapsible': True
        }

        # ç«‹å³è¾“å‡ºæ—¥å¿—
        yield {
            'type': 'log',
            'step': 'extract_features',
            'source': 'extract_features',
            'content': 'æ­£åœ¨åˆ†ææ‚£è€…ä¿¡æ¯...\n',
            'newline': True
        }

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if state['history_messages']:
            context_parts.append("### å†å²å¯¹è¯")
            for msg in state['history_messages'][-5:]:
                role = "ç”¨æˆ·" if msg['type'] == 'user' else "AI"
                context_parts.append(f"**{role}**: {msg['content'][:200]}...")

        if state['user_attachments']:
            context_parts.append("\n### ç”¨æˆ·ä¸Šä¼ çš„é™„ä»¶")
            for att in state['user_attachments']:
                context_parts.append(f"- {att['original_filename']}")

        context = "\n".join(context_parts)
        prompt = self.prompts.extract_features(context, state['user_query'])

        full_response = ""

        try:
            # å¤„ç†é™„ä»¶
            file_ids = []
            if state['user_attachments']:
                from app.services.file_service import file_service
                file_ids, only_images = await file_service.process_attachments(
                    state['user_attachments']
                )

                # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œä½¿ç”¨VLæ¨¡å‹
                if only_images and len(file_ids) == 1:
                    image_att = state['user_attachments'][0]
                    async for token in llm_service.chat_with_image_stream(
                            text=prompt,
                            image_path=image_att['file_path'],
                            history=[]
                    ):
                        full_response += token
                else:
                    # ä½¿ç”¨ç»Ÿä¸€æ¥å£
                    async for token in llm_service.chat_with_context(
                            user_query=prompt,
                            file_ids=file_ids,
                            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚",
                            model=settings.qwen_long_model
                    ):
                        full_response += token
            else:
                # æ— é™„ä»¶ï¼šæ™®é€šå¯¹è¯
                async for token in llm_service.chat_with_context(
                        user_query=prompt,
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚"
                ):
                    full_response += token

            state['patient_features'] = full_response

            # è¾“å‡ºç»“æœ
            yield {
                'type': 'result',
                'step': 'extract_features',
                'content': full_response,
                'summary': 'âœ… ç‰¹å¾æå–å®Œæˆ'
            }

        except Exception as e:
            error_msg = f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
            yield {
                'type': 'log',
                'step': 'extract_features',
                'source': 'extract_features',
                'content': error_msg,
                'newline': True
            }
            state['errors'].append(f'extract_features: {str(e)}')

        # ç»“æŸåŒºå—
        yield {'type': 'section_end', 'step': 'extract_features'}

    async def _step_generate_queries(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢æ¡ä»¶"""
        state['current_step'] = 'generate_queries'

        yield {
            'type': 'section_start',
            'step': 'generate_queries',
            'title': 'ğŸ” ç”Ÿæˆæ£€ç´¢æ¡ä»¶',
            'collapsible': True
        }

        yield {
            'type': 'log',
            'step': 'generate_queries',
            'source': 'generate_queries',
            'content': 'æ­£åœ¨ç”Ÿæˆæ£€ç´¢æ¡ä»¶...\n',
            'newline': True
        }

        prompt = self.prompts.generate_queries(state['patient_features'])
        full_response = ""

        try:
            async for token in llm_service.chat_with_context(
                    user_query=prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ£€ç´¢æ¡ä»¶ç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                full_response += token

            # è§£æJSON
            start = full_response.find('{')
            end = full_response.rfind('}') + 1
            if start != -1 and end > start:
                queries = json.loads(full_response[start:end])
                state['pubmed_query'] = queries.get('pubmed_query', '')
                state['clinical_trial_keywords'] = queries.get('clinical_trial_keywords', '')
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON")

            yield {
                'type': 'result',
                'step': 'generate_queries',
                'content': f"""**PubMed æ£€ç´¢å¼**: `{state['pubmed_query']}`

**ä¸´åºŠè¯•éªŒå…³é”®è¯**: `{state['clinical_trial_keywords']}`""",
                'summary': 'âœ… æ£€ç´¢æ¡ä»¶ç”Ÿæˆå®Œæˆ',
                'data': {
                    'pubmed_query': state['pubmed_query'],
                    'clinical_trial_keywords': state['clinical_trial_keywords']
                }
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_queries',
                'source': 'generate_queries',
                'content': f'âš ï¸ è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¡ä»¶\n',
                'newline': True
            }
            state['pubmed_query'] = state['user_query']
            state['clinical_trial_keywords'] = state['user_query']

        yield {'type': 'section_end', 'step': 'generate_queries'}

    async def _step_search(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤3: æ‰§è¡Œæ£€ç´¢ï¼ˆä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼‰"""
        state['current_step'] = 'search'

        yield {
            'type': 'section_start',
            'step': 'search',
            'title': 'ğŸ“š æ‰§è¡Œå¤šæºæ£€ç´¢',
            'collapsible': True
        }

        progress_queue = asyncio.Queue()
        target_count = settings.max_search_results

        async def search_all():
            """æ‰§è¡Œæ£€ç´¢ä»»åŠ¡"""
            try:
                papers = await search_service.search_papers_with_ranking(
                    state['pubmed_query'],
                    target_count,
                    progress_queue
                )
                state['papers'].extend(papers)

                trials = await search_service.search_trials_with_ranking(
                    state['clinical_trial_keywords'],
                    target_count,
                    progress_queue
                )
                state['trials'].extend(trials)

            except Exception as e:
                await progress_queue.put({
                    'type': 'log',
                    'source': 'search',
                    'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                    'newline': True
                })
            finally:
                await progress_queue.put({'type': 'DONE'})

        # å¯åŠ¨æ£€ç´¢ä»»åŠ¡
        search_task = asyncio.create_task(search_all())

        # è½¬å‘è¿›åº¦æ¶ˆæ¯
        while True:
            msg = await progress_queue.get()

            if isinstance(msg, dict):
                if msg.get('type') == 'DONE':
                    break
                elif msg.get('type') in ('log', 'result'):
                    # ç›´æ¥è½¬å‘
                    yield msg

        await search_task

        # æ±‡æ€»ç»“æœ
        yield {
            'type': 'result',
            'step': 'search',
            'content': f"""### ğŸ“Š æ£€ç´¢æ±‡æ€»

- **æ–‡çŒ®æ€»æ•°**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒ**: {len(state['trials'])} ä¸ª""",
            'summary': f'âœ… æ£€ç´¢å®Œæˆï¼ˆ{len(state["papers"])} ç¯‡æ–‡çŒ®ï¼Œ{len(state["trials"])} ä¸ªè¯•éªŒï¼‰',
            'data': {
                'paper_count': len(state['papers']),
                'trial_count': len(state['trials'])
            }
        }

        yield {'type': 'section_end', 'step': 'search'}

    async def _step_analyze_papers(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤4: åˆ†ææ–‡çŒ®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼‰"""
        state['current_step'] = 'analyze_papers'

        yield {
            'type': 'section_start',
            'step': 'analyze_papers',
            'title': 'ğŸ“„ åˆ†ææ–‡çŒ®',
            'collapsible': True
        }

        if not state['papers']:
            yield {
                'type': 'result',
                'step': 'analyze_papers',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®',
                'summary': 'â„¹ï¸ æ— æ–‡çŒ®å¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_papers'}
            return

        from app.services.file_service import file_service

        for i, paper in enumerate(state['papers']):
            yield {
                'type': 'log',
                'step': 'analyze_papers',
                'source': 'analyze_papers',
                'content': f'\nğŸ“„ åˆ†ææ–‡çŒ® {i+1}/{len(state["papers"])}: {paper["title"][:50]}...\n',
                'newline': True
            }

            pdf_path = paper.get('pdf_path')
            if not pdf_path or not os.path.exists(pdf_path):
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'source': 'analyze_papers',
                    'content': 'âš ï¸ PDFä¸å­˜åœ¨ï¼Œè·³è¿‡\n',
                    'newline': True
                }
                continue

            prompt = self.prompts.analyze_paper(
                state['patient_features'],
                state['user_query'],
                paper
            )

            analysis = ""
            try:
                # è·å–æ–‡ä»¶ID
                file_id = await file_service.get_or_upload_file(pdf_path)

                if not file_id:
                    raise Exception("æ–‡ä»¶ä¸Šä¼ å¤±è´¥")

                # ä½¿ç”¨ç»Ÿä¸€æ¥å£åˆ†æ
                async for token in llm_service.chat_with_context(
                        user_query=prompt,
                        file_ids=[file_id],
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ–‡çŒ®åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»PDFæ–‡æ¡£ï¼ŒæŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºç»“æ„åŒ–åˆ†æã€‚",
                        model=settings.qwen_long_model
                ):
                    analysis += token

                state['paper_analyses'].append({
                    'paper': paper,
                    'analysis': analysis
                })

                state['paper_analyses'].append({
                    'paper': paper,
                    'analysis': analysis
                })

                yield {
                    'type': 'result',
                    'step': 'analyze_papers',
                    'content': f"""### æ–‡çŒ® {i+1}: {paper['title']}

{analysis}""",
                    'data': {
                        'paper_id': paper.get('id'),
                        'pmid': paper.get('pmid'),
                        'title': paper['title']
                    }
                }

            except Exception as e:
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'source': 'analyze_papers',
                    'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n',
                    'newline': True
                }

        yield {
            'type': 'result',
            'step': 'analyze_papers',
            'content': '',
            'summary': f'âœ… æ–‡çŒ®åˆ†æå®Œæˆï¼ˆ{len(state["paper_analyses"])} ç¯‡ï¼‰'
        }

        yield {'type': 'section_end', 'step': 'analyze_papers'}

    async def _step_analyze_trials(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤5: åˆ†æä¸´åºŠè¯•éªŒ"""
        state['current_step'] = 'analyze_trials'

        yield {
            'type': 'section_start',
            'step': 'analyze_trials',
            'title': 'ğŸ’Š åˆ†æä¸´åºŠè¯•éªŒ',
            'collapsible': True
        }

        if not state['trials']:
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ä¸´åºŠè¯•éªŒ',
                'summary': 'â„¹ï¸ æ— è¯•éªŒå¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_trials'}
            return

        yield {
            'type': 'log',
            'step': 'analyze_trials',
            'source': 'analyze_trials',
            'content': f'æ­£åœ¨åˆ†æ {len(state["trials"])} ä¸ªä¸´åºŠè¯•éªŒ...\n',
            'newline': True
        }

        trials_text = []
        for i, trial in enumerate(state['trials']):
            trial_info = f"""### è¯•éªŒ {i+1}: {trial.get('title')}
- **NCT ID**: {trial.get('nct_id')}
- **çŠ¶æ€**: {trial.get('status')}
- **é˜¶æ®µ**: {trial.get('phase')}
- **ç–¾ç—…**: {trial.get('conditions')}
- **èµåŠ©æ–¹**: {trial.get('sponsor')}
"""
            trials_text.append(trial_info)

        prompt = self.prompts.analyze_trials(
            state['patient_features'],
            '\n'.join(trials_text)
        )

        analysis = ""
        try:
            async for token in llm_service.chat_with_context(
                    user_query=prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸´åºŠè¯•éªŒåˆ†æåŠ©æ‰‹ã€‚",
                    model=settings.qwen_long_model
            ):
                analysis += token

            state['trial_analysis'] = analysis

            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': analysis,
                'summary': f'âœ… ä¸´åºŠè¯•éªŒåˆ†æå®Œæˆï¼ˆ{len(state["trials"])} ä¸ªï¼‰'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'analyze_trials',
                'source': 'analyze_trials',
                'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n',
                'newline': True
            }

        yield {'type': 'section_end', 'step': 'analyze_trials'}

    async def _step_generate_final(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        state['current_step'] = 'generate_final'

        yield {
            'type': 'section_start',
            'step': 'generate_final',
            'title': 'ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š',
            'collapsible': False
        }

        yield {
            'type': 'log',
            'step': 'generate_final',
            'source': 'generate_final',
            'content': 'æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š...\n',
            'newline': True
        }

        papers_summary = []
        for i, item in enumerate(state['paper_analyses']):
            summary = f"**æ–‡çŒ® {i+1}**: {item['paper']['title']} - {item['analysis'][:200]}..."
            papers_summary.append(summary)

        prompt = self.prompts.generate_final_report(
            state['user_query'],
            state['patient_features'],
            '\n'.join(papers_summary) if papers_summary else "æš‚æ— ",
            state['trial_analysis']
        )

        final_answer = ""
        try:
            async for token in llm_service.chat_with_context(
                    user_query=prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å’¨è¯¢æŠ¥å‘Šç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                final_answer += token
                # æµå¼è¾“å‡º
                yield {
                    'type': 'token',
                    'step': 'generate_final',
                    'content': token
                }

            state['final_answer'] = final_answer

            yield {
                'type': 'result',
                'step': 'generate_final',
                'content': '',
                'summary': 'âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_final',
                'source': 'generate_final',
                'content': f'âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n',
                'newline': True
            }

        yield {'type': 'section_end', 'step': 'generate_final'}

    async def _generate_title(self, state: WorkflowState, conversation_id: int, user_id: int):
        """ç”Ÿæˆå¯¹è¯æ ‡é¢˜"""
        try:
            title_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹åŒ»ç–—å’¨è¯¢å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼ˆä¸è¶…è¿‡15ä¸ªå­—ï¼‰ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

æ‚£è€…ç‰¹å¾ï¼š{state['patient_features'][:300]}...

è¦æ±‚ï¼š
1. çªå‡ºç–¾ç—…/ç—‡çŠ¶å…³é”®è¯
2. ä¸è¶…è¿‡15ä¸ªå­—
3. ç›´æ¥è¾“å‡ºæ ‡é¢˜ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹
4. ä¸ä½¿ç”¨å¼•å·ã€ä¹¦åå·ç­‰æ ‡ç‚¹ç¬¦å·

æ ‡é¢˜ï¼š"""

            new_title = ""
            async for token in llm_service.chat_with_context(
                    user_query=title_prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ ‡é¢˜ç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                new_title += token

            # æ¸…ç†æ ‡é¢˜
            new_title = new_title.strip().replace('\n', '').replace('"', '').replace("'", '')

            if new_title and len(new_title) > 2:
                if len(new_title) > 15:
                    new_title = new_title[:15] + "..."

                async with get_db_session() as db:
                    from app.schemas.conversation import ConversationUpdateSchema
                    from app.crud import conversation as crud_conversation
                    await crud_conversation.update_conversation(
                        db,
                        conversation_id=conversation_id,
                        conversation_schema=ConversationUpdateSchema(title=new_title),
                        user_id=user_id
                    )

                print(f"âœ… å¯¹è¯å·²è‡ªåŠ¨é‡å‘½åä¸ºã€Œ{new_title}ã€")

        except Exception as e:
            print(f"ç”Ÿæˆæ ‡é¢˜å¤±è´¥: {e}")

    async def _create_execution(self, conversation_id: int, user_id: int) -> int:
        """åˆ›å»ºæ‰§è¡Œè®°å½•"""
        async with get_db_session() as db:
            execution = WorkflowExecution(
                conversation_id=conversation_id,
                user_id=user_id,
                workflow_type='multi_source',
                status='running',
                current_step='initializing'
            )
            db.add(execution)
            await db.commit()
            return execution.id

    async def _update_execution(self, execution_id: int, status: str, error: str = None):
        """æ›´æ–°æ‰§è¡ŒçŠ¶æ€"""
        async with get_db_session() as db:
            execution = await db.get(WorkflowExecution, execution_id)
            execution.status = status
            if status == 'completed':
                execution.completed_at = func.now()
            if error:
                execution.error_message = error
            await db.commit()

    async def _load_history(self, conversation_id: int) -> List[Dict]:
        """åŠ è½½å†å²å¯¹è¯"""
        async with get_db_session() as db:
            result = await db.execute(
                select(Message)
                .where(Message.conversation_id == conversation_id)
                .order_by(Message.created_at.desc())
                .limit(10)
            )
            messages = result.scalars().all()

            return [
                {
                    'type': 'user' if m.message_type == MessageType.USER else 'assistant',
                    'content': m.content
                }
                for m in reversed(list(messages))
            ]

    async def _save_result(self, state: WorkflowState, execution_id: int):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        async with get_db_session() as db:
            full_content = f"""# å¤šæºæ£€ç´¢åˆ†ææŠ¥å‘Š

## 1. æ‚£è€…ç‰¹å¾åˆ†æ
{state['patient_features']}

---

## 2. æ£€ç´¢æ¡ä»¶
- **PubMed**: `{state['pubmed_query']}`
- **ä¸´åºŠè¯•éªŒ**: `{state['clinical_trial_keywords']}`

---

## 3. æ£€ç´¢ç»“æœ
- **æ–‡çŒ®æ•°é‡**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒæ•°é‡**: {len(state['trials'])} ä¸ª

---

## 4. æ–‡çŒ®åˆ†æ
"""

            if state['paper_analyses']:
                for i, item in enumerate(state['paper_analyses']):
                    full_content += f"\n### æ–‡çŒ® {i+1}: {item['paper']['title']}\n\n"
                    full_content += f"{item['analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— æ–‡çŒ®åˆ†æ\n\n---\n"

            full_content += f"\n## 5. ä¸´åºŠè¯•éªŒåˆ†æ\n\n"
            if state['trial_analysis']:
                full_content += f"{state['trial_analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— ä¸´åºŠè¯•éªŒåˆ†æ\n\n---\n"

            full_content += f"\n## 6. ç»¼åˆæŠ¥å‘Š\n\n{state['final_answer']}\n"

            message_schema = MessageCreateSchema(
                conversation_id=state['conversation_id'],
                content=full_content,
                message_type=MessageType.ASSISTANT,
                attachments=[]
            )

            saved_message = await crud_message.create_message(
                db,
                message_schema=message_schema,
                user_id=state['user_id']
            )

            execution = await db.get(WorkflowExecution, execution_id)
            execution.result_message_id = saved_message['id']
            execution.patient_features = state['patient_features']
            execution.search_queries = json.dumps({
                'pubmed': state['pubmed_query'],
                'clinical_trial': state['clinical_trial_keywords']
            })
            await db.commit()


workflow_service = WorkflowService()