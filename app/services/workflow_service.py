"""
å·¥ä½œæµæœåŠ¡ V3 - ä¼˜åŒ–ç‰ˆæœ¬
- ä½¿ç”¨ç‹¬ç«‹çš„æç¤ºè¯æ¨¡å—
- ä½¿ç”¨ä¼˜åŒ–çš„æ£€ç´¢æœåŠ¡
- æµç¨‹æ›´æ¸…æ™°
"""
import os
import json
from typing import TypedDict, AsyncGenerator, List, Dict
import asyncio
from sqlalchemy import select, func

from app.core.config import settings
from app.db.database import get_db_session
from app.services.llm_service import llm_service
from app.services.search_service import optimized_search_service
from app.prompts.workflow_prompts import WorkflowPrompts
from app.models import WorkflowExecution, Message, MessageType
from app.crud import message as crud_message
from app.schemas.message import MessageCreateSchema


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    conversation_id: int
    user_id: int
    user_query: str
    user_attachments: List[Dict]
    history_messages: List[Dict]
    patient_features: str
    pubmed_query: str
    clinical_trial_keywords: str
    papers: List[Dict]
    trials: List[Dict]
    paper_analyses: List[Dict]
    trial_analysis: str
    final_answer: str
    current_step: str
    errors: List[str]


class WorkflowService:
    """ä¼˜åŒ–çš„å·¥ä½œæµæœåŠ¡"""

    def __init__(self):
        self.prompts = WorkflowPrompts()

    async def execute_with_streaming(
            self,
            conversation_id: int,
            user_id: int,
            user_query: str,
            user_attachments: List[Dict] = None
    ) -> AsyncGenerator[Dict, None]:
        """æ‰§è¡Œå·¥ä½œæµå¹¶æµå¼è¾“å‡º"""

        # åˆ›å»ºæ‰§è¡Œè®°å½•
        execution_id = await self._create_execution(conversation_id, user_id)

        # åˆå§‹åŒ–çŠ¶æ€
        state: WorkflowState = {
            'conversation_id': conversation_id,
            'user_id': user_id,
            'user_query': user_query,
            'user_attachments': user_attachments or [],
            'history_messages': await self._load_history(conversation_id),
            'patient_features': '',
            'pubmed_query': '',
            'clinical_trial_keywords': '',
            'papers': [],
            'trials': [],
            'paper_analyses': [],
            'trial_analysis': '',
            'final_answer': '',
            'current_step': '',
            'errors': []
        }

        try:
            # æ‰§è¡Œæ­¥éª¤
            async for chunk in self._step_extract_features(state):
                yield chunk

            async for chunk in self._step_generate_queries(state):
                yield chunk

            async for chunk in self._step_search(state):
                yield chunk

            async for chunk in self._step_analyze_papers(state):
                yield chunk

            async for chunk in self._step_analyze_trials(state):
                yield chunk

            async for chunk in self._step_generate_final(state):
                yield chunk

            # ä¿å­˜ç»“æœ
            await self._save_result(state, execution_id)

            # æ›´æ–°æ‰§è¡ŒçŠ¶æ€
            await self._update_execution(execution_id, 'completed')

            yield {'type': 'done', 'content': ''}

        except Exception as e:
            await self._update_execution(execution_id, 'failed', str(e))
            yield {
                'type': 'error',
                'step': state.get('current_step', 'unknown'),
                'content': f'âŒ æ‰§è¡Œå¤±è´¥: {str(e)}'
            }

    async def _step_extract_features(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤1: æå–æ‚£è€…ç‰¹å¾"""
        state['current_step'] = 'extract_features'

        yield {
            'type': 'section_start',
            'step': 'extract_features',
            'title': 'æå–æ‚£è€…ç‰¹å¾',
            'collapsible': True
        }

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if state['history_messages']:
            context_parts.append("### å†å²å¯¹è¯")
            for msg in state['history_messages'][-5:]:
                role = "ç”¨æˆ·" if msg['type'] == 'user' else "AI"
                context_parts.append(f"**{role}**: {msg['content'][:200]}...")

        if state['user_attachments']:
            context_parts.append("\n### ç”¨æˆ·ä¸Šä¼ çš„é™„ä»¶")
            for att in state['user_attachments']:
                context_parts.append(f"- {att['original_filename']}")

        context = "\n".join(context_parts)

        # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
        prompt = self.prompts.extract_features(context, state['user_query'])
        messages = [{"role": "user", "content": prompt}]

        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
        image_attachments = [att for att in state['user_attachments']
                             if att.get('mime_type', '').startswith('image/')]

        full_response = ""

        yield {
            'type': 'log',
            'step': 'extract_features',
            'content': 'ğŸ¤” æ­£åœ¨åˆ†ææ‚£è€…ä¿¡æ¯...\n'
        }

        try:
            if image_attachments:
                for att in image_attachments:
                    async for token in llm_service.chat_with_image_stream(
                            text=prompt,
                            image_path=att['file_path'],
                            history=[]
                    ):
                        full_response += token
            else:
                async for token in llm_service.chat_stream(messages=messages):
                    full_response += token

            state['patient_features'] = full_response

            yield {
                'type': 'result',
                'step': 'extract_features',
                'content': full_response,
                'summary': 'âœ… æ‚£è€…ç‰¹å¾æå–å®Œæˆ'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'extract_features',
                'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
            }
            state['errors'].append(f'extract_features: {str(e)}')

        yield {'type': 'section_end', 'step': 'extract_features'}

    async def _step_generate_queries(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢æ¡ä»¶"""
        state['current_step'] = 'generate_queries'

        yield {
            'type': 'section_start',
            'step': 'generate_queries',
            'title': 'ç”Ÿæˆæ£€ç´¢æ¡ä»¶',
            'collapsible': True
        }

        # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
        prompt = self.prompts.generate_queries(state['patient_features'])
        messages = [{"role": "user", "content": prompt}]

        yield {
            'type': 'log',
            'step': 'generate_queries',
            'content': 'ğŸ” æ­£åœ¨ç”Ÿæˆæ£€ç´¢æ¡ä»¶...\n'
        }

        full_response = ""
        try:
            async for token in llm_service.chat_stream(messages=messages):
                full_response += token

            # è§£æJSON
            start = full_response.find('{')
            end = full_response.rfind('}') + 1
            if start != -1 and end > start:
                queries = json.loads(full_response[start:end])
                state['pubmed_query'] = queries.get('pubmed_query', '')
                state['clinical_trial_keywords'] = queries.get('clinical_trial_keywords', '')
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON")

            yield {
                'type': 'result',
                'step': 'generate_queries',
                'content': f"""**PubMed æ£€ç´¢å¼**: `{state['pubmed_query']}`

**ä¸´åºŠè¯•éªŒå…³é”®è¯**: `{state['clinical_trial_keywords']}`""",
                'summary': 'âœ… æ£€ç´¢æ¡ä»¶ç”Ÿæˆå®Œæˆ',
                'data': {
                    'pubmed_query': state['pubmed_query'],
                    'clinical_trial_keywords': state['clinical_trial_keywords']
                }
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_queries',
                'content': f'âš ï¸ è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¡ä»¶: {str(e)}\n'
            }
            state['pubmed_query'] = state['user_query']
            state['clinical_trial_keywords'] = state['user_query']
            state['errors'].append(f'generate_queries: {str(e)}')

        yield {'type': 'section_end', 'step': 'generate_queries'}

    async def _step_search(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤3: æ‰§è¡Œæ£€ç´¢ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„æ£€ç´¢æœåŠ¡ï¼‰"""
        state['current_step'] = 'search'

        yield {
            'type': 'section_start',
            'step': 'search',
            'title': 'æ‰§è¡Œå¤šæºæ£€ç´¢',
            'collapsible': True
        }

        progress_queue = asyncio.Queue()
        target_count = settings.max_search_results  # 5ç¯‡

        # å¯åŠ¨æ£€ç´¢ä»»åŠ¡
        async def search_all():
            # æ£€ç´¢æ–‡çŒ®ï¼ˆä¼šè‡ªåŠ¨æ’åºå’Œå»é‡ï¼‰
            papers = await optimized_search_service.search_papers_with_ranking(
                state['pubmed_query'],
                target_count,
                progress_queue
            )
            state['papers'].extend(papers)

            # æ£€ç´¢ä¸´åºŠè¯•éªŒï¼ˆä¼šè‡ªåŠ¨æ’åºï¼‰
            trials = await optimized_search_service.search_trials_with_ranking(
                state['clinical_trial_keywords'],
                target_count,
                progress_queue
            )
            state['trials'].extend(trials)

            await progress_queue.put({'type': 'DONE'})

        search_task = asyncio.create_task(search_all())

        # å¤„ç†è¿›åº¦æ¶ˆæ¯
        while True:
            msg = await progress_queue.get()

            if isinstance(msg, dict):
                if msg.get('type') == 'DONE':
                    break
                elif msg.get('type') in ('log', 'result'):
                    yield msg

        await search_task

        # æ±‡æ€»ç»“æœ
        yield {
            'type': 'result',
            'step': 'search',
            'content': f"""### ğŸ“Š æ£€ç´¢æ±‡æ€»

- **æ–‡çŒ®æ€»æ•°**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒ**: {len(state['trials'])} ä¸ª""",
            'summary': f'âœ… å¤šæºæ£€ç´¢å®Œæˆï¼ˆ{len(state["papers"])} ç¯‡æ–‡çŒ®ï¼Œ{len(state["trials"])} ä¸ªè¯•éªŒï¼‰',
            'data': {
                'paper_count': len(state['papers']),
                'trial_count': len(state['trials'])
            }
        }

        yield {'type': 'section_end', 'step': 'search'}

    async def _step_analyze_papers(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤4: åˆ†ææ–‡çŒ®ï¼ˆä½¿ç”¨æç¤ºè¯æ¨¡æ¿ï¼‰"""
        state['current_step'] = 'analyze_papers'

        yield {
            'type': 'section_start',
            'step': 'analyze_papers',
            'title': 'åˆ†ææ–‡çŒ®',
            'collapsible': True
        }

        if not state['papers']:
            yield {
                'type': 'result',
                'step': 'analyze_papers',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®',
                'summary': 'â„¹ï¸ æ— æ–‡çŒ®å¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_papers'}
            return

        for i, paper in enumerate(state['papers']):
            yield {
                'type': 'log',
                'step': 'analyze_papers',
                'content': f'\nğŸ“„ åˆ†ææ–‡çŒ® {i+1}/{len(state["papers"])}: {paper["title"]}\n'
            }

            pdf_path = paper.get('pdf_path')
            if not pdf_path or not os.path.exists(pdf_path):
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'content': 'âš ï¸ PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡\n'
                }
                continue

            # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
            prompt = self.prompts.analyze_paper(
                state['patient_features'],
                state['user_query'],
                paper
            )

            analysis = ""
            try:
                async for token in llm_service.chat_with_pdf_stream(
                        text=prompt,
                        pdf_path=pdf_path,
                        history=[]
                ):
                    analysis += token

                state['paper_analyses'].append({
                    'paper': paper,
                    'analysis': analysis
                })

                yield {
                    'type': 'result',
                    'step': 'analyze_papers',
                    'content': f"""### æ–‡çŒ® {i+1}: {paper['title']}

{analysis}""",
                    'data': {
                        'paper_id': paper.get('id'),
                        'pmid': paper.get('pmid'),
                        'title': paper['title']
                    }
                }

            except Exception as e:
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
                }

        yield {
            'type': 'result',
            'step': 'analyze_papers',
            'content': '',
            'summary': f'âœ… æ–‡çŒ®åˆ†æå®Œæˆï¼ˆ{len(state["paper_analyses"])} ç¯‡ï¼‰'
        }

        yield {'type': 'section_end', 'step': 'analyze_papers'}

    async def _step_analyze_trials(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤5: åˆ†æä¸´åºŠè¯•éªŒï¼ˆä½¿ç”¨æç¤ºè¯æ¨¡æ¿ï¼‰"""
        state['current_step'] = 'analyze_trials'

        yield {
            'type': 'section_start',
            'step': 'analyze_trials',
            'title': 'åˆ†æä¸´åºŠè¯•éªŒ',
            'collapsible': True
        }

        if not state['trials']:
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ä¸´åºŠè¯•éªŒ',
                'summary': 'â„¹ï¸ æ— è¯•éªŒå¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_trials'}
            return

        yield {
            'type': 'log',
            'step': 'analyze_trials',
            'content': f'ğŸ¤” æ­£åœ¨åˆ†æ {len(state["trials"])} ä¸ªä¸´åºŠè¯•éªŒ...\n'
        }

        # æ ¼å¼åŒ–è¯•éªŒä¿¡æ¯
        trials_text = []
        for i, trial in enumerate(state['trials']):
            trial_info = f"""### è¯•éªŒ {i+1}: {trial.get('title')}
- **NCT ID**: {trial.get('nct_id')}
- **çŠ¶æ€**: {trial.get('status')}
- **é˜¶æ®µ**: {trial.get('phase')}
- **ç–¾ç—…**: {trial.get('conditions')}
- **èµåŠ©æ–¹**: {trial.get('sponsor')}
"""
            trials_text.append(trial_info)

        # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
        prompt = self.prompts.analyze_trials(
            state['patient_features'],
            chr(10).join(trials_text)
        )

        messages = [{"role": "user", "content": prompt}]

        analysis = ""
        try:
            async for token in llm_service.chat_stream(
                    messages=messages,
                    model=settings.qwen_long_model
            ):
                analysis += token

            state['trial_analysis'] = analysis

            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': analysis,
                'summary': f'âœ… ä¸´åºŠè¯•éªŒåˆ†æå®Œæˆï¼ˆ{len(state["trials"])} ä¸ªï¼‰'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'analyze_trials',
                'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
            }

        yield {'type': 'section_end', 'step': 'analyze_trials'}

    async def _step_generate_final(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆä½¿ç”¨æç¤ºè¯æ¨¡æ¿ï¼‰"""
        state['current_step'] = 'generate_final'

        yield {
            'type': 'section_start',
            'step': 'generate_final',
            'title': 'ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š',
            'collapsible': False
        }

        yield {
            'type': 'log',
            'step': 'generate_final',
            'content': 'ğŸ“ æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š...\n'
        }

        # æ±‡æ€»æ–‡çŒ®åˆ†æ
        papers_summary = []
        for i, item in enumerate(state['paper_analyses']):
            summary = f"**æ–‡çŒ® {i+1}**: {item['paper']['title']} - {item['analysis'][:200]}..."
            papers_summary.append(summary)

        # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
        prompt = self.prompts.generate_final_report(
            state['user_query'],
            state['patient_features'],
            chr(10).join(papers_summary) if papers_summary else "æš‚æ— ",
            state['trial_analysis']
        )

        messages = [{"role": "user", "content": prompt}]

        final_answer = ""
        try:
            async for token in llm_service.chat_stream(messages=messages):
                final_answer += token

            state['final_answer'] = final_answer

            yield {
                'type': 'result',
                'step': 'generate_final',
                'content': final_answer,
                'summary': 'âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_final',
                'content': f'âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n'
            }

        yield {'type': 'section_end', 'step': 'generate_final'}

    async def _create_execution(self, conversation_id: int, user_id: int) -> int:
        """åˆ›å»ºæ‰§è¡Œè®°å½•"""
        async with get_db_session() as db:
            execution = WorkflowExecution(
                conversation_id=conversation_id,
                user_id=user_id,
                workflow_type='multi_source',
                status='running',
                current_step='initializing'
            )
            db.add(execution)
            await db.commit()
            return execution.id

    async def _update_execution(self, execution_id: int, status: str, error: str = None):
        """æ›´æ–°æ‰§è¡ŒçŠ¶æ€"""
        async with get_db_session() as db:
            execution = await db.get(WorkflowExecution, execution_id)
            execution.status = status
            if status == 'completed':
                execution.completed_at = func.now()
            if error:
                execution.error_message = error
            await db.commit()

    async def _load_history(self, conversation_id: int) -> List[Dict]:
        """åŠ è½½å†å²å¯¹è¯"""
        async with get_db_session() as db:
            result = await db.execute(
                select(Message)
                .where(Message.conversation_id == conversation_id)
                .order_by(Message.created_at.desc())
                .limit(10)
            )
            messages = result.scalars().all()

            return [
                {
                    'type': 'user' if m.message_type == MessageType.USER else 'assistant',
                    'content': m.content
                }
                for m in reversed(list(messages))
            ]

    async def _save_result(self, state: WorkflowState, execution_id: int):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        async with get_db_session() as db:
            full_content = f"""# å¤šæºæ£€ç´¢åˆ†ææŠ¥å‘Š

## 1. æ‚£è€…ç‰¹å¾åˆ†æ
{state['patient_features']}

---

## 2. æ£€ç´¢æ¡ä»¶
- **PubMed**: `{state['pubmed_query']}`
- **ä¸´åºŠè¯•éªŒ**: `{state['clinical_trial_keywords']}`

---

## 3. æ£€ç´¢ç»“æœ
- **æ–‡çŒ®æ•°é‡**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒæ•°é‡**: {len(state['trials'])} ä¸ª

---

## 4. æ–‡çŒ®åˆ†æ
"""

            if state['paper_analyses']:
                for i, item in enumerate(state['paper_analyses']):
                    full_content += f"\n### æ–‡çŒ® {i+1}: {item['paper']['title']}\n\n"
                    full_content += f"{item['analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— æ–‡çŒ®åˆ†æ\n\n---\n"

            full_content += f"\n## 5. ä¸´åºŠè¯•éªŒåˆ†æ\n\n"
            if state['trial_analysis']:
                full_content += f"{state['trial_analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— ä¸´åºŠè¯•éªŒåˆ†æ\n\n---\n"

            full_content += f"\n## 6. ç»¼åˆæŠ¥å‘Š\n\n{state['final_answer']}\n"

            message_schema = MessageCreateSchema(
                conversation_id=state['conversation_id'],
                content=full_content,
                message_type=MessageType.ASSISTANT,
                attachments=[]
            )

            saved_message = await crud_message.create_message(
                db,
                message_schema=message_schema,
                user_id=state['user_id']
            )

            execution = await db.get(WorkflowExecution, execution_id)
            execution.result_message_id = saved_message['id']
            execution.patient_features = state['patient_features']
            execution.search_queries = json.dumps({
                'pubmed': state['pubmed_query'],
                'clinical_trial': state['clinical_trial_keywords']
            })
            await db.commit()


# å…¨å±€å®ä¾‹
workflow_service = WorkflowService()