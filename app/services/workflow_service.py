"""
å·¥ä½œæµæœåŠ¡
app/services/workflow_service.py
"""
import os
import json
from typing import TypedDict, AsyncGenerator, List, Dict, Optional
import asyncio
import logging
from sqlalchemy import select, func, update

from app.core.config import settings
from app.db.database import get_db_session
from app.services.llm_service import llm_service
from app.services.search_service import search_service
from app.prompts.workflow_prompts import WorkflowPrompts
from app.models import WorkflowExecution, Message, MessageType, MessageStatus
from app.crud import message as crud_message
from app.schemas.message import MessageCreateSchema
from app.core.logger import get_logger

logger = get_logger(__name__)
logging.basicConfig(level=logging.INFO)


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    conversation_id: int
    user_id: int
    user_query: str
    user_attachments: List[Dict]
    history_messages: List[Dict]
    patient_features: str
    pubmed_query: str
    europepmc_query: str  # æ–°å¢ï¼šEurope PMC æ£€ç´¢æ¡ä»¶
    clinical_trial_keywords: str
    papers: List[Dict]
    trials: List[Dict]
    paper_analyses: List[Dict]
    trial_analysis: str
    final_answer: str
    current_step: str
    errors: List[str]


class WorkflowService:
    """ä¼˜åŒ–çš„å·¥ä½œæµæœåŠ¡"""

    def __init__(self):
        self.prompts = WorkflowPrompts()

    async def execute_with_streaming(
            self,
            conversation_id: int,
            user_id: int,
            user_query: str,
            message_id: int,  # æ·»åŠ  message_id å‚æ•°
            user_attachments: Optional[List[Dict]] = None,
            is_first_conversation: bool = False
    ) -> AsyncGenerator[Dict, None]:
        """æ‰§è¡Œå·¥ä½œæµå¹¶æµå¼è¾“å‡º"""

        execution_id = await self._create_execution(conversation_id, user_id)
        logger.info(f"å¼€å§‹æ‰§è¡Œå·¥ä½œæµï¼Œå¯¹è¯ID: {conversation_id}, æ¶ˆæ¯ID: {message_id}, æ˜¯å¦æ–°å¯¹è¯: {is_first_conversation}")

        state: WorkflowState = {
            'conversation_id': conversation_id,
            'user_id': user_id,
            'user_query': user_query,
            'user_attachments': user_attachments or [],
            'history_messages': await self._load_history(conversation_id),
            'patient_features': '',
            'pubmed_query': '',
            'europepmc_query': '',  # æ–°å¢åˆå§‹åŒ–
            'clinical_trial_keywords': '',
            'papers': [],
            'trials': [],
            'paper_analyses': [],
            'trial_analysis': '',
            'final_answer': '',
            'current_step': '',
            'errors': []
        }

        try:
            # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            async for chunk in self._step_extract_features(state):
                yield chunk
                # æ·»åŠ å»¶è¿Ÿç¡®ä¿å‰ç«¯æ¥æ”¶
                await asyncio.sleep(0.01)

            async for chunk in self._step_generate_queries(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_search(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_analyze_papers(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_analyze_trials(state):
                yield chunk
                await asyncio.sleep(0.01)

            async for chunk in self._step_generate_final(state):
                yield chunk
                await asyncio.sleep(0.01)

            # ä¿å­˜ç»“æœ
            await self._save_result(state, execution_id, message_id)
            await self._update_execution(execution_id, 'completed')
            logger.info(f"å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œæ‰§è¡ŒID: {execution_id}")

            # ç”Ÿæˆæ ‡é¢˜
            if is_first_conversation:
                logger.info(f"å¼€å§‹ç”Ÿæˆå¯¹è¯æ ‡é¢˜ï¼Œå¯¹è¯ID: {conversation_id}")
                await self._generate_title(state, conversation_id, user_id)

            # æœ€ç»ˆå®Œæˆä¿¡å·
            yield {'type': 'done', 'content': ''}

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error_detail}")

            await self._update_execution(execution_id, 'failed', str(e))
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ•°æ®åº“
            await self._save_error_result(state, execution_id, message_id, str(e))
            
            yield {
                'type': 'error',
                'step': state.get('current_step', 'unknown'),
                'content': f'âŒ æ‰§è¡Œå¤±è´¥: {str(e)}'
            }

    async def _step_extract_features(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤1: æå–æ‚£è€…ç‰¹å¾ï¼ˆä¿®å¤æ—¥å¿—è¾“å‡ºï¼‰"""
        state['current_step'] = 'extract_features'

        # å¼€å§‹åŒºå—
        yield {
            'type': 'section_start',
            'step': 'extract_features',
            'title': 'ğŸ” æå–æ‚£è€…ç‰¹å¾',
            'collapsible': True
        }

        # ç«‹å³è¾“å‡ºæ—¥å¿—
        yield {
            'type': 'log',
            'step': 'extract_features',
            'source': 'extract_features',
            'content': 'æ­£åœ¨åˆ†ææ‚£è€…ä¿¡æ¯...\n\n',
            'newline': True
        }

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if state['history_messages']:
            context_parts.append("### å†å²å¯¹è¯")
            for msg in state['history_messages'][-5:]:
                role = "ç”¨æˆ·" if msg['type'] == 'user' else "AI"
                context_parts.append(f"**{role}**: {msg['content'][:200]}...")

        if state['user_attachments']:
            context_parts.append("\n### ç”¨æˆ·ä¸Šä¼ çš„é™„ä»¶")
            for att in state['user_attachments']:
                context_parts.append(f"- {att['original_filename']}")

        context = "\n".join(context_parts)
        prompt = self.prompts.extract_features(context, state['user_query'])

        full_response = ""

        try:
            # å¤„ç†é™„ä»¶
            file_ids = []
            if state['user_attachments']:
                from app.services.file_service import file_service
                
                # è¾“å‡ºä¸Šä¼ æ–‡ä»¶æ—¥å¿—
                yield {
                    'type': 'log',
                    'step': 'extract_features',
                    'source': 'extract_features',
                    'content': f'æ­£åœ¨ä¸Šä¼ å’Œè§£æ {len(state["user_attachments"])} ä¸ªæ–‡ä»¶...\n\n',
                    'newline': True
                }
                
                file_ids, only_images = await file_service.process_attachments(
                    state['user_attachments']
                )
                
                # è¾“å‡ºè§£ææ–‡ä»¶æ—¥å¿—
                yield {
                    'type': 'log',
                    'step': 'extract_features',
                    'source': 'extract_features',
                    'content': 'æ–‡ä»¶ä¸Šä¼ å®Œæˆï¼Œæ­£åœ¨è§£ææ–‡ä»¶å†…å®¹...\n\n',
                    'newline': True
                }

                # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œä½¿ç”¨VLæ¨¡å‹
                if only_images and len(file_ids) == 1:
                    image_att = state['user_attachments'][0]
                    async for token in llm_service.chat_with_image_stream(
                            text=prompt,
                            image_path=image_att['file_path'],
                            history=[]
                    ):
                        full_response += token
                        # æµå¼è¾“å‡ºç»“æœï¼ˆå¢é‡ï¼‰
                        yield {
                            'type': 'result',
                            'step': 'extract_features',
                            'content': token,  # åªè¿”å›å¢é‡ token
                            'is_incremental': True
                        }
                else:
                    # ä½¿ç”¨ç»Ÿä¸€æ¥å£
                    async for token in llm_service.chat_with_context(
                            user_query=prompt,
                            file_ids=file_ids,
                            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚",
                            model=settings.qwen_long_model
                    ):
                        full_response += token
                        # æµå¼è¾“å‡ºç»“æœï¼ˆå¢é‡ï¼‰
                        yield {
                            'type': 'result',
                            'step': 'extract_features',
                            'content': token,  # åªè¿”å›å¢é‡ token
                            'is_incremental': True
                        }
            else:
                # æ— é™„ä»¶ï¼šæ™®é€šå¯¹è¯
                async for token in llm_service.chat_with_context(
                        user_query=prompt,
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚"
                ):
                    full_response += token
                    # æµå¼è¾“å‡ºç»“æœï¼ˆå¢é‡ï¼‰
                    yield {
                        'type': 'result',
                        'step': 'extract_features',
                        'content': token,  # åªè¿”å›å¢é‡ token
                        'is_incremental': True
                    }

            state['patient_features'] = full_response
            
            # æŒ‰ç…§ä¸ LLM çš„çº¦å®šæ ¡éªŒè¾“å‡º
            if 'EXTRACT_FAILED:' in full_response or 'æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æå–å‡ºæœ‰æ•ˆçš„æ‚£è€…ç‰¹å¾' in full_response:
                # LLM æ˜ç¡®è¡¨ç¤ºæ— æ³•æå–
                error_msg = full_response.replace('EXTRACT_FAILED:', '').strip()
                if not error_msg:
                    error_msg = 'âŒ æœªèƒ½æå–å‡ºæœ‰æ•ˆçš„æ‚£è€…ç‰¹å¾ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯'
                
                yield {
                    'type': 'result',
                    'step': 'extract_features',
                    'content': f'âŒ {error_msg}',
                    'is_incremental': False,
                    'summary': 'âŒ ç‰¹å¾æå–å¤±è´¥'
                }
                raise ValueError(f'æ‚£è€…ç‰¹å¾æå–å¤±è´¥: {error_msg}')
            
            # åŸºæœ¬é•¿åº¦æ ¡éªŒï¼ˆé˜²æ­¢å¼‚å¸¸æƒ…å†µï¼‰
            if len(full_response.strip()) < 20:
                error_msg = 'âŒ è¿”å›å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æå–å¤±è´¥ï¼Œè¯·æä¾›æ›´å¤šæ‚£è€…ä¿¡æ¯'
                yield {
                    'type': 'result',
                    'step': 'extract_features',
                    'content': error_msg,
                    'is_incremental': False,
                    'summary': 'âŒ ç‰¹å¾æå–å¤±è´¥'
                }
                raise ValueError('æ‚£è€…ç‰¹å¾å†…å®¹è¿‡çŸ­')
            
            # æˆåŠŸæå–ï¼Œæ¨é€å®Œæ•´å†…å®¹
            yield {
                'type': 'result',
                'step': 'extract_features',
                'content': full_response,
                'is_incremental': False,
                'summary': 'âœ… ç‰¹å¾æå–å®Œæˆ'
            }

        except Exception as e:
            error_msg = f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
            yield {
                'type': 'log',
                'step': 'extract_features',
                'source': 'extract_features',
                'content': error_msg,
                'newline': True
            }
            state['errors'].append(f'extract_features: {str(e)}')
            # ç»“æŸåŒºå—
            yield {'type': 'section_end', 'step': 'extract_features'}
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢å·¥ä½œæµ
            raise

    async def _step_generate_queries(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢æ¡ä»¶"""
        state['current_step'] = 'generate_queries'

        yield {
            'type': 'section_start',
            'step': 'generate_queries',
            'title': 'ğŸ” ç”Ÿæˆæ£€ç´¢æ¡ä»¶',
            'collapsible': True
        }

        yield {
            'type': 'log',
            'step': 'generate_queries',
            'source': 'generate_queries',
            'content': 'æ­£åœ¨ç”Ÿæˆæ£€ç´¢æ¡ä»¶...\n\n',
            'newline': True
        }

        prompt = self.prompts.generate_queries(state['patient_features'])
        full_response = ""

        try:
            async for token in llm_service.chat_with_context(
                    user_query=prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ£€ç´¢æ¡ä»¶ç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                full_response += token
                # æµå¼æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
                yield {
                    'type': 'log',
                    'step': 'generate_queries',
                    'source': 'generate_queries',
                    'content': token,
                    'newline': False
                }

            # æŒ‰ç…§ä¸ LLM çš„çº¦å®šæ ¡éªŒè¾“å‡º
            if 'GENERATE_FAILED:' in full_response:
                # LLM æ˜ç¡®è¡¨ç¤ºæ— æ³•ç”Ÿæˆ
                error_msg = full_response.replace('GENERATE_FAILED:', '').strip()
                if not error_msg:
                    error_msg = 'æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„æ£€ç´¢æ¡ä»¶ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯'
                
                yield {
                    'type': 'result',
                    'step': 'generate_queries',
                    'content': f'âŒ {error_msg}',
                    'summary': 'âŒ æ£€ç´¢æ¡ä»¶ç”Ÿæˆå¤±è´¥'
                }
                raise ValueError(f'æ£€ç´¢æ¡ä»¶ç”Ÿæˆå¤±è´¥: {error_msg}')
            
            # è§£æJSON
            start = full_response.find('{')
            end = full_response.rfind('}') + 1
            if start != -1 and end > start:
                queries = json.loads(full_response[start:end])
                state['pubmed_query'] = queries.get('pubmed_query', '').strip()
                state['europepmc_query'] = queries.get('europepmc_query', '').strip()
                state['clinical_trial_keywords'] = queries.get('clinical_trial_keywords', '').strip()
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON")
            
            # æ£€æŸ¥è§£æç»“æœæ˜¯å¦ä¸ºç©º
            if not state['pubmed_query'] and not state['europepmc_query'] and not state['clinical_trial_keywords']:
                error_msg = 'âŒ ç”Ÿæˆçš„æ£€ç´¢æ¡ä»¶ä¸ºç©ºï¼Œè¯·æä¾›æ›´å…·ä½“çš„æ‚£è€…ä¿¡æ¯'
                yield {
                    'type': 'result',
                    'step': 'generate_queries',
                    'content': error_msg,
                    'summary': 'âŒ æ£€ç´¢æ¡ä»¶ç”Ÿæˆå¤±è´¥'
                }
                raise ValueError('æ£€ç´¢æ¡ä»¶ä¸ºç©º')

            yield {
                'type': 'result',
                'step': 'generate_queries',
                'content': f"""**PubMed æ£€ç´¢å¼**: `{state['pubmed_query']}`

**Europe PMC æ£€ç´¢å¼**: `{state['europepmc_query']}`

**ä¸´åºŠè¯•éªŒå…³é”®è¯**: `{state['clinical_trial_keywords']}`""",
                'summary': 'âœ… æ£€ç´¢æ¡ä»¶ç”Ÿæˆå®Œæˆ',
                'data': {
                    'pubmed_query': state['pubmed_query'],
                    'europepmc_query': state['europepmc_query'],
                    'clinical_trial_keywords': state['clinical_trial_keywords']
                }
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_queries',
                'source': 'generate_queries',
                'content': f'\nâŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n',
                'newline': True
            }
            state['errors'].append(f'generate_queries: {str(e)}')
            # ç»“æŸåŒºå—
            yield {'type': 'section_end', 'step': 'generate_queries'}
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢å·¥ä½œæµ
            raise

    async def _step_search(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤3: æ‰§è¡Œæ£€ç´¢ï¼ˆæ”¯æŒè‡ªåŠ¨æ”¾å®½é‡è¯•ï¼‰"""
        state['current_step'] = 'search'

        yield {
            'type': 'section_start',
            'step': 'search',
            'title': 'ğŸ“š æ‰§è¡Œå¤šæºæ£€ç´¢',
            'collapsible': True
        }
        logging.getLogger("workflow_service").info("section_start search")

        progress_queue = asyncio.Queue()
        target_count = settings.max_search_results
        max_retries = 2  # æœ€å¤šé‡è¯•2æ¬¡
        
        for retry in range(max_retries + 1):
            if retry > 0:
                yield {
                    'type': 'log',
                    'source': 'search',
                    'content': f'\nâš ï¸ ç¬¬{retry}æ¬¡æ£€ç´¢ç»“æœä¸º0ï¼Œæ­£åœ¨æ”¾å®½æ¡ä»¶é‡è¯•...\n',
                    'newline': True
                }
                # æ”¾å®½æ£€ç´¢æ¡ä»¶
                state['pubmed_query'], state['europepmc_query'] = await self._relax_queries(
                    state['pubmed_query'], 
                    state['europepmc_query'],
                    state['patient_features']
                )
                yield {
                    'type': 'log',
                    'source': 'search',
                    'content': f'ğŸ”„ æ”¾å®½å PubMed: `{state["pubmed_query"]}`\nğŸ”„ æ”¾å®½å Europe PMC: `{state["europepmc_query"]}`\n',
                    'newline': True
                }

            async def search_all():
                """æ‰§è¡Œæ£€ç´¢ä»»åŠ¡"""
                try:
                    # åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„æ£€ç´¢æ¡ä»¶
                    papers_pubmed = await search_service._fetch_pubmed_papers(
                        state['pubmed_query'],
                        target_count,
                        progress_queue
                    )
                    papers_europepmc = await search_service._fetch_europepmc_papers(
                        state['europepmc_query'],
                        target_count,
                        progress_queue
                    )
                    
                    # åˆå¹¶ã€å»é‡ã€æ’åº
                    all_papers = []
                    if isinstance(papers_pubmed, list):
                        all_papers.extend(papers_pubmed)
                    if isinstance(papers_europepmc, list):
                        all_papers.extend(papers_europepmc)
                    
                    # å»é‡
                    all_papers = search_service._deduplicate_papers(all_papers)
                    
                    # è®¡ç®—ç›¸å…³åº¦å¹¶æ’åºï¼ˆä½¿ç”¨ PubMed query ä½œä¸ºåŸºå‡†ï¼‰
                    for paper in all_papers:
                        title_score = search_service._calculate_relevance(state['pubmed_query'], paper.get('title', ''))
                        abstract_score = search_service._calculate_relevance(state['pubmed_query'], paper.get('abstract', ''))
                        paper['relevance_score'] = (title_score * 0.7 + abstract_score * 0.3)
                    
                    all_papers.sort(key=lambda p: p.get('relevance_score', 0), reverse=True)
                    selected_papers = all_papers[:target_count]
                    
                    state['papers'].extend(selected_papers)

                    trials = await search_service.search_trials_with_ranking(
                        state['clinical_trial_keywords'],
                        target_count,
                        progress_queue
                    )
                    state['trials'].extend(trials)
                    try:
                        logging.getLogger("workflow_service").info(
                            "trials fetched count=%d keywords=%s",
                            len(state['trials']),
                            state.get('clinical_trial_keywords')
                        )
                        # é‡‡æ ·å‰3ä¸ªæ ‡é¢˜ç”¨äºå¿«é€Ÿç¡®è®¤
                        sample_titles = [t.get('title') for t in state['trials'][:3]]
                        logging.getLogger("workflow_service").info(
                            "trials sample titles=%s",
                            sample_titles
                        )
                    except Exception:
                        pass

                except Exception as e:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'search',
                        'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                        'newline': True
                    })
                finally:
                    await progress_queue.put({'type': 'DONE'})

            # å¯åŠ¨æ£€ç´¢ä»»åŠ¡
            search_task = asyncio.create_task(search_all())

            # è½¬å‘è¿›åº¦æ¶ˆæ¯
            while True:
                msg = await progress_queue.get()

                if isinstance(msg, dict):
                    if msg.get('type') == 'DONE':
                        break
                    elif msg.get('type') in ('log', 'result', 'progress'):
                        # ç›´æ¥è½¬å‘
                        if msg.get('type') == 'progress':
                            logging.getLogger("workflow_service").info(
                                "forward progress source=%s id=%s status=%s",
                                msg.get('source'), msg.get('id'), msg.get('status')
                            )
                        yield msg

            await search_task
            
            # æ£€æŸ¥ç»“æœ
            if len(state['papers']) > 0 or retry >= max_retries:
                break  # æœ‰ç»“æœæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€€å‡º

        # æ±‡æ€»ç»“æœ
        yield {
            'type': 'result',
            'step': 'search',
            'content': f"""### ğŸ“Š æ£€ç´¢æ±‡æ€»

- **æ–‡çŒ®æ€»æ•°**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒ**: {len(state['trials'])} ä¸ª""",
            'summary': f'âœ… æ£€ç´¢å®Œæˆï¼ˆ{len(state["papers"])} ç¯‡æ–‡çŒ®ï¼Œ{len(state["trials"])} ä¸ªè¯•éªŒï¼‰',
            'data': {
                'paper_count': len(state['papers']),
                'trial_count': len(state['trials'])
            }
        }

        yield {'type': 'section_end', 'step': 'search'}
    
    async def _relax_queries(self, pubmed_query: str, europepmc_query: str, patient_features: str) -> tuple:
        """æ”¾å®½æ£€ç´¢æ¡ä»¶ï¼ˆç§»é™¤æœ€ä¸é‡è¦çš„æ¡ä»¶ï¼‰"""
        # ç®€å•çš„æ”¾å®½ç­–ç•¥ï¼šç§»é™¤ AND åé¢çš„ä¸€ä¸ªæ¡ä»¶
        if ' AND ' in pubmed_query:
            parts = pubmed_query.split(' AND ')
            relaxed_pubmed = ' AND '.join(parts[:-1]) if len(parts) > 1 else parts[0]
        else:
            relaxed_pubmed = pubmed_query
        
        if ',' in europepmc_query:
            parts = [p.strip() for p in europepmc_query.split(',')]
            relaxed_europepmc = ', '.join(parts[:-1]) if len(parts) > 1 else parts[0]
        else:
            relaxed_europepmc = europepmc_query
        
        return relaxed_pubmed, relaxed_europepmc

    async def _step_analyze_papers(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤4: åˆ†ææ–‡çŒ®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼‰"""
        state['current_step'] = 'analyze_papers'

        yield {
            'type': 'section_start',
            'step': 'analyze_papers',
            'title': 'ğŸ“„ åˆ†ææ–‡çŒ®',
            'collapsible': True
        }

        if not state['papers']:
            yield {
                'type': 'result',
                'step': 'analyze_papers',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®',
                'summary': 'â„¹ï¸ æ— æ–‡çŒ®å¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_papers'}
            return

        from app.services.file_service import file_service

        for i, paper in enumerate(state['papers']):
            yield {
                'type': 'log',
                'step': 'analyze_papers',
                'source': 'analyze_papers',
                'content': f'\nğŸ“„ åˆ†ææ–‡çŒ® {i+1}/{len(state["papers"])}: {paper["title"][:50]}...\n\n',
                'newline': True
            }

            pdf_path = paper.get('pdf_path')
            if not pdf_path or not os.path.exists(pdf_path):
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'source': 'analyze_papers',
                    'content': 'âš ï¸ PDFä¸å­˜åœ¨ï¼Œè·³è¿‡\n',
                    'newline': True
                }
                continue

            prompt = self.prompts.analyze_paper(
                state['patient_features'],
                state['user_query'],
                paper
            )

            analysis = ""
            try:
                # è·å–æ–‡ä»¶ID
                file_id = await file_service.get_or_upload_file(pdf_path)

                if not file_id:
                    raise Exception("æ–‡ä»¶ä¸Šä¼ å¤±è´¥")

                # ä½¿ç”¨ç»Ÿä¸€æ¥å£åˆ†æ
                async for token in llm_service.chat_with_context(
                        user_query=prompt,
                        file_ids=[file_id],
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ–‡çŒ®åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»PDFæ–‡æ¡£ï¼ŒæŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºç»“æ„åŒ–åˆ†æã€‚",
                        model=settings.qwen_long_model
                ):
                    analysis += token
                    # æµå¼è¾“å‡ºï¼ˆå¢é‡ï¼‰
                    yield {
                        'type': 'result',
                        'step': 'analyze_papers',
                        'content': token,
                        'is_incremental': True
                    }

                state['paper_analyses'].append({
                    'paper': paper,
                    'analysis': analysis
                })
                
                # æœ€åæ¨é€å®Œæ•´å†…å®¹
                yield {
                    'type': 'result',
                    'step': 'analyze_papers',
                    'content': f"""### æ–‡çŒ® {i+1}: {paper['title']}

{analysis}""",
                    'is_incremental': False,
                    'data': {
                        'paper_id': paper.get('id'),
                        'pmid': paper.get('pmid'),
                        'title': paper['title']
                    }
                }

            except Exception as e:
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'source': 'analyze_papers',
                    'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n',
                    'newline': True
                }

        yield {
            'type': 'result',
            'step': 'analyze_papers',
            'content': '',
            'summary': f'âœ… æ–‡çŒ®åˆ†æå®Œæˆï¼ˆ{len(state["paper_analyses"])} ç¯‡ï¼‰'
        }

        yield {'type': 'section_end', 'step': 'analyze_papers'}

    async def _step_analyze_trials(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤5: åˆ†æä¸´åºŠè¯•éªŒ"""
        state['current_step'] = 'analyze_trials'

        yield {
            'type': 'section_start',
            'step': 'analyze_trials',
            'title': 'ğŸ’Š åˆ†æä¸´åºŠè¯•éªŒ',
            'collapsible': True
        }

        if not state['trials']:
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ä¸´åºŠè¯•éªŒ',
                'summary': 'â„¹ï¸ æ— è¯•éªŒå¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_trials'}
            return

        yield {
            'type': 'log',
            'step': 'analyze_trials',
            'source': 'analyze_trials',
            'content': f'æ­£åœ¨åˆ†æ {len(state["trials"])} ä¸ªä¸´åºŠè¯•éªŒ...\n\n',
            'newline': True
        }

        trials_text = []
        for i, trial in enumerate(state['trials']):
            trial_info = f"""### è¯•éªŒ {i+1}: {trial.get('title')}
- **NCT ID**: {trial.get('nct_id')}
- **çŠ¶æ€**: {trial.get('status')}
- **é˜¶æ®µ**: {trial.get('phase')}
- **ç–¾ç—…**: {trial.get('conditions')}
- **èµåŠ©æ–¹**: {trial.get('sponsor')}
"""
            trials_text.append(trial_info)

        prompt = self.prompts.analyze_trials(
            state['patient_features'],
            '\n'.join(trials_text)
        )

        analysis = ""
        try:
            logger.info(
                "analyze_trials start trials=%d model=%s prompt_len=%d",
                len(state['trials']),
                settings.qwen_long_model,
                len(prompt)
            )
            _token_count = 0
            async for token in llm_service.chat_with_context(
                    user_query=prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸´åºŠè¯•éªŒåˆ†æåŠ©æ‰‹ã€‚",
                    model=settings.qwen_long_model
            ):
                analysis += token
                _token_count += 1
                # æµå¼è¾“å‡ºç»“æœï¼ˆå¢é‡ï¼‰
                yield {
                    'type': 'result',
                    'step': 'analyze_trials',
                    'content': token,
                    'is_incremental': True
                }

            logger.info(
                "analyze_trials done tokens=%d content_len=%d",
                _token_count,
                len(analysis)
            )
            if not analysis:
                logger.warning("No analysis output")

            state['trial_analysis'] = analysis
            
            # æœ€åæ¨é€å®Œæ•´å†…å®¹å’Œsummary
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': analysis,
                'is_incremental': False,
                'summary': f'âœ… ä¸´åºŠè¯•éªŒåˆ†æå®Œæˆï¼ˆ{len(state["trials"])} ä¸ªï¼‰'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'analyze_trials',
                'source': 'analyze_trials',
                'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n',
                'newline': True
            }
            logger.exception("analyze_trials error: %s", str(e))

        yield {'type': 'section_end', 'step': 'analyze_trials'}

    async def _step_generate_final(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        state['current_step'] = 'generate_final'

        yield {
            'type': 'section_start',
            'step': 'generate_final',
            'title': 'ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š',
            'collapsible': False
        }

        yield {
            'type': 'log',
            'step': 'generate_final',
            'source': 'generate_final',
            'content': 'æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š...\n\n',
            'newline': True
        }

        papers_summary = []
        for i, item in enumerate(state['paper_analyses']):
            summary = f"**æ–‡çŒ® {i+1}**: {item['paper']['title']} - {item['analysis'][:200]}..."
            papers_summary.append(summary)

        prompt = self.prompts.generate_final_report(
            state['user_query'],
            state['patient_features'],
            '\n'.join(papers_summary) if papers_summary else "æš‚æ— ",
            state['trial_analysis']
        )

        final_answer = ""
        try:
            async for token in llm_service.chat_with_context(
                    user_query=prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å’¨è¯¢æŠ¥å‘Šç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                final_answer += token
                # æµå¼è¾“å‡º
                yield {
                    'type': 'token',
                    'step': 'generate_final',
                    'content': token
                }

            state['final_answer'] = final_answer

            yield {
                'type': 'result',
                'step': 'generate_final',
                'content': '',
                'summary': 'âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_final',
                'source': 'generate_final',
                'content': f'âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n',
                'newline': True
            }

        yield {'type': 'section_end', 'step': 'generate_final'}

    async def _generate_title(self, state: WorkflowState, conversation_id: int, user_id: int):
        """ç”Ÿæˆå¯¹è¯æ ‡é¢˜"""
        try:
            title_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹åŒ»ç–—å’¨è¯¢å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼ˆä¸è¶…è¿‡15ä¸ªå­—ï¼‰ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

æ‚£è€…ç‰¹å¾ï¼š{state['patient_features'][:300]}...

è¦æ±‚ï¼š
1. çªå‡ºç–¾ç—…/ç—‡çŠ¶å…³é”®è¯
2. ä¸è¶…è¿‡15ä¸ªå­—
3. ç›´æ¥è¾“å‡ºæ ‡é¢˜ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹
4. ä¸ä½¿ç”¨å¼•å·ã€ä¹¦åå·ç­‰æ ‡ç‚¹ç¬¦å·

æ ‡é¢˜ï¼š"""

            new_title = ""
            async for token in llm_service.chat_with_context(
                    user_query=title_prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ ‡é¢˜ç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                new_title += token

            # æ¸…ç†æ ‡é¢˜
            new_title = new_title.strip().replace('\n', '').replace('"', '').replace("'", '')

            if new_title and len(new_title) > 2:
                if len(new_title) > 15:
                    new_title = new_title[:15] + "..."

                async with get_db_session() as db:
                    from app.schemas.conversation import ConversationUpdateSchema
                    from app.crud import conversation as crud_conversation
                    await crud_conversation.update_conversation(
                        db,
                        conversation_id=conversation_id,
                        conversation_schema=ConversationUpdateSchema(title=new_title),
                        user_id=user_id
                    )

                logger.info(f"å¯¹è¯ {conversation_id} å·²è‡ªåŠ¨é‡å‘½åä¸ºã€Œ{new_title}ã€")
            else:
                logger.warning(f"ç”Ÿæˆçš„æ ‡é¢˜æ— æ•ˆï¼Œæ ‡é¢˜: {new_title}")

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ ‡é¢˜å¤±è´¥: {e}")

    async def _create_execution(self, conversation_id: int, user_id: int) -> int:
        """åˆ›å»ºæ‰§è¡Œè®°å½•"""
        async with get_db_session() as db:
            execution = WorkflowExecution(
                conversation_id=conversation_id,
                user_id=user_id,
                workflow_type='multi_source',
                status='running',
                current_step='initializing'
            )
            db.add(execution)
            await db.commit()
            return execution.id

    async def _update_execution(self, execution_id: int, status: str, error: Optional[str] = None):
        """æ›´æ–°æ‰§è¡ŒçŠ¶æ€"""
        async with get_db_session() as db:
            execution = await db.get(WorkflowExecution, execution_id)
            if execution is None:
                logger.warning(f"æ‰¾ä¸åˆ°æ‰§è¡Œè®°å½•: {execution_id}")
                return
            execution.status = status
            if status == 'completed':
                execution.completed_at = func.now()
            if error:
                execution.error_message = error
            await db.commit()

    async def _load_history(self, conversation_id: int) -> List[Dict]:
        """åŠ è½½å†å²å¯¹è¯"""
        async with get_db_session() as db:
            result = await db.execute(
                select(Message)
                .where(Message.conversation_id == conversation_id)
                .order_by(Message.created_at.desc())
                .limit(10)
            )
            messages = result.scalars().all()

            return [
                {
                    'type': 'user' if m.message_type == MessageType.USER else 'assistant',
                    'content': m.content
                }
                for m in reversed(list(messages))
            ]

    async def _save_result(self, state: WorkflowState, execution_id: int, message_id: int):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        async with get_db_session() as db:
            full_content = f"""# å¤šæºæ£€ç´¢åˆ†ææŠ¥å‘Š

## 1. æ‚£è€…ç‰¹å¾åˆ†æ
{state['patient_features']}

---

## 2. æ£€ç´¢æ¡ä»¶
- **PubMed**: `{state['pubmed_query']}`
- **ä¸´åºŠè¯•éªŒ**: `{state['clinical_trial_keywords']}`

---

## 3. æ£€ç´¢ç»“æœ
- **æ–‡çŒ®æ•°é‡**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒæ•°é‡**: {len(state['trials'])} ä¸ª

---

## 4. æ–‡çŒ®åˆ†æ
"""

            if state['paper_analyses']:
                for i, item in enumerate(state['paper_analyses']):
                    full_content += f"\n### æ–‡çŒ® {i+1}: {item['paper']['title']}\n\n"
                    full_content += f"{item['analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— æ–‡çŒ®åˆ†æ\n\n---\n"

            full_content += f"\n## 5. ä¸´åºŠè¯•éªŒåˆ†æ\n\n"
            if state['trial_analysis']:
                full_content += f"{state['trial_analysis']}\n\n---\n"
            else:
                full_content += "\næš‚æ— ä¸´åºŠè¯•éªŒåˆ†æ\n\n---\n"

            full_content += f"\n## 6. ç»¼åˆæŠ¥å‘Š\n\n{state['final_answer']}\n"

            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "workflow_type": "multi_source",
                "patient_features": state['patient_features'],
                "search_queries": {
                    "pubmed": state['pubmed_query'],
                    "europepmc": state['europepmc_query'],
                    "clinical_trial": state['clinical_trial_keywords']
                },
                "papers": [
                    {
                        "id": paper.get('id'),
                        "pmid": paper.get('pmid'),
                        "title": paper.get('title'),
                        "authors": paper.get('authors')
                    }
                    for paper in state['papers']
                ],
                "trials": [
                    {
                        "nct_id": trial.get('nct_id'),
                        "title": trial.get('title')
                    }
                    for trial in state['trials']
                ],
                "attachments": [
                    {
                        "filename": att.get('filename'),
                        "original_filename": att.get('original_filename')
                    }
                    for att in state['user_attachments']
                ]
            }

            # æ›´æ–°ç°æœ‰æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯
            await crud_message.update_message(
                db,
                message_id=message_id,
                content=full_content,
                status=MessageStatus.COMPLETED
            )
            
            # æ›´æ–°æ¶ˆæ¯å…ƒæ•°æ®
            await db.execute(
                update(Message)
                .where(Message.id == message_id)
                .values(metadata_json=json.dumps(metadata, ensure_ascii=False))
            )

            execution = await db.get(WorkflowExecution, execution_id)
            if execution is None:
                logger.warning(f"æ‰¾ä¸åˆ°æ‰§è¡Œè®°å½•: {execution_id}")
                return
            execution.result_message_id = message_id
            execution.patient_features = state['patient_features']
            execution.search_queries = json.dumps({
                'pubmed': state['pubmed_query'],
                'clinical_trial': state['clinical_trial_keywords']
            })
            await db.commit()
    
    async def _save_error_result(self, state: WorkflowState, execution_id: int, message_id: int, error_msg: str):
        """ä¿å­˜é”™è¯¯ç»“æœåˆ°æ•°æ®åº“"""
        async with get_db_session() as db:
            # æ„å»ºé”™è¯¯ä¿¡æ¯å†…å®¹
            error_content = f"""âŒ **å¤šæºæ£€ç´¢æ‰§è¡Œå¤±è´¥**\n\n"""
            
            # æ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå»é™¤é‡å¤çš„å‰ç¼€ï¼‰
            clean_error_msg = error_msg
            if 'ï¼š' in error_msg:
                # æå–å†’å·åé¢çš„å†…å®¹
                clean_error_msg = error_msg.split('ï¼š', 1)[1].strip()
            
            error_content += f"""{clean_error_msg}\n\n---\n\nè¯·æ ¹æ®ä»¥ä¸Šæç¤ºè°ƒæ•´æ‚¨çš„è¾“å…¥ï¼Œç„¶åé‡è¯•ã€‚
"""
            
            # æ›´æ–°ç°æœ‰æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯
            await crud_message.update_message(
                db,
                message_id=message_id,
                content=error_content,
                status=MessageStatus.FAILED
            )


workflow_service = WorkflowService()