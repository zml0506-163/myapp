"""
å·¥ä½œæµæœåŠ¡
app/services/workflow_service.py
"""
import os
import json
import time
from typing import TypedDict, AsyncGenerator, List, Dict, Optional, Set
import asyncio
import logging
from sqlalchemy import select, func, update

from app.core.config import settings
from app.db.database import get_db_session
from app.services.llm_service import llm_service
from app.services.search_service import search_service
from app.prompts.workflow_prompts import WorkflowPrompts
from app.models import WorkflowExecution, Message, MessageType, MessageStatus
from app.crud import message as crud_message
from app.schemas.message import MessageCreateSchema
from app.core.logger import get_logger
from app.tools_api.factory import resolve_tool_facade
from app.tools_api.models import Trial as ToolTrial
from app.workflows.router import make_plan

logger = get_logger(__name__)
logging.basicConfig(level=logging.INFO)


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    conversation_id: int
    user_id: int
    user_query: str
    user_attachments: List[Dict]
    history_messages: List[Dict]
    patient_features: str
    pubmed_query: str
    europepmc_query: str  # æ–°å¢ï¼šEurope PMC æ£€ç´¢æ¡ä»¶
    clinical_trial_keywords: str
    papers: List[Dict]
    trials: List[Dict]
    paper_analyses: List[Dict]
    trial_analysis: str
    final_answer: str
    current_step: str
    errors: List[str]
    intent: Dict[str, bool]


class WorkflowService:
    """ä¼˜åŒ–çš„å·¥ä½œæµæœåŠ¡"""

    def __init__(self):
        self.prompts = WorkflowPrompts()
        # å·¥å…·æ¥å£å±‚ï¼ˆå¯åˆ‡æ¢ local/mcpï¼‰ï¼Œä¿æŒå‘åå…¼å®¹
        self.tools = resolve_tool_facade()
        # æ‰§è¡Œçº§åˆ«è®¡æ—¶ä¸æ­¥æ•°ç»Ÿè®¡ï¼ˆä»…ç”¨äºæ—¥å¿—å±•ç¤ºï¼‰
        self._start_ts: float = 0.0
        self._steps_done: int = 0
        self._budget_tokens: int = 0

    async def _detect_intent(self, user_query: str) -> Dict[str, bool]:
        """åŸºäºç”¨æˆ·é—®é¢˜è¯†åˆ«æ„å›¾ï¼šæ˜¯å¦åªæ£€ç´¢æ–‡çŒ®/åªæ£€ç´¢ä¸´åºŠè¯•éªŒ/ä¸¤è€…éƒ½æ£€ç´¢"""
        q = (user_query or "").lower()
        trials_keywords = ["ä¸´åºŠè¯•éªŒ", "è¯•éªŒ", "nct", "clinical trial", "å…¥ç»„", "æ’é™¤æ ‡å‡†"]
        papers_keywords = ["æ–‡çŒ®", "è®ºæ–‡", "pmid", "ç ”ç©¶", "ç»¼è¿°", "paper"]
        use_trials = any(k in q for k in trials_keywords)
        use_papers = any(k in q for k in papers_keywords)
        # å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®æŒ‡å‡ºï¼Œåˆ™é»˜è®¤ä¸¤è€…éƒ½æ£€ç´¢
        if not use_trials and not use_papers:
            use_trials = True
            use_papers = True
        return {"use_papers": use_papers, "use_trials": use_trials}

    async def execute_with_streaming(
            self,
            conversation_id: int,
            user_id: int,
            user_query: str,
            message_id: int,  # æ·»åŠ  message_id å‚æ•°
            user_attachments: Optional[List[Dict]] = None,
            is_first_conversation: bool = False
    ) -> AsyncGenerator[Dict, None]:
        """æ‰§è¡Œå·¥ä½œæµå¹¶æµå¼è¾“å‡º"""

        execution_id = await self._create_execution(conversation_id, user_id)
        logger.info(f"å¼€å§‹æ‰§è¡Œå·¥ä½œæµï¼Œå¯¹è¯ID: {conversation_id}, æ¶ˆæ¯ID: {message_id}, æ˜¯å¦æ–°å¯¹è¯: {is_first_conversation}")

        state: WorkflowState = {
            'conversation_id': conversation_id,
            'user_id': user_id,
            'user_query': user_query,
            'user_attachments': user_attachments or [],
            'history_messages': await self._load_history(conversation_id),
            'patient_features': '',
            'pubmed_query': '',
            'europepmc_query': '',  # æ–°å¢åˆå§‹åŒ–
            'clinical_trial_keywords': '',
            'papers': [],
            'trials': [],
            'paper_analyses': [],
            'trial_analysis': '',
            'final_answer': '',
            'current_step': '',
            'errors': [],
            'intent': {'use_papers': True, 'use_trials': True}
        }

        try:
            # è®°å½•æ‰§è¡Œèµ·å§‹æ—¶é—´
            self._start_ts = time.time()
            self._steps_done = 0
            self._budget_tokens = 0
            # å¯é€‰ï¼šå±•ç¤ºè·¯ç”±è®¡åˆ’ï¼ˆä»…æ—¥å¿—/å±•ç¤ºï¼Œä¸æ”¹å˜å®é™…æ‰§è¡Œï¼‰

            # å¯é€‰ï¼šå±•ç¤ºå‹ planï¼ˆä¸æ”¹æµç¨‹ï¼‰
            if settings.deliberate_enabled:
                yield {
                    'type': 'section_start',
                    'step': 'plan_deliberate',
                    'title': 'ğŸ§© è§„åˆ’ï¼ˆå±•ç¤ºå‹ï¼‰',
                    'collapsible': True,
                }
                yield {
                    'type': 'log',
                    'step': 'plan_deliberate',
                    'source': 'router',
                    'content': 'plan: display_only=true reason=fixed_plan\n',
                    'newline': True,
                }
                yield {'type': 'section_end', 'step': 'plan_deliberate'}

            # æ ¹æ®ç”¨æˆ·é—®é¢˜è¯†åˆ«æ„å›¾ï¼ˆå†³å®šä½¿ç”¨å“ªäº›æ£€ç´¢å·¥å…·ï¼‰
            state['intent'] = await self._detect_intent(state['user_query'])

            # é¢„åŠ è½½ç¼“å­˜çš„æ‚£è€…ç‰¹å¾ï¼ˆæ— é™„ä»¶æ—¶ä¼˜å…ˆå¤ç”¨ï¼‰
            cached_pf = await self._load_cached_patient_features(state['conversation_id'])
            if cached_pf and not state['user_attachments']:
                state['patient_features'] = cached_pf

            # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            async for chunk in self._step_extract_features(state):
                yield chunk

            async for chunk in self._step_generate_queries(state):
                yield chunk

            async for chunk in self._step_search(state):
                yield chunk

            async for chunk in self._step_analyze_papers(state):
                yield chunk

            async for chunk in self._step_analyze_trials(state):
                yield chunk

            # å¯é€‰ï¼šå±•ç¤ºå‹ rerank ä¸ groundingï¼ˆä¸æ”¹æµç¨‹ï¼Œä»…æ—¥å¿—ï¼‰
            if settings.deliberate_enabled:
                # rerank å±•ç¤ºï¼ˆä¿ç•™å±•ç¤ºï¼Œä¸æ”¹æµç¨‹ï¼‰
                yield {
                    'type': 'section_start',
                    'step': 'rerank_deliberate',
                    'title': 'ğŸ”€ å€™é€‰é‡æ’ï¼ˆå±•ç¤ºå‹ï¼‰',
                    'collapsible': True,
                }
                rerank_basis = 'relevance,diversity,balance'
                paper_cnt = len(state.get('papers', []) or [])
                trial_cnt = len(state.get('trials', []) or [])
                yield {
                    'type': 'log',
                    'step': 'rerank_deliberate',
                    'source': 'workflow',
                    'content': f'rerank: display_only=true basis={rerank_basis} candidates=paper:{paper_cnt},trial:{trial_cnt}\n',
                    'newline': True,
                }
                yield {'type': 'section_end', 'step': 'rerank_deliberate'}

                # grounding å®é™…æ ¡éªŒï¼ˆé‡è¦ï¼‰ï¼šåœ¨å±•ç¤ºæ®µä½ç½®è¾“å‡ºçœŸå®æ ¡éªŒæ—¥å¿—
                async for chunk in self._step_grounding_check(state):
                    yield chunk

            async for chunk in self._step_generate_final(state):
                yield chunk



            # ä¿å­˜ç»“æœ
            await self._save_result(state, execution_id, message_id)
            await self._update_execution(execution_id, 'completed')
            logger.info(f"å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œæ‰§è¡ŒID: {execution_id}")

            # ç”Ÿæˆæ ‡é¢˜
            if is_first_conversation:
                logger.info(f"å¼€å§‹ç”Ÿæˆå¯¹è¯æ ‡é¢˜ï¼Œå¯¹è¯ID: {conversation_id}")
                new_title = await self._generate_title(state, conversation_id, user_id)
                # é€šçŸ¥å‰ç«¯æ ‡é¢˜å·²æ›´æ–°
                if new_title:
                    yield {
                        'type': 'title_updated',
                        'conversation_id': conversation_id,
                        'title': new_title
                    }

            # æœ€ç»ˆå®Œæˆä¿¡å·
            yield {'type': 'done', 'content': ''}

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error_detail}")

            await self._update_execution(execution_id, 'failed', str(e))
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ•°æ®åº“
            await self._save_error_result(state, execution_id, message_id, str(e))
            
            yield {
                'type': 'error',
                'step': state.get('current_step', 'unknown'),
                'content': f'âŒ æ‰§è¡Œå¤±è´¥: {str(e)}'
            }

    async def _step_extract_features(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤1: æå–æ‚£è€…ç‰¹å¾ï¼ˆä¿®å¤æ—¥å¿—è¾“å‡ºï¼‰"""
        state['current_step'] = 'extract_features'

        # å¼€å§‹åŒºå—
        yield {
            'type': 'section_start',
            'step': 'extract_features',
            'title': 'ğŸ” æå–æ‚£è€…ç‰¹å¾',
            'collapsible': True
        }

        # è‹¥å·²æœ‰ç¼“å­˜æ‚£è€…ç‰¹å¾ä¸”å½“å‰æ²¡æœ‰é™„ä»¶ï¼Œåˆ™ç›´æ¥å¤ç”¨å¹¶è·³è¿‡æå–
        if state['patient_features'] and not state['user_attachments']:
            yield {
                'type': 'result',
                'step': 'extract_features',
                'content': state['patient_features'],
                'is_incremental': False,
                'summary': 'âœ… å¤ç”¨æ‚£è€…ç‰¹å¾ï¼ˆè·³è¿‡æå–ï¼‰'
            }
            yield {'type': 'section_end', 'step': 'extract_features'}
            return

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if state['history_messages']:
            context_parts.append("### å†å²å¯¹è¯")
            for msg in state['history_messages'][-5:]:
                role = "ç”¨æˆ·" if msg['type'] == 'user' else "AI"
                context_parts.append(f"**{role}**: {msg['content'][:200]}...")

        if state['user_attachments']:
            context_parts.append("\n### ç”¨æˆ·ä¸Šä¼ çš„é™„ä»¶")
            for att in state['user_attachments']:
                context_parts.append(f"- {att['original_filename']}")

        context = "\n".join(context_parts)
        prompt = self.prompts.extract_features(context, state['user_query'])

        full_response = ""

        try:
            # å¤„ç†é™„ä»¶
            file_ids = []
            if state['user_attachments']:
                from app.services.file_service import file_service
                
                # è¾“å‡ºä¸Šä¼ æ–‡ä»¶æ—¥å¿—
                yield {
                    'type': 'log',
                    'step': 'extract_features',
                    'source': 'extract_features',
                    'content': f'æ­£åœ¨ä¸Šä¼ å’Œè§£æ {len(state["user_attachments"])} ä¸ªæ–‡ä»¶...\n\n',
                    'newline': True
                }
                
                file_ids, only_images = await file_service.process_attachments(
                    state['user_attachments']
                )
                
                # è¾“å‡ºè§£ææ–‡ä»¶æ—¥å¿—
                yield {
                    'type': 'log',
                    'step': 'extract_features',
                    'source': 'extract_features',
                    'content': 'æ–‡ä»¶ä¸Šä¼ å®Œæˆï¼Œæ­£åœ¨è§£ææ–‡ä»¶å†…å®¹...\n\n',
                    'newline': True
                }

                # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œä½¿ç”¨VLæ¨¡å‹
                if only_images and len(file_ids) == 1:
                    image_att = state['user_attachments'][0]
                    async for token in llm_service.chat_with_image_stream(
                            text=prompt,
                            image_path=image_att['file_path'],
                            history=[]
                    ):
                        full_response += token
                        self._budget_tokens += 1
                        # æµå¼è¾“å‡ºç»“æœï¼ˆå¢é‡ï¼‰
                        yield {
                            'type': 'result',
                            'step': 'extract_features',
                            'content': token,  # åªè¿”å›å¢é‡ token
                            'is_incremental': True
                        }
                else:
                    # ä½¿ç”¨ç»Ÿä¸€æ¥å£
                    async for token in llm_service.chat_with_context(
                            user_query=prompt,
                            file_ids=file_ids,
                            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚",
                            model=settings.qwen_long_model
                    ):
                        full_response += token
                        self._budget_tokens += 1
                        # æµå¼è¾“å‡ºç»“æœï¼ˆå¢é‡ï¼‰
                        yield {
                            'type': 'result',
                            'step': 'extract_features',
                            'content': token,  # åªè¿”å›å¢é‡ token
                            'is_incremental': True
                        }
            else:
                # æ— é™„ä»¶ï¼šæ™®é€šå¯¹è¯
                async for token in llm_service.chat_with_context(
                        user_query=prompt,
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯åˆ†æåŠ©æ‰‹ã€‚"
                ):
                    full_response += token
                    self._budget_tokens += 1
                    # æµå¼è¾“å‡ºç»“æœï¼ˆå¢é‡ï¼‰
                    yield {
                        'type': 'result',
                        'step': 'extract_features',
                        'content': token,  # åªè¿”å›å¢é‡ token
                        'is_incremental': True
                    }

            state['patient_features'] = full_response
            
            # æŒ‰ç…§ä¸ LLM çš„çº¦å®šæ ¡éªŒè¾“å‡º
            if 'EXTRACT_FAILED:' in full_response or 'æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æå–å‡ºæœ‰æ•ˆçš„æ‚£è€…ç‰¹å¾' in full_response:
                # LLM æ˜ç¡®è¡¨ç¤ºæ— æ³•æå–
                error_msg = full_response.replace('EXTRACT_FAILED:', '').strip()
                if not error_msg:
                    error_msg = 'âŒ æœªèƒ½æå–å‡ºæœ‰æ•ˆçš„æ‚£è€…ç‰¹å¾ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯'
                
                yield {
                    'type': 'result',
                    'step': 'extract_features',
                    'content': f'âŒ {error_msg}',
                    'is_incremental': False,
                    'summary': 'âŒ ç‰¹å¾æå–å¤±è´¥'
                }
                raise ValueError(f'æ‚£è€…ç‰¹å¾æå–å¤±è´¥: {error_msg}')
            
            # åŸºæœ¬é•¿åº¦æ ¡éªŒï¼ˆé˜²æ­¢å¼‚å¸¸æƒ…å†µï¼‰
            if len(full_response.strip()) < 20:
                error_msg = 'âŒ è¿”å›å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æå–å¤±è´¥ï¼Œè¯·æä¾›æ›´å¤šæ‚£è€…ä¿¡æ¯'
                yield {
                    'type': 'result',
                    'step': 'extract_features',
                    'content': error_msg,
                    'is_incremental': False,
                    'summary': 'âŒ ç‰¹å¾æå–å¤±è´¥'
                }
                raise ValueError('æ‚£è€…ç‰¹å¾å†…å®¹è¿‡çŸ­')
            
            # æˆåŠŸæå–ï¼Œæ¨é€å®Œæ•´å†…å®¹
            yield {
                'type': 'result',
                'step': 'extract_features',
                'content': full_response,
                'is_incremental': False,
                'summary': 'âœ… ç‰¹å¾æå–å®Œæˆ'
            }

        except Exception as e:
            error_msg = f'âŒ åˆ†æå¤±è´¥: {str(e)}\n'
            yield {
                'type': 'log',
                'step': 'extract_features',
                'source': 'extract_features',
                'content': error_msg,
                'newline': True
            }
            state['errors'].append(f'extract_features: {str(e)}')
            # ç»“æŸåŒºå—
            yield {'type': 'section_end', 'step': 'extract_features'}
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢å·¥ä½œæµ
            raise

    async def _step_generate_queries(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢æ¡ä»¶"""
        state['current_step'] = 'generate_queries'

        yield {
            'type': 'section_start',
            'step': 'generate_queries',
            'title': 'ğŸ” ç”Ÿæˆæ£€ç´¢æ¡ä»¶',
            'collapsible': True
        }
        # æŒ‰ç”¨æˆ·æ„å›¾å®Œå…¨è·³è¿‡è¯¥æ­¥éª¤ï¼ˆæ— éœ€ç”Ÿæˆä»»ä½•æ£€ç´¢æ¡ä»¶ï¼‰
        if not (state.get('intent', {}).get('use_papers', True) or state.get('intent', {}).get('use_trials', True)):
            yield {
                'type': 'result',
                'step': 'generate_queries',
                'content': 'â„¹ï¸ å·²æŒ‰ç”¨æˆ·æ„å›¾è·³è¿‡æ£€ç´¢æ¡ä»¶ç”Ÿæˆ',
                'summary': 'â„¹ï¸ è·³è¿‡æ£€ç´¢æ¡ä»¶ç”Ÿæˆ'
            }
            yield {'type': 'section_end', 'step': 'generate_queries'}
            return

        yield {
            'type': 'log',
            'step': 'generate_queries',
            'source': 'generate_queries',
            'content': 'æ­£åœ¨ç”Ÿæˆæ£€ç´¢æ¡ä»¶...\n\n',
            'newline': True
        }

        need_papers = state.get('intent', {}).get('use_papers', True)
        need_trials = state.get('intent', {}).get('use_trials', True)
        prompt = self.prompts.generate_queries_selective(state['patient_features'], need_papers, need_trials)
        full_response = ""

        try:
            async for token in llm_service.chat_with_context(
                    user_query=prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ£€ç´¢æ¡ä»¶ç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                full_response += token
                self._budget_tokens += 1
                # æµå¼æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
                yield {
                    'type': 'log',
                    'step': 'generate_queries',
                    'source': 'generate_queries',
                    'content': token,
                    'newline': False
                }

            # æŒ‰ç…§ä¸ LLM çš„çº¦å®šæ ¡éªŒè¾“å‡º
            if 'GENERATE_FAILED:' in full_response:
                # LLM æ˜ç¡®è¡¨ç¤ºæ— æ³•ç”Ÿæˆ
                error_msg = full_response.replace('GENERATE_FAILED:', '').strip()
                if not error_msg:
                    error_msg = 'æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„æ£€ç´¢æ¡ä»¶ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯'
                
                yield {
                    'type': 'result',
                    'step': 'generate_queries',
                    'content': f'âŒ {error_msg}',
                    'summary': 'âŒ æ£€ç´¢æ¡ä»¶ç”Ÿæˆå¤±è´¥'
                }
                raise ValueError(f'æ£€ç´¢æ¡ä»¶ç”Ÿæˆå¤±è´¥: {error_msg}')
            
            # è§£æJSON
            start = full_response.find('{')
            end = full_response.rfind('}') + 1
            if start != -1 and end > start:
                queries = json.loads(full_response[start:end])
                state['pubmed_query'] = queries.get('pubmed_query', '').strip()
                state['europepmc_query'] = queries.get('europepmc_query', '').strip()
                state['clinical_trial_keywords'] = queries.get('clinical_trial_keywords', '').strip()
            else:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON")
            
            # æ ¹æ®ç”¨æˆ·æ„å›¾è¿‡æ»¤ä¸éœ€è¦çš„æ£€ç´¢é¡¹
            if not state.get('intent', {}).get('use_papers', True):
                state['pubmed_query'] = ''
                state['europepmc_query'] = ''
            if not state.get('intent', {}).get('use_trials', True):
                state['clinical_trial_keywords'] = ''

            # æ£€æŸ¥è§£æç»“æœæ˜¯å¦ä¸ºç©º
            if not state['pubmed_query'] and not state['europepmc_query'] and not state['clinical_trial_keywords']:
                error_msg = 'âŒ ç”Ÿæˆçš„æ£€ç´¢æ¡ä»¶ä¸ºç©ºï¼Œè¯·æä¾›æ›´å…·ä½“çš„æ‚£è€…ä¿¡æ¯'
                yield {
                    'type': 'result',
                    'step': 'generate_queries',
                    'content': error_msg,
                    'summary': 'âŒ æ£€ç´¢æ¡ä»¶ç”Ÿæˆå¤±è´¥'
                }
                raise ValueError('æ£€ç´¢æ¡ä»¶ä¸ºç©º')

            yield {
                'type': 'result',
                'step': 'generate_queries',
                'content': f"""**PubMed æ£€ç´¢å¼**: `{state['pubmed_query']}`

**Europe PMC æ£€ç´¢å¼**: `{state['europepmc_query']}`

**ä¸´åºŠè¯•éªŒå…³é”®è¯**: `{state['clinical_trial_keywords']}`""",
                'summary': 'âœ… æ£€ç´¢æ¡ä»¶ç”Ÿæˆå®Œæˆ',
                'data': {
                    'pubmed_query': state['pubmed_query'],
                    'europepmc_query': state['europepmc_query'],
                    'clinical_trial_keywords': state['clinical_trial_keywords']
                }
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_queries',
                'source': 'generate_queries',
                'content': f'\nâŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n',
                'newline': True
            }
            state['errors'].append(f'generate_queries: {str(e)}')
            # ç»“æŸåŒºå—
            yield {'type': 'section_end', 'step': 'generate_queries'}
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢å·¥ä½œæµ
            raise

    async def _step_search(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤3: æ‰§è¡Œæ£€ç´¢ï¼ˆæ”¯æŒè‡ªåŠ¨æ”¾å®½é‡è¯•ï¼‰"""
        state['current_step'] = 'search'

        yield {
            'type': 'section_start',
            'step': 'search',
            'title': 'ğŸ“š æ‰§è¡Œå¤šæºæ£€ç´¢',
            'collapsible': True
        }
        # æŒ‰ç”¨æˆ·æ„å›¾è·³è¿‡æ£€ç´¢
        if not (state.get('intent', {}).get('use_papers', True) or state.get('intent', {}).get('use_trials', True)):
            yield {
                'type': 'result',
                'step': 'search',
                'content': 'â„¹ï¸ å·²æŒ‰ç”¨æˆ·æ„å›¾è·³è¿‡æ£€ç´¢',
                'summary': 'â„¹ï¸ è·³è¿‡æ£€ç´¢'
            }
            yield {'type': 'section_end', 'step': 'search'}
            return
        logging.getLogger("workflow_service").info("section_start search")

        progress_queue = asyncio.Queue()
        target_count = settings.max_search_results
        max_retries = 2  # æœ€å¤šé‡è¯•2æ¬¡
        need_papers = state.get('intent', {}).get('use_papers', True)
        need_trials = state.get('intent', {}).get('use_trials', True)
        
        for retry in range(max_retries + 1):
            if retry > 0:
                yield {
                    'type': 'log',
                    'source': 'search',
                    'content': f'\nâš ï¸ ç¬¬{retry}æ¬¡æ£€ç´¢ç»“æœä¸º0ï¼Œæ­£åœ¨æ”¾å®½æ¡ä»¶é‡è¯•...\n',
                    'newline': True
                }
                relaxed_msgs = []
                # æ”¾å®½æ£€ç´¢æ¡ä»¶ï¼ˆæ–‡çŒ®+è¯•éªŒï¼‰
                if need_papers:
                    state['pubmed_query'], state['europepmc_query'] = await self._relax_queries(
                        state['pubmed_query'], 
                        state['europepmc_query'],
                        state['patient_features']
                    )
                    relaxed_msgs.append(f'ğŸ”„ æ”¾å®½å PubMed: `{state["pubmed_query"]}`')
                    relaxed_msgs.append(f'ğŸ”„ æ”¾å®½å Europe PMC: `{state["europepmc_query"]}`')
                # æ”¾å®½è¯•éªŒå…³é”®è¯
                if need_trials:
                    state['clinical_trial_keywords'] = await self._relax_trials_keywords(
                        state['clinical_trial_keywords'],
                        state['patient_features']
                    )
                    relaxed_msgs.append(f'ğŸ”„ æ”¾å®½å Trials: `{state["clinical_trial_keywords"]}`')
                if relaxed_msgs:
                    yield {
                        'type': 'log',
                        'source': 'search',
                        'content': "\n".join(relaxed_msgs) + "\n",
                        'newline': True
                    }

            async def search_all():
                """æ‰§è¡Œæ£€ç´¢ä»»åŠ¡"""
                try:
                    async def _fetch_papers_via_tools(query: str, label: str, sources: List[str], fallback_coro):
                        if not query:
                            return []
                        await progress_queue.put({
                            'type': 'log',
                            'source': label,
                            'content': f'ğŸ” ä½¿ç”¨å·¥å…·æ¥å£æ£€ç´¢ {label}ï¼Œæ£€ç´¢å¼: `{query}`\n',
                            'newline': True
                        })
                        try:
                            result = await self.tools.search_papers(
                                query=query,
                                size=target_count,
                                sources=sources
                            )
                            papers = [paper.dict() for paper in result.papers]
                            await progress_queue.put({
                                'type': 'log',
                                'source': label,
                                'content': f'âœ… å·¥å…·æ¥å£è¿”å› {len(papers)} ç¯‡æ–‡çŒ®\n',
                                'newline': True
                            })
                            return papers
                        except Exception as tool_error:
                            await progress_queue.put({
                                'type': 'log',
                                'source': label,
                                'content': f'âš ï¸ å·¥å…·æ¥å£æ£€ç´¢å¤±è´¥ï¼Œå›é€€æœ¬åœ°å®ç°: {tool_error}\n',
                                'newline': True
                            })
                            return await fallback_coro()

                    logger.info(
                        "search start pubmed_query=%s europepmc_query=%s trials_keywords=%s",
                        state.get('pubmed_query'),
                        state.get('europepmc_query'),
                        state.get('clinical_trial_keywords')
                    )

                    all_papers: List[Dict] = []

                    if need_papers and (state['pubmed_query'] or state['europepmc_query']):
                        tasks: List[asyncio.Task] = []

                        if state['pubmed_query']:
                            async def _fallback_pubmed():
                                return await search_service._fetch_pubmed_papers(
                                    state['pubmed_query'],
                                    target_count,
                                    progress_queue
                                )
                            tasks.append(asyncio.create_task(_fetch_papers_via_tools(
                                state['pubmed_query'],
                                'pubmed',
                                ['pubmed'],
                                _fallback_pubmed
                            )))

                        if state['europepmc_query']:
                            async def _fallback_europepmc():
                                return await search_service._fetch_europepmc_papers(
                                    state['europepmc_query'],
                                    target_count,
                                    progress_queue
                                )
                            tasks.append(asyncio.create_task(_fetch_papers_via_tools(
                                state['europepmc_query'],
                                'europepmc',
                                ['europepmc'],
                                _fallback_europepmc
                            )))

                        if tasks:
                            paper_batches = await asyncio.gather(*tasks)
                            for batch in paper_batches:
                                if batch:
                                    all_papers.extend(batch)

                    # å»é‡ã€æ‰“åˆ†å¹¶é™åˆ¶æ•°é‡
                    if all_papers:
                        state['papers'].extend(all_papers)
                        state['papers'] = self._trim_and_score_papers(
                            state['papers'],
                            state['pubmed_query'],
                            state['europepmc_query'],
                            target_count
                        )

                    # ä»…åœ¨éœ€è¦è¯•éªŒæ£€ç´¢æ—¶æ‰§è¡Œ
                    if state.get('intent', {}).get('use_trials', True) and state['clinical_trial_keywords']:
                        try:
                            trials_result = await self.tools.search_trials(
                                state['clinical_trial_keywords'],
                                target_count,
                            )
                            # ToolsFacade ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹ï¼›æ­¤å¤„è½¬æ¢ä¸ºåŸæ¥çš„ dict ç»“æ„
                            converted = [
                                {
                                    'nct_id': t.nct_id,
                                    'title': t.title,
                                    'status': t.status,
                                    'phase': t.phase,
                                    'conditions': t.conditions,
                                    'sponsor': t.sponsor,
                                    'locations': t.locations,
                                    'source_url': t.source_url,
                                }
                                for t in trials_result.trials
                            ]
                            state['trials'].extend(converted)
                        except Exception as _e:
                            # å›é€€è€å®ç°ï¼Œä¿æŒå…¼å®¹
                            trials = await search_service.search_trials_with_ranking(
                                state['clinical_trial_keywords'],
                                target_count,
                                progress_queue
                            )
                            state['trials'].extend(trials)
                        if state['trials']:
                            state['trials'] = self._trim_trials(state['trials'], target_count)
                        if not state['trials']:
                            logger.info("trials empty for keywords=%s", state.get('clinical_trial_keywords'))
                        try:
                            logging.getLogger("workflow_service").info(
                                "trials fetched count=%d keywords=%s",
                                len(state['trials']),
                                state.get('clinical_trial_keywords')
                            )
                            # é‡‡æ ·å‰3ä¸ªæ ‡é¢˜ç”¨äºå¿«é€Ÿç¡®è®¤
                            sample_titles = [t.get('title') for t in state['trials'][:3]]
                            logging.getLogger("workflow_service").info(
                                "trials sample titles=%s",
                                sample_titles
                            )
                        except Exception:
                            pass

                except Exception as e:
                    await progress_queue.put({
                        'type': 'log',
                        'source': 'search',
                        'content': f'âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n',
                        'newline': True
                    })
                finally:
                    await progress_queue.put({'type': 'DONE'})

            # å¯åŠ¨æ£€ç´¢ä»»åŠ¡
            search_task = asyncio.create_task(search_all())

            # è½¬å‘è¿›åº¦æ¶ˆæ¯
            while True:
                msg = await progress_queue.get()

                if isinstance(msg, dict):
                    if msg.get('type') == 'DONE':
                        break
                    elif msg.get('type') in ('log', 'result', 'progress'):
                        # ç›´æ¥è½¬å‘
                        if msg.get('type') == 'progress':
                            logging.getLogger("workflow_service").info(
                                "forward progress source=%s id=%s status=%s",
                                msg.get('source'), msg.get('id'), msg.get('status')
                            )
                        yield msg

            await search_task
            
            # æ£€æŸ¥ç»“æœ
            if self._should_stop_search(state, need_papers, need_trials) or retry >= max_retries:
                break  # æœ‰ç»“æœæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€€å‡º

        # æ±‡æ€»ç»“æœ
        yield {
            'type': 'result',
            'step': 'search',
            'content': f"""### ğŸ“Š æ£€ç´¢æ±‡æ€»

- **æ–‡çŒ®æ€»æ•°**: {len(state['papers'])} ç¯‡
- **ä¸´åºŠè¯•éªŒ**: {len(state['trials'])} ä¸ª""",
            'summary': f'âœ… æ£€ç´¢å®Œæˆï¼ˆ{len(state["papers"])} ç¯‡æ–‡çŒ®ï¼Œ{len(state["trials"])} ä¸ªè¯•éªŒï¼‰',
            'data': {
                'paper_count': len(state['papers']),
                'trial_count': len(state['trials'])
            }
        }

        yield {'type': 'section_end', 'step': 'search'}

    def _trim_and_score_papers(
            self,
            papers: List[Dict],
            pubmed_query: str,
            europepmc_query: str,
            limit: int
    ) -> List[Dict]:
        deduped = search_service._deduplicate_papers(papers)
        for paper in deduped:
            query = self._select_query_for_paper(paper, pubmed_query, europepmc_query)
            title_score = search_service._calculate_relevance(query, paper.get('title', ''))
            abstract_score = search_service._calculate_relevance(query, paper.get('abstract', ''))
            paper['relevance_score'] = (title_score * 0.7 + abstract_score * 0.3)
        deduped.sort(key=lambda p: p.get('relevance_score', 0), reverse=True)
        return deduped[:limit] if limit and limit > 0 else deduped

    def _select_query_for_paper(self, paper: Dict, pubmed_query: str, europepmc_query: str) -> str:
        source = (paper.get('source_type') or '').lower()
        if source == 'europepmc' and europepmc_query:
            return europepmc_query
        if source == 'pubmed' and pubmed_query:
            return pubmed_query
        # fallbackï¼šä»»é€‰å¯ç”¨çš„ query
        if pubmed_query:
            return pubmed_query
        if europepmc_query:
            return europepmc_query
        return ''

    def _should_stop_search(self, state: WorkflowState, need_papers: bool, need_trials: bool) -> bool:
        has_papers = len(state.get('papers', [])) > 0
        has_trials = len(state.get('trials', [])) > 0
        if need_papers and not need_trials:
            return has_papers
        if need_trials and not need_papers:
            return has_trials
        if need_papers and need_trials:
            return has_papers or has_trials
        return True

    def _trim_trials(self, trials: List[Dict], limit: int) -> List[Dict]:
        seen: Set[str] = set()
        unique: List[Dict] = []
        for trial in trials:
            key = (trial.get('nct_id') or '').strip()
            if key and key in seen:
                continue
            if key:
                seen.add(key)
            unique.append(trial)
        return unique[:limit] if limit and limit > 0 else unique
    
    async def _relax_queries(self, pubmed_query: str, europepmc_query: str, patient_features: str) -> tuple:
        """æ”¾å®½æ£€ç´¢æ¡ä»¶ï¼ˆç§»é™¤æœ€ä¸é‡è¦çš„æ¡ä»¶ï¼‰"""
        # ç®€å•çš„æ”¾å®½ç­–ç•¥ï¼šç§»é™¤ AND åé¢çš„ä¸€ä¸ªæ¡ä»¶
        if ' AND ' in pubmed_query:
            parts = pubmed_query.split(' AND ')
            relaxed_pubmed = ' AND '.join(parts[:-1]) if len(parts) > 1 else parts[0]
        else:
            relaxed_pubmed = pubmed_query
        
        if ',' in europepmc_query:
            parts = [p.strip() for p in europepmc_query.split(',')]
            relaxed_europepmc = ', '.join(parts[:-1]) if len(parts) > 1 else parts[0]
        else:
            relaxed_europepmc = europepmc_query
        
        return relaxed_pubmed, relaxed_europepmc

    async def _relax_trials_keywords(self, trial_keywords: str, patient_features: str) -> str:
        """æ”¾å®½ä¸´åºŠè¯•éªŒå…³é”®è¯ï¼šå‡å°‘è¿‡çª„è¯ã€å¢åŠ åŒä¹‰è¯/æ ¸å¿ƒè¯ï¼Œè¾“å‡ºé€—å·åˆ†éš”çš„3-5ä¸ªå…³é”®è¯"""
        base = (trial_keywords or '').strip()
        prompt = f"""åŸºäºæ‚£è€…ç‰¹å¾ä¸å½“å‰ä¸´åºŠè¯•éªŒå…³é”®è¯ï¼Œç”Ÿæˆæ›´å®½æ¾çš„å…³é”®è¯ï¼ˆ3-5ä¸ªï¼Œé€—å·åˆ†éš”ï¼‰ã€‚

æ‚£è€…ç‰¹å¾ï¼š{patient_features[:400]}
å½“å‰å…³é”®è¯ï¼š{base or 'ï¼ˆç©ºï¼‰'}

è¦æ±‚ï¼š
- å»é™¤è¿‡çª„çš„ä¿®é¥°è¯ï¼Œä¿ç•™ç–¾ç—…åç§°ã€è¯ç‰©/æœºåˆ¶ã€é˜¶æ®µç­‰æ ¸å¿ƒè¯
- ä»…è¾“å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼›ä¸è¦è¾“å‡ºé¢å¤–è¯´æ˜
- è‹¥å½“å‰ä¸ºç©ºï¼Œè¯·æ ¹æ®æ‚£è€…ç‰¹å¾ç”Ÿæˆåˆç†çš„3-5ä¸ªå…³é”®è¯
"""
        resp = ''
        try:
            async for token in llm_service.chat_with_context(
                user_query=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢ç­–ç•¥åŠ©æ‰‹ï¼Œè´Ÿè´£æ”¾å®½ä¸´åºŠè¯•éªŒå…³é”®è¯ã€‚"
            ):
                resp += token
        except Exception:
            return base or ''
        # è§„èŒƒåŒ–ï¼šä»¥é€—å·åˆ†å‰²ï¼Œå»ç©ºç™½ï¼Œæœ€å¤š5ä¸ª
        parts = [p.strip() for p in resp.split(',') if p.strip()]
        return ', '.join(parts[:5])
    async def _step_analyze_papers(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤4: åˆ†ææ–‡çŒ®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ¥å£ï¼‰"""
        state['current_step'] = 'analyze_papers'

        yield {
            'type': 'section_start',
            'step': 'analyze_papers',
            'title': 'ğŸ“„ åˆ†ææ–‡çŒ®',
            'collapsible': True
        }
        # æŒ‰ç”¨æˆ·æ„å›¾è·³è¿‡æ–‡çŒ®åˆ†æ
        if not state.get('intent', {}).get('use_papers', True):
            yield {
                'type': 'result',
                'step': 'analyze_papers',
                'content': 'â„¹ï¸ å·²æŒ‰ç”¨æˆ·æ„å›¾è·³è¿‡æ–‡çŒ®åˆ†æ',
                'summary': 'â„¹ï¸ è·³è¿‡æ–‡çŒ®åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_papers'}
            return

        if not state['papers']:
            yield {
                'type': 'result',
                'step': 'analyze_papers',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®',
                'summary': 'â„¹ï¸ æ— æ–‡çŒ®å¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_papers'}
            return

        from app.services.file_service import file_service

        for i, paper in enumerate(state['papers']):
            yield {
                'type': 'log',
                'step': 'analyze_papers',
                'source': 'analyze_papers',
                'content': f'\nğŸ“„ åˆ†ææ–‡çŒ® {i+1}/{len(state["papers"])}: {paper["title"][:50]}...\n\n',
                'newline': True
            }

            pdf_path = paper.get('pdf_path')
            if not pdf_path or not os.path.exists(pdf_path):
                yield {
                    'type': 'log',
                    'step': 'analyze_papers',
                    'source': 'analyze_papers',
                    'content': 'âš ï¸ PDFä¸å­˜åœ¨ï¼Œè·³è¿‡\n',
                    'newline': True
                }
                continue

            prompt = self.prompts.analyze_paper(
                state['patient_features'],
                state['user_query'],
                paper
            )

            analysis = ""
            try:
                # ä¼˜å…ˆé€šè¿‡å·¥å…·æ¥å£å±‚è¿›è¡Œ PDF æµå¼åˆ†æ
                async for token in self.tools.analyze_pdf_stream(
                        patient_features=state['patient_features'],
                        user_query=state['user_query'],
                        pdf_path=pdf_path,
                ):  # type: ignore
                    analysis += token
                    self._budget_tokens += 1
                    yield {
                        'type': 'result',
                        'step': 'analyze_papers',
                        'content': token,
                        'is_incremental': True
                    }
                
                # æˆåŠŸåˆ†æåï¼Œå°†ç»“æœæ·»åŠ åˆ°çŠ¶æ€ä¸­
                state['paper_analyses'].append({
                    'paper': paper,
                    'analysis': analysis
                })
                
                # æœ€åæ¨é€å®Œæ•´å†…å®¹
                yield {
                    'type': 'result',
                    'step': 'analyze_papers',
                    'content': f"""### æ–‡çŒ® {i+1}: {paper['title']}

{analysis}""",
                    'is_incremental': False,
                    'data': {
                        'paper_id': paper.get('id'),
                        'pmid': paper.get('pmid'),
                        'title': paper['title']
                    }
                }
            except Exception as e:
                # å›é€€ï¼šæ²¿ç”¨ç°æœ‰ llm_service + file_service è·¯å¾„ï¼Œä¿è¯å…¼å®¹
                try:
                    file_id = await file_service.get_or_upload_file(pdf_path)
                    if not file_id:
                        raise Exception("æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                    
                    prompt = self.prompts.analyze_paper(
                        state['patient_features'],
                        state['user_query'],
                        paper
                    )
                    
                    analysis = ""
                    async for token in llm_service.chat_with_context(
                            user_query=prompt,
                            file_ids=[file_id],
                            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ–‡çŒ®åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»PDFæ–‡æ¡£ï¼ŒæŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºç»“æ„åŒ–åˆ†æã€‚",
                            model=settings.qwen_long_model
                    ):
                        analysis += token
                        self._budget_tokens += 1
                        yield {
                            'type': 'result',
                            'step': 'analyze_papers',
                            'content': token,
                            'is_incremental': True
                        }
                    
                    # æˆåŠŸåˆ†æåï¼Œå°†ç»“æœæ·»åŠ åˆ°çŠ¶æ€ä¸­
                    state['paper_analyses'].append({
                        'paper': paper,
                        'analysis': analysis
                    })
                    
                    # æœ€åæ¨é€å®Œæ•´å†…å®¹
                    yield {
                        'type': 'result',
                        'step': 'analyze_papers',
                        'content': f"""### æ–‡çŒ® {i+1}: {paper['title']}

{analysis}""",
                        'is_incremental': False,
                        'data': {
                            'paper_id': paper.get('id'),
                            'pmid': paper.get('pmid'),
                            'title': paper['title']
                        }
                    }
                except Exception as fallback_e:
                    yield {
                        'type': 'log',
                        'step': 'analyze_papers',
                        'source': 'analyze_papers',
                        'content': f'âŒ åˆ†æå¤±è´¥: {str(fallback_e)}\n',
                        'newline': True
                    }
                    continue

        yield {
            'type': 'result',
            'step': 'analyze_papers',
            'content': '',
            'summary': f'âœ… æ–‡çŒ®åˆ†æå®Œæˆï¼ˆ{len(state["paper_analyses"])} ç¯‡ï¼‰'
        }

        yield {'type': 'section_end', 'step': 'analyze_papers'}

    async def _step_analyze_trials(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤5: åˆ†æä¸´åºŠè¯•éªŒ"""
        state['current_step'] = 'analyze_trials'

        yield {
            'type': 'section_start',
            'step': 'analyze_trials',
            'title': 'ğŸ’Š åˆ†æä¸´åºŠè¯•éªŒ',
            'collapsible': True
        }
        # æŒ‰ç”¨æˆ·æ„å›¾è·³è¿‡ä¸´åºŠè¯•éªŒåˆ†æ
        if not state.get('intent', {}).get('use_trials', True):
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': 'â„¹ï¸ å·²æŒ‰ç”¨æˆ·æ„å›¾è·³è¿‡ä¸´åºŠè¯•éªŒåˆ†æ',
                'summary': 'â„¹ï¸ è·³è¿‡ä¸´åºŠè¯•éªŒåˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_trials'}
            return

        if not state['trials']:
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': 'â„¹ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ä¸´åºŠè¯•éªŒ',
                'summary': 'â„¹ï¸ æ— è¯•éªŒå¯åˆ†æ'
            }
            yield {'type': 'section_end', 'step': 'analyze_trials'}
            return

        yield {
            'type': 'log',
            'step': 'analyze_trials',
            'source': 'analyze_trials',
            'content': f'æ­£åœ¨åˆ†æ {len(state["trials"])} ä¸ªä¸´åºŠè¯•éªŒ...\n\n',
            'newline': True
        }

        trials_text = []
        for i, trial in enumerate(state['trials']):
            trial_info = f"""### è¯•éªŒ {i+1}: {trial.get('title')}
- **NCT ID**: {trial.get('nct_id')}
- **çŠ¶æ€**: {trial.get('status')}
- **é˜¶æ®µ**: {trial.get('phase')}
- **ç–¾ç—…**: {trial.get('conditions')}
- **èµåŠ©æ–¹**: {trial.get('sponsor')}
"""
            trials_text.append(trial_info)

        # ä½¿ç”¨å·¥å…·æ¥å£å±‚è¿›è¡Œæµå¼åˆ†æï¼Œä¿æŒ SSE è¾“å‡ºä¸å˜
        analysis = ""
        try:
            # è½¬æ¢ä¸ºå·¥å…·å±‚ Trial æ¨¡å‹
            tool_trials = [
                ToolTrial(
                    nct_id=t.get('nct_id', ''),
                    title=t.get('title', ''),
                    status=t.get('status'),
                    phase=t.get('phase'),
                    conditions=t.get('conditions'),
                    sponsor=t.get('sponsor'),
                    locations=t.get('locations'),
                    source_url=t.get('source_url'),
                )
                for t in state['trials']
            ]

            _token_count = 0
            async for token in self.tools.analyze_trials_stream(
                state['patient_features'],
                tool_trials,
            ):  # type: ignore
                analysis += token
                _token_count += 1
                self._budget_tokens += 1
                yield {
                    'type': 'result',
                    'step': 'analyze_trials',
                    'content': token,
                    'is_incremental': True,
                }

            logger.info(
                "analyze_trials done tokens=%d content_len=%d",
                _token_count,
                len(analysis),
            )
            if not analysis:
                logger.warning("No analysis output")

            state['trial_analysis'] = analysis
            yield {
                'type': 'result',
                'step': 'analyze_trials',
                'content': analysis,
                'is_incremental': False,
                'summary': f'âœ… ä¸´åºŠè¯•éªŒåˆ†æå®Œæˆï¼ˆ{len(state["trials"])} ä¸ªï¼‰',
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'analyze_trials',
                'source': 'analyze_trials',
                'content': f'âŒ åˆ†æå¤±è´¥: {str(e)}\n',
                'newline': True
            }
            logger.exception("analyze_trials error: %s", str(e))

        yield {'type': 'section_end', 'step': 'analyze_trials'}

    async def _step_generate_final(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        state['current_step'] = 'generate_final'

        yield {
            'type': 'section_start',
            'step': 'generate_final',
            'title': 'ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š',
            'collapsible': False
        }

        yield {
            'type': 'log',
            'step': 'generate_final',
            'source': 'generate_final',
            'content': 'æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š...\n\n',
            'newline': True
        }

        papers_summary = []
        for i, item in enumerate(state['paper_analyses']):
            summary = f"**æ–‡çŒ® {i+1}**: {item['paper']['title']} - {item['analysis'][:200]}..."
            papers_summary.append(summary)

        prompt = self.prompts.generate_final_report(
            state['user_query'],
            state['patient_features'],
            '\n'.join(papers_summary) if papers_summary else "æš‚æ— ",
            state['trial_analysis']
        )

        final_answer = ""
        try:
            # ä¼˜å…ˆé€šè¿‡å·¥å…·æ¥å£å±‚ç”ŸæˆæŠ¥å‘Šï¼ˆä¸€æ¬¡æ€§æ–‡æœ¬ï¼‰ï¼Œå†æŒ‰å­—ç¬¦å›æ”¾ä¸º token ä»¥ä¿æŒå‰ç«¯ä½“éªŒ
            try:
                report = await self.tools.generate_report(
                    user_query=state['user_query'],
                    patient_features=state['patient_features'],
                    papers_summary='\n'.join(papers_summary) if papers_summary else "æš‚æ— ",
                    trial_analysis=state['trial_analysis'],
                )
                final_answer = report.final_answer or ""
                for ch in final_answer:
                    yield {
                        'type': 'token',
                        'step': 'generate_final',
                        'content': ch,
                    }
                    self._budget_tokens += 1
            except Exception:
                # å›é€€ï¼šæ²¿ç”¨ç°æœ‰ llm_service æµå¼è·¯å¾„
                async for token in llm_service.chat_with_context(
                        user_query=prompt,
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å’¨è¯¢æŠ¥å‘Šç”ŸæˆåŠ©æ‰‹ã€‚",
                        model=settings.qwen_long_model
                ):
                    final_answer += token
                    self._budget_tokens += 1
                    yield {
                        'type': 'token',
                        'step': 'generate_final',
                        'content': token
                    }

            # ä¿å­˜æœ€ç»ˆç­”æ¡ˆå¹¶è¾“å‡ºå®Œæˆæ±‡æ€»
            state['final_answer'] = final_answer
            yield {
                'type': 'result',
                'step': 'generate_final',
                'content': '',
                'summary': 'âœ… æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ'
            }

        except Exception as e:
            yield {
                'type': 'log',
                'step': 'generate_final',
                'source': 'generate_final',
                'content': f'âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n',
                'newline': True
            }

        yield {'type': 'section_end', 'step': 'generate_final'}

    async def _step_grounding_check(self, state: WorkflowState) -> AsyncGenerator[Dict, None]:
        """è¯æ®å¯¹é½ä¸å†²çªæ£€æµ‹ï¼šè¾“å‡ºç»“æ„åŒ–æ—¥å¿—ï¼ˆä¸æ”¹å˜ä¸šåŠ¡ç»“æœï¼‰ã€‚"""
        import re
        yield {
            'type': 'section_start',
            'step': 'grounding_deliberate',
            'title': 'ğŸ§· è¯æ®å¯¹é½ï¼ˆGroundingï¼‰',
            'collapsible': True,
        }
        # Grounding æ–‡æœ¬æ¥æºï¼šä¸´åºŠè¯•éªŒåˆ†æ + å„æ–‡çŒ®åˆ†ææ­£æ–‡
        trial_text = state.get('trial_analysis') or ''
        paper_texts = []
        for item in state.get('paper_analyses', []) or []:
            try:
                paper = item.get('paper') or {}
                title = paper.get('title') or ''
                analysis = item.get('analysis') or ''
                if title or analysis:
                    paper_texts.append(f"{title}\n{analysis}")
            except Exception:
                continue
        text = trial_text + ('\n' if trial_text and paper_texts else '') + '\n'.join(paper_texts)
        # æå–å¼•ç”¨é”šç‚¹
        pmids = set(re.findall(r"PMID[:\s]?\d+", text, flags=re.IGNORECASE))
        ncts = set(re.findall(r"NCT\d+", text, flags=0))
        refs_count = len(pmids) + len(ncts)
        if refs_count == 0:
            yield {'type': 'log', 'step': 'grounding_deliberate', 'source': 'grounding', 'content': 'warn: no_citations_found\n', 'newline': True}
        else:
            yield {'type': 'log', 'step': 'grounding_deliberate', 'source': 'grounding', 'content': f'citations: count={refs_count} pmids={len(pmids)} ncts={len(ncts)}\n', 'newline': True}

        # ç®€å•ä¸€è‡´æ€§/å†²çªæ£€æµ‹ï¼ˆå¯å‘å¼ï¼‰
        lower = text.lower()
        has_positive = any(k in lower for k in ['æ˜¾è‘—æé«˜', 'significant improvement', 'effective'])
        has_negative = any(k in lower for k in ['æœªæ˜¾ç¤ºæ˜¾è‘—', 'no significant', 'ineffective'])
        if has_positive and has_negative:
            yield {'type': 'log', 'step': 'grounding_deliberate', 'source': 'grounding', 'content': 'conflict: positive_vs_negative_evidence\n', 'newline': True}

        # è¿½æº¯æ€§ï¼šå±•ç¤ºè‹¥å¹²å¼•ç”¨æ ·ä¾‹
        sample_refs = list(pmids)[:3] + list(ncts)[:3]
        if sample_refs:
            yield {'type': 'log', 'step': 'grounding_deliberate', 'source': 'grounding', 'content': f'trace: sample_refs={", ".join(sample_refs)}\n', 'newline': True}

        yield {'type': 'section_end', 'step': 'grounding_deliberate'}

        # å¯é€‰ï¼šå±•ç¤ºå‹ critiqueï¼ˆä¸æ”¹æµç¨‹ï¼‰
        if settings.deliberate_enabled:
            yield {
                'type': 'section_start',
                'step': 'critique_deliberate',
                'title': 'ğŸ§ª è¯„å®¡ï¼ˆå±•ç¤ºå‹ï¼‰',
                'collapsible': True,
            }
            yield {
                'type': 'log',
                'step': 'critique_deliberate',
                'source': 'router',
                'content': 'critique: display_only=true checks=[format,consistency]\n',
                'newline': True,
            }
            yield {'type': 'section_end', 'step': 'critique_deliberate'}

    async def _generate_title(self, state: WorkflowState, conversation_id: int, user_id: int) -> Optional[str]:
        """ç”Ÿæˆå¯¹è¯æ ‡é¢˜ï¼Œè¿”å›æ–°æ ‡é¢˜"""
        try:
            title_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹åŒ»ç–—å’¨è¯¢å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼ˆä¸è¶…è¿‡15ä¸ªå­—ï¼‰ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

æ‚£è€…ç‰¹å¾ï¼š{state['patient_features'][:300]}...

è¦æ±‚ï¼š
1. çªå‡ºç–¾ç—…/ç—‡çŠ¶å…³é”®è¯
2. ä¸è¶…è¿‡15ä¸ªå­—
3. ç›´æ¥è¾“å‡ºæ ‡é¢˜ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹
4. ä¸ä½¿ç”¨å¼•å·ã€ä¹¦åå·ç­‰æ ‡ç‚¹ç¬¦å·

æ ‡é¢˜ï¼š"""

            new_title = ""
            async for token in llm_service.chat_with_context(
                    user_query=title_prompt,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ ‡é¢˜ç”ŸæˆåŠ©æ‰‹ã€‚"
            ):
                new_title += token

            # æ¸…ç†æ ‡é¢˜
            new_title = new_title.strip().replace('\n', '').replace('"', '').replace("'", '')

            if new_title and len(new_title) > 2:
                if len(new_title) > 15:
                    new_title = new_title[:15] + "..."

                async with get_db_session() as db:
                    from app.schemas.conversation import ConversationUpdateSchema
                    from app.crud import conversation as crud_conversation
                    await crud_conversation.update_conversation(
                        db,
                        conversation_id=conversation_id,
                        conversation_schema=ConversationUpdateSchema(title=new_title),
                        user_id=user_id
                    )

                logger.info(f"å¯¹è¯ {conversation_id} å·²è‡ªåŠ¨é‡å‘½åä¸ºã€Œ{new_title}ã€")
                return new_title
            else:
                logger.warning(f"ç”Ÿæˆçš„æ ‡é¢˜æ— æ•ˆï¼Œæ ‡é¢˜: {new_title}")
                return None

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ ‡é¢˜å¤±è´¥: {e}")
            return None

    async def _create_execution(self, conversation_id: int, user_id: int) -> int:
        """åˆ›å»ºæ‰§è¡Œè®°å½•"""
        async with get_db_session() as db:
            execution = WorkflowExecution(
                conversation_id=conversation_id,
                user_id=user_id,
                workflow_type='multi_source',
                status='running',
                current_step='initializing'
            )
            db.add(execution)
            await db.commit()
            return execution.id

    async def _update_execution(self, execution_id: int, status: str, error: Optional[str] = None):
        """æ›´æ–°æ‰§è¡ŒçŠ¶æ€"""
        async with get_db_session() as db:
            execution = await db.get(WorkflowExecution, execution_id)
            if execution is None:
                logger.warning(f"æ‰¾ä¸åˆ°æ‰§è¡Œè®°å½•: {execution_id}")
                return
            execution.status = status
            if status == 'completed':
                execution.completed_at = func.now()
            if error:
                execution.error_message = error
            await db.commit()

    async def _load_history(self, conversation_id: int) -> List[Dict]:
        """åŠ è½½å†å²å¯¹è¯"""
        async with get_db_session() as db:
            result = await db.execute(
                select(Message)
                .where(Message.conversation_id == conversation_id)
                .order_by(Message.created_at.desc())
                .limit(10)
            )
            messages = result.scalars().all()

            return [
                {
                    'type': 'user' if m.message_type == MessageType.USER else 'assistant',
                    'content': m.content
                }
                for m in reversed(list(messages))
            ]

    async def _load_cached_patient_features(self, conversation_id: int) -> Optional[str]:
        """ä»ä¹‹å‰çš„å·¥ä½œæµæ‰§è¡Œè®°å½•ä¸­åŠ è½½ç¼“å­˜çš„æ‚£è€…ç‰¹å¾"""
        async with get_db_session() as db:
            result = await db.execute(
                select(WorkflowExecution)
                .where(WorkflowExecution.conversation_id == conversation_id)
                .where(WorkflowExecution.patient_features.isnot(None))
                .order_by(WorkflowExecution.created_at.desc())
                .limit(1)
            )
            execution = result.scalar_one_or_none()
            
            if execution and execution.patient_features:
                logger.info(f"ä»æ‰§è¡Œè®°å½• {execution.id} ä¸­åŠ è½½ç¼“å­˜çš„æ‚£è€…ç‰¹å¾")
                return execution.patient_features
            
            return None

    async def _save_result(self, state: WorkflowState, execution_id: int, message_id: int):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        async with get_db_session() as db:
            # åŠ¨æ€æ„å»ºæŠ¥å‘Šå†…å®¹
            full_parts: list[str] = []
            full_parts.append("# å¤šæºæ£€ç´¢åˆ†ææŠ¥å‘Š\n\n")

            # 1. æ‚£è€…ç‰¹å¾
            full_parts.append("## 1. æ‚£è€…ç‰¹å¾åˆ†æ\n")
            full_parts.append(f"{state['patient_features']}\n\n---\n")

            # 2. æ£€ç´¢æ¡ä»¶ï¼ˆæŒ‰éœ€è¾“å‡ºï¼‰
            full_parts.append("\n## 2. æ£€ç´¢æ¡ä»¶\n")
            added_any = False
            if state.get('intent', {}).get('use_papers', True):
                if state['pubmed_query']:
                    full_parts.append(f"- **PubMed**: `{state['pubmed_query']}`\n"); added_any = True
                if state['europepmc_query']:
                    full_parts.append(f"- **Europe PMC**: `{state['europepmc_query']}`\n"); added_any = True
            if state.get('intent', {}).get('use_trials', True) and state['clinical_trial_keywords']:
                full_parts.append(f"- **ä¸´åºŠè¯•éªŒ**: `{state['clinical_trial_keywords']}`\n"); added_any = True
            if not added_any:
                full_parts.append("- æš‚æ— \n")
            full_parts.append("\n---\n")

            # 3. æ£€ç´¢ç»“æœæ±‡æ€»
            full_parts.append("\n## 3. æ£€ç´¢ç»“æœ\n")
            full_parts.append(f"- **æ–‡çŒ®æ•°é‡**: {len(state['papers'])} ç¯‡\n")
            full_parts.append(f"- **ä¸´åºŠè¯•éªŒæ•°é‡**: {len(state['trials'])} ä¸ª\n\n---\n")

            # 4. æ–‡çŒ®åˆ†æï¼ˆå¦‚æœ‰ä¸”ç”¨æˆ·éœ€è¦ï¼‰
            if state.get('intent', {}).get('use_papers', True) and state['paper_analyses']:
                full_parts.append("\n## 4. æ–‡çŒ®åˆ†æ\n\n")
                for i, item in enumerate(state['paper_analyses']):
                    full_parts.append(f"\n### æ–‡çŒ® {i+1}: {item['paper']['title']}\n\n")
                    full_parts.append(f"{item['analysis']}\n\n---\n")

            # 5. ä¸´åºŠè¯•éªŒåˆ†æï¼ˆå¦‚æœ‰ä¸”ç”¨æˆ·éœ€è¦ï¼‰
            if state.get('intent', {}).get('use_trials', True) and state['trial_analysis']:
                full_parts.append("\n## 5. ä¸´åºŠè¯•éªŒåˆ†æ\n\n")
                full_parts.append(f"{state['trial_analysis']}\n\n---\n")

            # 6. ç»¼åˆæŠ¥å‘Š
            full_parts.append(f"\n## 6. ç»¼åˆæŠ¥å‘Š\n\n{state['final_answer']}\n")

            full_content = "".join(full_parts)

            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "workflow_type": "multi_source",
                "patient_features": state['patient_features'],
                "search_queries": {
                    "pubmed": state['pubmed_query'],
                    "europepmc": state['europepmc_query'],
                    "clinical_trial": state['clinical_trial_keywords']
                },
                "papers": [
                    {
                        "id": paper.get('id'),
                        "pmid": paper.get('pmid'),
                        "title": paper.get('title'),
                        "authors": paper.get('authors')
                    }
                    for paper in state['papers']
                ],
                "trials": [
                    {
                        "nct_id": trial.get('nct_id'),
                        "title": trial.get('title')
                    }
                    for trial in state['trials']
                ],
                "attachments": [
                    {
                        "filename": att.get('filename'),
                        "original_filename": att.get('original_filename')
                    }
                    for att in state['user_attachments']
                ]
            }

            # æ›´æ–°ç°æœ‰æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯
            await crud_message.update_message(
                db,
                message_id=message_id,
                content=full_content,
                status=MessageStatus.COMPLETED
            )
            
            # æ›´æ–°æ¶ˆæ¯å…ƒæ•°æ®
            await db.execute(
                update(Message)
                .where(Message.id == message_id)
                .values(metadata_json=json.dumps(metadata, ensure_ascii=False))
            )

            execution = await db.get(WorkflowExecution, execution_id)
            if execution is None:
                logger.warning(f"æ‰¾ä¸åˆ°æ‰§è¡Œè®°å½•: {execution_id}")
                return
            execution.result_message_id = message_id
            execution.patient_features = state['patient_features']
            execution.search_queries = json.dumps({
                'pubmed': state['pubmed_query'],
                'clinical_trial': state['clinical_trial_keywords']
            })
            await db.commit()
    
    async def _save_error_result(self, state: WorkflowState, execution_id: int, message_id: int, error_msg: str):
        """ä¿å­˜é”™è¯¯ç»“æœåˆ°æ•°æ®åº“"""
        async with get_db_session() as db:
            # æ„å»ºé”™è¯¯ä¿¡æ¯å†…å®¹
            error_content = f"""âŒ **å¤šæºæ£€ç´¢æ‰§è¡Œå¤±è´¥**\n\n"""
            
            # æ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå»é™¤é‡å¤çš„å‰ç¼€ï¼‰
            clean_error_msg = error_msg
            if 'ï¼š' in error_msg:
                # æå–å†’å·åé¢çš„å†…å®¹
                clean_error_msg = error_msg.split('ï¼š', 1)[1].strip()
            
            error_content += f"""{clean_error_msg}\n\n---\n\nè¯·æ ¹æ®ä»¥ä¸Šæç¤ºè°ƒæ•´æ‚¨çš„è¾“å…¥ï¼Œç„¶åé‡è¯•ã€‚
"""
            
            # æ›´æ–°ç°æœ‰æ¶ˆæ¯ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯
            await crud_message.update_message(
                db,
                message_id=message_id,
                content=error_content,
                status=MessageStatus.FAILED
            )


workflow_service = WorkflowService()